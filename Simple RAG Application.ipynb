{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d21ecf0-a782-4db8-a699-ed01b1d0ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RAG Application from an uploaded pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51830369-84a9-4fcf-a6a4-cdebdceb30ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8cf5072-81c0-464c-b0cd-72305edd4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_ENDPOINT\"] = os.getenv(\"LANGSMITH_ENDPOINT\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "665f313b-8a6c-48cd-a957-bd180f489382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://api.smith.langchain.com', 'GenAIAppWithLangChain')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"LANGSMITH_ENDPOINT\"], os.environ[\"LANGSMITH_PROJECT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d722dfd-53b3-464f-a7af-a0d1c907c8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/divyanshpurohit/Desktop/Projects/Jupyter NB/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2b1626-95eb-469f-9a36-9ef732721ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyMuPDFLoader at 0x11339d910>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyMuPDFLoader(\"./Data/PDFs/Report.pdf\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aac2e557-ff25-405f-9f99-098bd7525087",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f8f8de3-0fd4-49ca-a2cc-5a425c1ba0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f291496f-f3ad-4149-a1d1-607aa90438a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4279e84b-4521-423d-98fe-193ac170d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter.split_documents(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5402a6d7-5959-47b7-ad45-c2b408cd4144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering Data Intensive Systems - 2IMD10\n",
      "EDS - PROJECT REPORT\n",
      "Team Number - 16\n",
      "Full Name\n",
      "Discord Username\n",
      "Email\n",
      "Divyansh Purohit\n",
      "wah shampy\n",
      "d.purohit@student.tue.nl\n",
      "Likhit Vesalapu\n",
      "likhit7.\n",
      "l.vesalapu@student.tue.nl\n",
      "Prathamesh Samal\n",
      "viper 101\n",
      "p.samal@student.tue.nl\n",
      "Elena Terzieva\n",
      "ellie218388\n",
      "e.e.terzieva@student.tue.nl\n",
      "Eindhoven, February 1, 2026\n",
      "\n",
      "1\n",
      "ABSTRACT\n",
      "Accurate cardinality estimation is fundamental to query opti-\n",
      "mization in graph databases, enabling the selection of efficient\n",
      "execution plans for regular path queries. In this report, we\n",
      "present a hybrid cardinality estimator that combines multi-\n",
      "ple statistical synopses including per-label statistics, pairwise\n",
      "label correlations, and characteristic sets with a weighted\n",
      "and stratified sampling strategy for complex queries. Our ap-\n",
      "proach balances estimation accuracy against preparation time\n",
      "and memory overhead, achieving competitive performance\n",
      "on both synthetic and real-world workloads.\n",
      "2\n",
      "INTRODUCTION\n",
      "The efficiency of query processing is an important topic in\n",
      "the domain of databases, where cardinality estimation plays\n",
      "a central role. Modern graph and XML database systems\n",
      "rely on cardinality estimators as a core component of their\n",
      "cost based query optimizers. A cardinality estimator is a\n",
      "statistical module that, given an abstract description of a query\n",
      "\n",
      "rely on cardinality estimators as a core component of their\n",
      "cost based query optimizers. A cardinality estimator is a\n",
      "statistical module that, given an abstract description of a query\n",
      "(a labeled path pattern starting from some node and ending\n",
      "at another), predicts how many results this query is likely to\n",
      "produce. These predictions are crucial for choosing efficient\n",
      "execution plans, selecting join orders, and deciding which\n",
      "physical operators to use. Without an accurate cardinality\n",
      "estimator, an optimizer can easily choose plans that are orders\n",
      "of magnitude slower than necessary.\n",
      "Traditional cardinality estimators for path queries are based\n",
      "on compact summaries of structural synopses that record\n",
      "per-label statistics, degree distributions, and limited informa-\n",
      "tion about how labels coincide. Although such synopses can\n",
      "be very effective for short, simple path patterns, their accuracy\n",
      "rapidly degrades for long paths, patterns with repetition, or\n",
      "\n",
      "tion about how labels coincide. Although such synopses can\n",
      "be very effective for short, simple path patterns, their accuracy\n",
      "rapidly degrades for long paths, patterns with repetition, or\n",
      "data with strong structural correlations. In these cases, naive\n",
      "independence assumptions break down and purely synopsis\n",
      "based estimators can produce large errors.\n",
      "To address these challenges, our work investigates a sam-\n",
      "pling based cardinality estimator that involves four com-\n",
      "plementary approaches: Stratified reservoir sampling, End-\n",
      "biased sampling, Correlated sampling, XSEED based Simula-\n",
      "tion.\n",
      "3\n",
      "LITERATURE SURVEY\n",
      "The topic of cardinality estimation continues to be one of the\n",
      "most important in query optimization. There are different\n",
      "techniques that have been developed for estimating the size\n",
      "of the results produced by a query. In this project, we are\n",
      "dealing with RDF databases that differ from the typical re-\n",
      "lational schemas because of their structure, which leads to\n",
      "\n",
      "of the results produced by a query. In this project, we are\n",
      "dealing with RDF databases that differ from the typical re-\n",
      "lational schemas because of their structure, which leads to\n",
      "different assumptions and decisions made when estimating\n",
      "the cardinality of a query. In query optimisation, synopses are\n",
      "precomputed summaries of database data (like histograms,\n",
      "sketches, samples) that database systems use to quickly es-\n",
      "timate the cost of different query execution plans. Bonifati\n",
      "et al. survey cardinality estimation techniques specifically\n",
      "designed for graph query languages. The two approaches\n",
      "discussed are cardinality of paths and cardinality on patterns.\n",
      "With cardinality on paths, the query path is split into simpler\n",
      "segments and predefined formulas are used for concentra-\n",
      "tion, union and inversion are combined to estimate the result.\n",
      "Cardinality on patterns, on the other hand focuses on graph\n",
      "patterns with characteristic sets or graph summaries. With\n",
      "\n",
      "tion, union and inversion are combined to estimate the result.\n",
      "Cardinality on patterns, on the other hand focuses on graph\n",
      "patterns with characteristic sets or graph summaries. With\n",
      "characteristic sets, each node is summarised by the set of\n",
      "outgoing edge labels it has, and these “characteristic sets” are\n",
      "used to estimate how many nodes (and joins) participate in\n",
      "a given pattern. They are discussed in a study by Neumann\n",
      "and Moerkotte, where they observe that in RDF databases,\n",
      "many subjects share the same set of outgoing predicates, and\n",
      "define a characteristic set as exactly this set of predicates\n",
      "attached to a subject [7]. With graph summarizers, the graph\n",
      "is compressed into a smaller summary (by grouping similar\n",
      "nodes/edges), and cardinalities for complex path or subgraph\n",
      "patterns are estimated by evaluating the query on this sum-\n",
      "mary instead of on the full graph.\n",
      "Two statistical methods for dealing with that are sampling\n",
      "and histograms. Estan and Naughton propose end-biased\n",
      "\n",
      "mary instead of on the full graph.\n",
      "Two statistical methods for dealing with that are sampling\n",
      "and histograms. Estan and Naughton propose end-biased\n",
      "samples for join cardinality estimation in relational databases.\n",
      "The method retains tuples with probability proportional to\n",
      "their frequency in join columns, so that high-frequency val-\n",
      "ues (responsible for most join output) are over-represented.\n",
      "This yields much more accurate join size estimates than uni-\n",
      "form sampling at comparable sample sizes [5]. Babcock and\n",
      "Chaudhuri introduce a technique based on Bayesian inference\n",
      "over precomputed random samples, which models uncertainty\n",
      "and produces estimates together with confidence information.\n",
      "Their method captures multi-dimensional correlations without\n",
      "assuming attribute independence and remains time-efficient.\n",
      "Importantly, it supports user- or application-specified trade-\n",
      "offs between performance and predictability, contributing to\n",
      "the design of more robust query optimisers [1].\n",
      "\n",
      "Importantly, it supports user- or application-specified trade-\n",
      "offs between performance and predictability, contributing to\n",
      "the design of more robust query optimisers [1].\n",
      "Histograms are one of the most widely used methods for\n",
      "cardinality estimation in relational DBMS. The basic idea\n",
      "is to partition the domain of an attribute into buckets and\n",
      "store, for each bucket, the number of tuples (and sometimes\n",
      "additional stats such as distinct values). Query selectivities\n",
      "are then approximated by assuming a uniform distribution of\n",
      "values inside each bucket and summing contributions from\n",
      "the buckets that overlap the predicate. A detailed survey\n",
      "of histogram-based and related synopsis techniques is given\n",
      "by Cormode, who reviews equi-width, equi-depth and more\n",
      "advanced frequency-based histograms, as well as their multi-\n",
      "dimensional extensions and maintenance trade-offs [3]. Joins\n",
      "can be very expensive, and to try to bridge the gap between\n",
      "\n",
      "advanced frequency-based histograms, as well as their multi-\n",
      "dimensional extensions and maintenance trade-offs [3]. Joins\n",
      "can be very expensive, and to try to bridge the gap between\n",
      "query efficiency and accuracy, Li et al. introduced the new\n",
      "algorithm Wander Join [6]. Wander Join is a join estimation\n",
      "method that treats a multi-table join like a graph and then\n",
      "performs random walks through the graph instead of com-\n",
      "puting the full join. Since it only touches a small number\n",
      "of data points and does not need to compute statistics for\n",
      "the full data, the approximation results are produced quickly\n",
      "and more accurately with the increase in performed random\n",
      "\n",
      "walks.\n",
      "4\n",
      "IMPLEMENTATION\n",
      "4.1\n",
      "Approach and Planning\n",
      "4.1.1\n",
      "Problem Analysis\n",
      "In order to address the problem of cardinality estimation for\n",
      "path queries, our team first examined the basic properties of\n",
      "graph data. Especially in networks, graph data shows heavy\n",
      "tailed, power-law distribution, variable-length traversals and\n",
      "Kleene closures in comparison to relational tables where data\n",
      "usually follows uniform distribution. The scoring metric for\n",
      "this project emphasizes the accuracy on real workloads with\n",
      "a weight of two and also penalizing the run times. This\n",
      "tuned our strategy to get accurate estimates without expensive\n",
      "computation.\n",
      "The provided graph contained 10,000 nodes, 22,122 edges,\n",
      "and 36 distinct edge labels. With an average out-degree of\n",
      "approximately 2.92 and a maximum out-degree of 43, the\n",
      "graph is relatively sparse (density approx 0.02%). The de-\n",
      "gree distribution follows a classic power-law pattern: 37% of\n",
      "nodes have out-degree 1, while only a handful of hub nodes\n",
      "\n",
      "graph is relatively sparse (density approx 0.02%). The de-\n",
      "gree distribution follows a classic power-law pattern: 37% of\n",
      "nodes have out-degree 1, while only a handful of hub nodes\n",
      "exceed degree 20. This skewed distribution means that a small\n",
      "number of high-degree nodes disproportionately contribute\n",
      "to path counts, making uniform sampling ineffective.\n",
      "The label distribution is similarly skewed, with label 8\n",
      "appearing in over 5,000 edges (23% of all edges), while\n",
      "12 labels appear fewer than 50 times each. This imbalance\n",
      "affects join selectivity and makes certain label combinations\n",
      "far more common than others. The query workload spanned\n",
      "diverse patterns:\n",
      "• Single predicates: e.g., ,1>,, ,8>,\n",
      "• Concatenations: e.g., ,1>/8>,, ,1>/8>/9>/8>, (up\n",
      "to length 5)\n",
      "• Kleene closures: e.g., ,(1>)+,, ,(8>)+,\n",
      "• Mixed patterns: e.g., ,1>/(8>)+/(9>)+,\n",
      "• Label unions: e.g., ,(0>|1>|2>|3>|4>)+,\n",
      "• Bound\n",
      "source\n",
      "queries:\n",
      "e.g.,\n",
      "200,1>/8>,,\n",
      "227,(1>)+/(8>)+,\n",
      "\n",
      "to length 5)\n",
      "• Kleene closures: e.g., ,(1>)+,, ,(8>)+,\n",
      "• Mixed patterns: e.g., ,1>/(8>)+/(9>)+,\n",
      "• Label unions: e.g., ,(0>|1>|2>|3>|4>)+,\n",
      "• Bound\n",
      "source\n",
      "queries:\n",
      "e.g.,\n",
      "200,1>/8>,,\n",
      "227,(1>)+/(8>)+,\n",
      "The scoring metric for this project emphasizes accuracy on\n",
      "real workloads with a weight of two while also penalizing\n",
      "preparation and estimation time. This guided our strategy: we\n",
      "needed accurate estimates without expensive runtime compu-\n",
      "tation.\n",
      "4.1.2\n",
      "Technique Selection Based on Liter-\n",
      "ature Survey\n",
      "Based on our literature review, we selected the following\n",
      "approaches for our hybrid estimation model:\n",
      "• End-biased Sampling: This strategy was chosen in\n",
      "order to address the variation brought on by power-law\n",
      "distributions. We reasoned that probabilistic estimate\n",
      "is not appropriate for frequent values (hubs), since they\n",
      "have the greatest influence on join size and variance. We\n",
      "eliminate the main cause of instability in the estimator\n",
      "by explicitly isolating and tracking the top-K highest-\n",
      "\n",
      "have the greatest influence on join size and variance. We\n",
      "eliminate the main cause of instability in the estimator\n",
      "by explicitly isolating and tracking the top-K highest-\n",
      "degree nodes using exact logic, leaving a lower-variance\n",
      "population (the tail) that is easier to sample.\n",
      "• Stratified Reservoir Sampling: Rare edge labels are\n",
      "not guaranteed to be covered by a global random sam-\n",
      "ple. To make sure that every potential transition in the\n",
      "graph has representative start nodes, we decided to use\n",
      "stratified reservoirs, which allocate a predetermined sam-\n",
      "ple budget per unique label. In order to reduce endless\n",
      "failures on queries with rare predicates, this choice was\n",
      "crucial.\n",
      "• Correlated Sampling by Hashing: Since independent\n",
      "random sampling statistically fails to capture the inter-\n",
      "section of joins in sparse samples, we ignored it for the\n",
      "tail. We make sure that sample choices are consistent\n",
      "across various query contexts by using a deterministic\n",
      "\n",
      "section of joins in sparse samples, we ignored it for the\n",
      "tail. We make sure that sample choices are consistent\n",
      "across various query contexts by using a deterministic\n",
      "hashing approach as suggested by Estan et al. given as\n",
      "follows:\n",
      "h(v) ≤pv\n",
      "(1)\n",
      "where pV is the sampling probability for a given value\n",
      "of v and h is the hashing function which maps values in\n",
      "the range [0,1].\n",
      "• XSEED-based Seed Simulation: In clustered graphs,\n",
      "the assumption of independence between path steps is\n",
      "broken by standard histograms. The idea of employing\n",
      "bound nodes as ”seeds” for cardinality estimation is pre-\n",
      "sented in the XSEED paper [11]. We incorporated this\n",
      "idea into our Sampling Model, treating our Stratified\n",
      "Samples as ”seeds” and executing the query path from\n",
      "them. This enables us to capture the structural relation-\n",
      "ships in the graph that are missed by static statistical\n",
      "methods.\n",
      "We decided to pursue with a Hybrid Cardinality Estimator\n",
      "that adjusts to the structural skew of the graph by combining\n",
      "\n",
      "ships in the graph that are missed by static statistical\n",
      "methods.\n",
      "We decided to pursue with a Hybrid Cardinality Estimator\n",
      "that adjusts to the structural skew of the graph by combining\n",
      "the above mentioned approaches. By dynamically replicat-\n",
      "ing graph traversals on a subgraph, this synergy enables the\n",
      "system to get around the drawbacks of static statistics and\n",
      "balance the accuracy with the runtime efficiency of sampling.\n",
      "4.1.3\n",
      "Expected Challenges\n",
      "We anticipated several challenges:\n",
      "• Balancing accuracy and speed: More sophisticated\n",
      "statistics and complex simulations improve accuracy\n",
      "but increase both preparation and estimation time. In\n",
      "\n",
      "addition, reservoir sizes and the estimation time penalty\n",
      "had to be carefully balanced. We anticipated that wander\n",
      "join approaches, which use random walks to estimate\n",
      "join cardinalities, would greatly benefit accuracy but\n",
      "adversely affect estimation time.\n",
      "• Handling Kleene closures: Single predicate queries\n",
      "like 1 and concatenation queries like 1 /2 were straight-\n",
      "forward and could be directly computed from our stored\n",
      "label statistics and join synopses. However, transitive\n",
      "closure queries can produce result sets ranging from\n",
      "empty to quadratic in graph size, making estimation\n",
      "inherently difficult.\n",
      "• Correlation capture: Independence assumptions be-\n",
      "tween path segments would lead to systematic over or\n",
      "under estimation, but capturing accurate and extensive\n",
      "correlations would require exhaustive precomputations.\n",
      "• Memory constraints: Storing detailed statistics for all\n",
      "label pairs (pairwise correlations) would scale quadrati-\n",
      "cally with the number of labels in the graph.\n",
      "4.2\n",
      "\n",
      "• Memory constraints: Storing detailed statistics for all\n",
      "label pairs (pairwise correlations) would scale quadrati-\n",
      "cally with the number of labels in the graph.\n",
      "4.2\n",
      "Implementation Progress and Results\n",
      "We used an iterative life cycle for our implementation. Before\n",
      "finalizing our ultimate End-Biased Stratified architecture, we\n",
      "ran our experiments with a number of approaches, including\n",
      "a computationally costly Random Walk attempt.\n",
      "4.2.1\n",
      "Iteration 1: Basic Synopsis\n",
      "We began by implementing foundational statistics computed\n",
      "in a single pass over the graph. This included edge counts\n",
      "per label and a matrix to store pairwise correlations between\n",
      "labels.\n",
      "By adding up the product of degrees, we were\n",
      "able to determine the precise number of pathways for any\n",
      "length-2 label combination. A join statistics matrix was also\n",
      "introduced to capture the number of two-hop paths between\n",
      "all label pairs, providing exact estimates for length-2 queries.\n",
      "Result:\n",
      "This baseline achieved reasonable accuracy\n",
      "\n",
      "introduced to capture the number of two-hop paths between\n",
      "all label pairs, providing exact estimates for length-2 queries.\n",
      "Result:\n",
      "This baseline achieved reasonable accuracy\n",
      "on simple queries but struggled with complex patterns and\n",
      "showed high variance on longer paths.\n",
      "4.2.2\n",
      "Iteration 2: Wander Joins and Ran-\n",
      "dom Walks\n",
      "We implemented a Wander Join estimator to increase accuracy\n",
      "for Kleene closures and long pathways. In order to naturally\n",
      "approximate the path distribution for each query, we con-\n",
      "ducted random walks beginning from a collection of sampled\n",
      "nodes. To further refine this approach, we explored several ad-\n",
      "vanced adaptive sampling techniques: the Filtering-Sampling\n",
      "Approach (FaSTest), that dynamically adjusted sample sizes\n",
      "based on query characteristics, along with an adaptive stop-\n",
      "ping criteria that terminated walks once the 95% confidence\n",
      "interval around the Horvitz-Thompson estimate fell below a\n",
      "5% error threshold, thereby saving time on dense queries.\n",
      "\n",
      "ping criteria that terminated walks once the 95% confidence\n",
      "interval around the Horvitz-Thompson estimate fell below a\n",
      "5% error threshold, thereby saving time on dense queries.\n",
      "Result: Since this method physically traversed the graph\n",
      "structure to capture correlations, this method produced re-\n",
      "markable accuracy, however estimation time (Test) grew sig-\n",
      "nificantly. Even with our optimizations, a large number of\n",
      "walks were required in order to get stable estimates, which\n",
      "went against the time constraints set up for this project. In-\n",
      "stead, we opted for a pre calculated sampling strategy.\n",
      "4.2.3\n",
      "Iteration 3: Hybrid Approach\n",
      "Our estimating model was developed through an ordered\n",
      "breakdown of the problem rather than being chosen as a sin-\n",
      "gle technique. In order to address particular data features, we\n",
      "integrated various approaches in three logical steps.\n",
      "Step 1: Decomposition by Degree The existence of high-\n",
      "degree nodes is the main cause of errors in graph estima-\n",
      "\n",
      "integrated various approaches in three logical steps.\n",
      "Step 1: Decomposition by Degree The existence of high-\n",
      "degree nodes is the main cause of errors in graph estima-\n",
      "tion. We developed a split-execution model based on the End-\n",
      "Biased Sampling concepts. We proposed treating the graph\n",
      "G as Ghead and Gtail, into two separate subgraphs. Since any\n",
      "probabilistic variance here would propagate quadratically dur-\n",
      "ing joins, we concluded that Ghead (the top-K nodes) must\n",
      "be effectively retained in memory and queried accurately. In\n",
      "Gtail, where the variance is inherently lower, the probabilistic\n",
      "estimation is strongly restricted.\n",
      "Step 2: Handling Sparsity Standard sampling is ineffec-\n",
      "tive for rare predicates in path queries, despite Estan et al.’s\n",
      "suggestion to weight by frequency. A global sample will be\n",
      "unable to capture a label L if it is present in only 0.1% of the\n",
      "graph. The sampling logic was expanded to be stratified. Each\n",
      "edge label is treated as a separate population strata. Regard-\n",
      "\n",
      "unable to capture a label L if it is present in only 0.1% of the\n",
      "graph. The sampling logic was expanded to be stratified. Each\n",
      "edge label is treated as a separate population strata. Regard-\n",
      "less of how uncommon the edge label is, we mathematically\n",
      "guarantee that the probability of providing a ”zero-estimate”\n",
      "for a valid query path is reduced by guaranteeing a constant\n",
      "reservoir size for each label.\n",
      "Step 3: Correlation Preservation In essence, a path query\n",
      "X →Y →Z is an intersection problem. The fundamental cor-\n",
      "relations needed to compute these crossings are eliminated by\n",
      "independent random sampling. We decided to use Correlated\n",
      "Hashing. In case, a node exists in the reservoir for label A (as\n",
      "a target), it will also be present in the reservoir for label B (as\n",
      "a source), as long as it satisfies the probability threshold, by\n",
      "making the inclusion of a node into a reservoir a deterministic\n",
      "function of its ID (h(v) ≤pv). This maintains the linkability.\n",
      "\n",
      "a source), as long as it satisfies the probability threshold, by\n",
      "making the inclusion of a node into a reservoir a deterministic\n",
      "function of its ID (h(v) ≤pv). This maintains the linkability.\n",
      "Step 4: Simulation-Based Estimation Finally, we used the\n",
      "Seed-Based approach from the XSEED paper to solve for\n",
      "complex query types that defy static formulas, such as Kleene\n",
      "closures (+). In addition to being statistical representatives,\n",
      "we thought of our stratified samples as ”active seeds.” Rather\n",
      "than merging histograms, the estimator measures reachability\n",
      "by physically running the query from these seeds and scal-\n",
      "ing the outcome by the inverse of the inclusion probability\n",
      "(1/pv).\n",
      "Result: This approach achieved the best trade-off between\n",
      "accuracy and estimation time on simple as well as complex\n",
      "queries. queries. The only drawback of this approach was that\n",
      "it occupied significantly more memory, which was further\n",
      "\n",
      "refined in the next iteration.\n",
      "4.2.4\n",
      "Iteration 4: Optimization\n",
      "The final iteration focused on performance optimization with-\n",
      "out sacrificing on the accuracy. This iteration was done in\n",
      "order to reduce Estimation Time (Test) and Preparation Time\n",
      "(Tprep).\n",
      "• Binary Search for Adjacency: It was inefficient to\n",
      "search adjacency lists linearly for edges that matched a\n",
      "given start label for the high-degree top-k nodes. Using\n",
      "binary search on the sorted edge lists, we optimized the\n",
      "sampling algorithm. This resulted in a targeted speedup\n",
      "for the most frequently used nodes by lowering the edge\n",
      "lookup difficulty from linear to logarithmic.\n",
      "• Memory Reuse: Our code initially generated fresh tem-\n",
      "porary lists for each node it handled during the prepa-\n",
      "ration stage. This required more memory and was slow\n",
      "due to large number of nodes. These list creations were\n",
      "relocated outside of the main loop. The same memory\n",
      "block is reused repeatedly since we only need to gener-\n",
      "\n",
      "due to large number of nodes. These list creations were\n",
      "relocated outside of the main loop. The same memory\n",
      "block is reused repeatedly since we only need to gener-\n",
      "ate the list once and then clear it after processing each\n",
      "node. As a result, the overhead of continuous memory\n",
      "allocation was significantly reduced.\n",
      "4.2.5\n",
      "Results\n",
      "Our final hybrid estimator shows a good trade-off between\n",
      "runtime and estimation accuracy. The leaderboard’s perfor-\n",
      "mance metrics are as follows\n",
      "Table 1: Final Leaderboard Performance\n",
      "Metric\n",
      "Value\n",
      "Discord Username\n",
      "viper 101\n",
      "Synthetic Accuracy (Eqsyn)\n",
      "1.18\n",
      "Real-Workload Accuracy (Eqreal)\n",
      "8.86\n",
      "Preparation Time (Tprep)\n",
      "153.88 ms\n",
      "Estimation Time (Test)\n",
      "32.58 ms\n",
      "Peak Memory Usage (M)\n",
      "24.07 MB\n",
      "Final Score\n",
      "49.63\n",
      "5\n",
      "PART 2: EFFICIENT QUERY EVALUATION\n",
      "In Part 2 of the project, we shift focus from cardinality es-\n",
      "timation to efficient query evaluation. The goal is to make\n",
      "Quicksilver SQUIFfy: Smart, Quick, and Frugal. We focused\n",
      "\n",
      "In Part 2 of the project, we shift focus from cardinality es-\n",
      "timation to efficient query evaluation. The goal is to make\n",
      "Quicksilver SQUIFfy: Smart, Quick, and Frugal. We focused\n",
      "heavily on balancing the trade-offs between execution speed,\n",
      "preparation time, and strict memory limits. We didn’t just\n",
      "want it to be fast; we wanted it to be robust and memory\n",
      "efficient as per Frugal requirement.\n",
      "5.1\n",
      "Strategy\n",
      "Our optimization strategy followed a systematic approach\n",
      "inspired by research into high-performance graph processing\n",
      "systems. We categorized our efforts into three pillars: Smart\n",
      "(choosing optimal algorithms), Quick (reducing computation\n",
      "time), and Frugal (minimizing memory overhead).\n",
      "5.1.1\n",
      "Smart\n",
      "We made informed decisions based on query patterns and\n",
      "empirical benchmarking:\n",
      "• Separate Bound Source/Target Handlers: We used\n",
      "forward BFS traversal for queries with bound sources\n",
      "and backward BFS from the target for queries with\n",
      "bound targets. This method helped us to avoid full-graph\n",
      "\n",
      "forward BFS traversal for queries with bound sources\n",
      "and backward BFS from the target for queries with\n",
      "bound targets. This method helped us to avoid full-graph\n",
      "traversals and significantly reduced the search space.\n",
      "• Node ID Mapping: We maintained a translation table\n",
      "to map the source and target node IDs from original to\n",
      "reordered IDs at evaluation time so that when the graph\n",
      "is reordered (discussed in Quick) and nodes get new\n",
      "IDs, the evaluator continues work with consistent node\n",
      "identifiers.\n",
      "5.1.2\n",
      "Quick: Reducing Computation Time\n",
      "Here our primary focus was eliminating redundant work and\n",
      "improving cache efficiency:\n",
      "• Label-indexed CSR: We pre-partition edges by label\n",
      "during graph loading instead of filtering them during\n",
      "BFS, which eliminated the label check from the hottest\n",
      "code path. This reduced branch mispredictions and im-\n",
      "proved instruction level parallelism.\n",
      "• Graph Reordering: We implemented BFS based node\n",
      "ID reassignment, starting from the highest-degree node.\n",
      "\n",
      "code path. This reduced branch mispredictions and im-\n",
      "proved instruction level parallelism.\n",
      "• Graph Reordering: We implemented BFS based node\n",
      "ID reassignment, starting from the highest-degree node.\n",
      "This approach assigns consecutive IDs to the nodes vis-\n",
      "ited together during traversal, eventually improving spa-\n",
      "tial locality when accessing adjacency data in the CSR\n",
      "arrays.\n",
      "• Tracking visited nodes using timestamp: Rather than\n",
      "clearing the visited array between queries (O(V) oper-\n",
      "ation), we used a monotonically increasing timestamp\n",
      "counter to indicate the status of a node. We marked a\n",
      "node with the current timestamp when visiting it, and\n",
      "the node is considered unvisited if its timestamp doesn’t\n",
      "match the current query’s timestamp (O(1) operation).\n",
      "5.1.3\n",
      "Frugal: Memory Efficiency\n",
      "Here we focused on minimizing dynamic memory operations\n",
      "because memory allocation and deallocation are expensive\n",
      "operations.\n",
      "• Memory Pooling: All vectors used during BFS traversal\n",
      "\n",
      "Here we focused on minimizing dynamic memory operations\n",
      "because memory allocation and deallocation are expensive\n",
      "operations.\n",
      "• Memory Pooling: All vectors used during BFS traversal\n",
      "were pre-allocated once during the prepare phase and\n",
      "reused across all queries via swap() operations rather\n",
      "than repeated allocation and deallocation.\n",
      "• CSR Format: We converted adjacency lists to Com-\n",
      "pressed Sparse Row format, storing edges in contiguous\n",
      "arrays indexed by node ID, to reduce memory fragmen-\n",
      "tation and improve cache utilization.\n",
      "\n",
      "5.2\n",
      "Literature Survey\n",
      "Work on fast graph query evaluation repeatedly shows that\n",
      "performance depends less on high-level query language rules\n",
      "and more on how traversal is executed: reducing wasted\n",
      "exploration, using cache-friendly graph layouts, and avoiding\n",
      "overhead inside the traversal loop. Shared-memory graph\n",
      "systems and RDF engines especially emphasize contiguous\n",
      "adjacency storage, locality-aware preprocessing, and access-\n",
      "path choices that prevent repeated filtering during evaluation.\n",
      "In direction-optimizing BFS, Beamer et al. show that\n",
      "switching between traversal styles can dramatically reduce\n",
      "the number of edges examined on real “small-world” graphs,\n",
      "which is exactly the problem when a naive traversal expands\n",
      "too broadly. Their key message is that how you traverse (direc-\n",
      "tion/strategy decisions) can matter as much as raw low-level\n",
      "optimization, because it directly shrinks the explored search\n",
      "space [2]. Moreover, when graph preprocessing changes ver-\n",
      "\n",
      "tion/strategy decisions) can matter as much as raw low-level\n",
      "optimization, because it directly shrinks the explored search\n",
      "space [2]. Moreover, when graph preprocessing changes ver-\n",
      "tex IDs (for example after reordering), systems typically need\n",
      "a stable way to preserve semantics across representations.\n",
      "In the graph ordering work by Wei et al., the goal is to re-\n",
      "arrange vertex order to improve cache behavior. However,\n",
      "this implicitly creates a “logical graph” (original IDs) and a\n",
      "“physical graph” (reordered IDs). The paper motivates the\n",
      "practical need to keep query evaluation correct even when\n",
      "the physical layout changes — conceptually the same reason\n",
      "many systems maintain an ID mapping layer around reordered\n",
      "graphs [9].\n",
      "For traversal-heavy workloads, research shows that perfor-\n",
      "mance is often memory-bound, so the fastest solutions focus\n",
      "on contiguous layouts and minimizing work in the tight loop.\n",
      "Ligra is a well-known shared-memory framework demon-\n",
      "\n",
      "mance is often memory-bound, so the fastest solutions focus\n",
      "on contiguous layouts and minimizing work in the tight loop.\n",
      "Ligra is a well-known shared-memory framework demon-\n",
      "strating that graph traversal becomes significantly faster when\n",
      "adjacency is stored and processed in cache-friendly ways,\n",
      "enabling efficient neighbor scans and reducing overhead from\n",
      "pointer-heavy structures. Although Ligra is a framework, its\n",
      "results strongly support the idea that CSR-like representations\n",
      "and streamlined traversal primitives are a practical baseline\n",
      "for high throughput BFS-style processing [8]. A second line\n",
      "of work targets locality directly via graph reordering. Wei et\n",
      "al. show that reordering vertices can reduce CPU cache miss\n",
      "ratios across several graph algorithms, producing speedups\n",
      "without changing the underlying algorithm. Their contribu-\n",
      "tion is important for query evaluation because it treats graph\n",
      "layout as an optimization target: by reassigning IDs so that\n",
      "\n",
      "without changing the underlying algorithm. Their contribu-\n",
      "tion is important for query evaluation because it treats graph\n",
      "layout as an optimization target: by reassigning IDs so that\n",
      "vertices accessed close in time are also close in memory, the\n",
      "cost of adjacency and metadata access drops [9]. Visited-\n",
      "state handling is another recurring bottleneck in fast traversal.\n",
      "Beamer et al. discuss optimizations such as using compact\n",
      "visited representations (for example bitmaps) to reduce costly\n",
      "random accesses and keep visited checks efficient on shared-\n",
      "memory machines. The point is that even “small” bookkeep-\n",
      "ing operations can dominate runtime when done at scale, so\n",
      "visited tracking must be engineered carefully [2]. A large\n",
      "part of the memory efficiency in graph query engines comes\n",
      "from keeping data structures compact and avoiding repeated\n",
      "allocations during query execution. Many systems therefore\n",
      "rely on two practical ideas such as storing adjacency in a con-\n",
      "\n",
      "from keeping data structures compact and avoiding repeated\n",
      "allocations during query execution. Many systems therefore\n",
      "rely on two practical ideas such as storing adjacency in a con-\n",
      "tiguous sparse format (typically CSR), and pre-allocating and\n",
      "reusing working buffers instead of allocating/freeing mem-\n",
      "ory for every query. Several systems make this point even\n",
      "more explicitly by standardizing around CSR as the baseline\n",
      "representation due to its compactness and predictable access\n",
      "pattern. Dhulipala et al. (GBBS) state that graphs in the suite\n",
      "are stored in Compressed Sparse Row (CSR), where neigh-\n",
      "bor lists live in contiguous arrays with an offset array per\n",
      "node. This design reduces fragmentation and supports cache-\n",
      "friendly scans. The key motivation is that pointer-heavy adja-\n",
      "cency lists create overhead and irregular access, while CSR\n",
      "improves both memory footprint and traversal throughput [4].\n",
      "Many high-performance systems reduce memory overhead\n",
      "\n",
      "cency lists create overhead and irregular access, while CSR\n",
      "improves both memory footprint and traversal throughput [4].\n",
      "Many high-performance systems reduce memory overhead\n",
      "by minimizing dynamic allocations during execution. Winter\n",
      "et al. (faimGraph) present a memory-management approach\n",
      "designed for fully-dynamic graphs on the GPU. While faim-\n",
      "Graph is GPU-focused, its core principle—using one large\n",
      "pre-allocation to avoid the overhead and fragmentation of\n",
      "repeated memory allocation or vector resizing—maps cleanly\n",
      "to CPU implementations. We adopted this by pre-allocating\n",
      "our working buffers once to keep runtime memory behavior\n",
      "stable. [10].\n",
      "5.3\n",
      "Implementation Progress\n",
      "We followed an iterative approach for optimizing our per-\n",
      "formance, implementing and benchmarking each technique\n",
      "independently to measure its overall impact. In the sections\n",
      "below, we explain each optimization iteration experiments.\n",
      "We also report the performance metrics achieved in each itera-\n",
      "\n",
      "independently to measure its overall impact. In the sections\n",
      "below, we explain each optimization iteration experiments.\n",
      "We also report the performance metrics achieved in each itera-\n",
      "tion, along with a improvement percentage from the previous\n",
      "iteration (represented by ∆). A positive value indicates im-\n",
      "provement in the performance parameter whereas a negative\n",
      "value represents decrease in the performance.\n",
      "5.3.1\n",
      "Iteration 1: Baseline Analysis\n",
      "The baseline implementation used standard adjacency lists\n",
      "with vectors of (label, target) pairs. Each query allocated\n",
      "fresh vectors for BFS traversal and used a boolean visited\n",
      "array that was cleared between queries. The simple evaluator\n",
      "performed linear scans through the adjacency lists, checking\n",
      "each edge’s label against the query predicate.\n",
      "Results:\n",
      "Profiling revealed several bottlenecks:\n",
      "re-\n",
      "peated memory allocation for each query, O(V) cost to clear\n",
      "the visited array, poor cache locality due to scattered memory\n",
      "\n",
      "Results:\n",
      "Profiling revealed several bottlenecks:\n",
      "re-\n",
      "peated memory allocation for each query, O(V) cost to clear\n",
      "the visited array, poor cache locality due to scattered memory\n",
      "access, and redundant label checks in the inner traversal loop.\n",
      "5.3.2\n",
      "Iteration 2: Bound-Aware Evaluation\n",
      "and Semi-Naive Transitive Closure\n",
      "After the experiments on our baseline, w e found that queries\n",
      "with bound sources or targets continued to unnecessarily tra-\n",
      "verse the whole graph. Furthermore, Kleene star searches\n",
      "\n",
      "Table 2: Iteration 1: Baseline Performance\n",
      "Metric\n",
      "Value\n",
      "Preparation Time (Tprep)\n",
      "145.87 ms\n",
      "Evaluation Time T syn\n",
      "eval\n",
      "1213.70 ms\n",
      "Evaluation Time T real\n",
      "eval\n",
      "17993.72 ms\n",
      "Load Time (Tload)\n",
      "3985.82 ms\n",
      "Peak Memory Usage (Meval)\n",
      "29.48 MB\n",
      "Score\n",
      "∼19,800\n",
      "were creating duplicate work by recalculating every reachable\n",
      "node at each cycle.\n",
      "• Bound-Aware Evaluation: We implemented separate\n",
      "independent evaluation handlers for queries with bound\n",
      "sources and bound targets. Instead of traversing the\n",
      "entire graph, we start a breadth first search from the\n",
      "single known node, significantly reducing the search\n",
      "space for constrained queries.\n",
      "• Semi-Naive Transitive Closure: We replaced the naive\n",
      "transitive closure algorithm with a semi-naive approach\n",
      "for Kleene star queries. Instead of recalculating all\n",
      "reachable nodes at each iteration, we process only newly\n",
      "discovered edges, avoiding redundant computation and\n",
      "significantly improving performance for recursive path\n",
      "patterns.\n",
      "\n",
      "reachable nodes at each iteration, we process only newly\n",
      "discovered edges, avoiding redundant computation and\n",
      "significantly improving performance for recursive path\n",
      "patterns.\n",
      "Results: The score improved from ˜19,800 to 7,248 (63%\n",
      "improvement). This confirmed that algorithmic issues were\n",
      "the major bottleneck. However, this also revealed that signifi-\n",
      "cant time was still being spent on constructing and managing\n",
      "intermediate result graphs, hinting us towards data structure\n",
      "optimizations.\n",
      "Table 3: Iteration 2: Bound-Aware Evaluation\n",
      "Metric\n",
      "Value\n",
      "∆\n",
      "Preparation Time (Tprep)\n",
      "1505.31 ms\n",
      "−932%\n",
      "Eval Time Syn (T syn\n",
      "eval)\n",
      "341.26 ms\n",
      "+72%\n",
      "Eval Time Real (T real\n",
      "eval )\n",
      "6403.16 ms\n",
      "+64%\n",
      "Load Time (Tload)\n",
      "4092.68 ms\n",
      "−3%\n",
      "Peak Memory (Meval)\n",
      "24.87 MB\n",
      "+16%\n",
      "Score\n",
      "7,248\n",
      "+63%\n",
      "5.3.3\n",
      "Iteration 3: Compact Intermediate\n",
      "Results\n",
      "After the algorithmic changes were implemented, profiling\n",
      "showed that creating and maintaining intermediate result\n",
      "graphs took a considerable amount of time. We observed\n",
      "\n",
      "Results\n",
      "After the algorithmic changes were implemented, profiling\n",
      "showed that creating and maintaining intermediate result\n",
      "graphs took a considerable amount of time. We observed\n",
      "that these intermediary structures kept duplicate data: reverse\n",
      "adjacency lists were kept even if joins only traveled forward,\n",
      "and labels were kept even though edges had already been\n",
      "filtered. To subside these issues, we used the following ap-\n",
      "proaches:\n",
      "• CompactIR Data Structure:\n",
      "We introduced a\n",
      "lightweight intermediate result structure that stores only\n",
      "source-to-target mappings. We removed labels from in-\n",
      "termediate results since once edges are filtered by label\n",
      "during selection, storing the label becomes redundant.\n",
      "We also eliminated the reverse adjacency list because\n",
      "join operations only traverse forward, never requiring\n",
      "backward lookups on intermediate data.\n",
      "• Deferred Deduplication: We also removed per-insert\n",
      "duplicate checking, which required expensive lookups\n",
      "\n",
      "join operations only traverse forward, never requiring\n",
      "backward lookups on intermediate data.\n",
      "• Deferred Deduplication: We also removed per-insert\n",
      "duplicate checking, which required expensive lookups\n",
      "on every edge addition. Instead, we allowed duplicates\n",
      "during construction and performed a batch sort-and-\n",
      "deduplicate operation at the end by using finalize().\n",
      "This approach is more cache-friendly and significantly\n",
      "reduces computation time.\n",
      "• Hash-based Existence Checking: For transitive closure\n",
      "computation, we replaced linear search with hash sets\n",
      "for tracking visited edges, reducing existence checks\n",
      "from O(N) to O(1) and speeding up Kleene star queries.\n",
      "Result: The score improved from 7,248 to 4,255 (41% im-\n",
      "provement). Memory usage and computation time were de-\n",
      "creased by eliminating redundant storage and delaying dedu-\n",
      "plication.\n",
      "Table 4: Iteration 3: Compact Intermediate Results\n",
      "Metric\n",
      "Value\n",
      "∆\n",
      "Preparation Time (Tprep)\n",
      "1476.81 ms\n",
      "+2%\n",
      "Eval Time Syn (T syn\n",
      "eval)\n",
      "212.29 ms\n",
      "+38%\n",
      "\n",
      "plication.\n",
      "Table 4: Iteration 3: Compact Intermediate Results\n",
      "Metric\n",
      "Value\n",
      "∆\n",
      "Preparation Time (Tprep)\n",
      "1476.81 ms\n",
      "+2%\n",
      "Eval Time Syn (T syn\n",
      "eval)\n",
      "212.29 ms\n",
      "+38%\n",
      "Eval Time Real (T real\n",
      "eval )\n",
      "3828.77 ms\n",
      "+40%\n",
      "Load Time (Tload)\n",
      "3911.26 ms\n",
      "+4%\n",
      "Peak Memory (Meval)\n",
      "10.40 MB\n",
      "+58%\n",
      "Score\n",
      "4,255\n",
      "+41%\n",
      "5.3.4\n",
      "Iteration 4: Direct BFS Traversal\n",
      "In the previous iteration, our approach continued to materi-\n",
      "alize intermediate result graphs and carry out explicit join\n",
      "operations in spite of the CompactIR enhancements. Since\n",
      "we only want the final reachable nodes for path queries, not\n",
      "the intermediary edges, we thought if this abstraction was\n",
      "even essential. As a result, we completely stopped using\n",
      "intermediate graph building.\n",
      "• Removed Intermediate Graph Construction: We\n",
      "eliminated the CompactIR data structure and join op-\n",
      "erations entirely. Instead of building intermediate result\n",
      "graphs and joining them, we now traverse the query path\n",
      "directly while maintaining only the current frontier. This\n",
      "\n",
      "erations entirely. Instead of building intermediate result\n",
      "graphs and joining them, we now traverse the query path\n",
      "directly while maintaining only the current frontier. This\n",
      "avoids materializing all intermediate edges, significantly\n",
      "reducing memory usage and computation time.\n",
      "• Specialized Evaluation Paths: We implemented three\n",
      "separate evaluation functions: forward BFS for bound-\n",
      "source queries, backward BFS for bound-target queries,\n",
      "\n",
      "and multi-source traversal for unbound queries. Each\n",
      "path is optimized for its specific access pattern.\n",
      "• Bitvector-based Tracking: We replaced hash sets with\n",
      "bitvectors for tracking frontier membership and visited\n",
      "nodes. For dense node ID spaces, bitvectors provide\n",
      "O(1) lookups with better cache locality than hash-based\n",
      "structures.\n",
      "• Label-indexed Edge Lists: Precomputed lists were\n",
      "added that provide direct access to all edges with a given\n",
      "label, eliminating the need to scan entire adjacency lists\n",
      "and filter by label during unbound evaluation.\n",
      "• Memory Optimizations: vector pre-reservation were\n",
      "added to reduce reallocations during BFS and used\n",
      "move semantics to avoid unnecessary vector copies when\n",
      "swapping frontiers.\n",
      "Result: The score improved from 4,255 to 2,063 (52%\n",
      "improvement). Eliminating intermediate graph construction\n",
      "was a major win, confirming that the join-based abstraction\n",
      "was unnecessary overhead. However, experiments showed\n",
      "\n",
      "improvement). Eliminating intermediate graph construction\n",
      "was a major win, confirming that the join-based abstraction\n",
      "was unnecessary overhead. However, experiments showed\n",
      "a new bottleneck: the unbound evaluation was allocating\n",
      "thousands of bitvectors per query, one for each active source.\n",
      "Table 5: Iteration 4: Direct BFS Traversal\n",
      "Metric\n",
      "Value\n",
      "∆\n",
      "Preparation Time (Tprep)\n",
      "1451.41 ms\n",
      "+2%\n",
      "Eval Time Syn (T syn\n",
      "eval)\n",
      "55.70 ms\n",
      "+74%\n",
      "Eval Time Real (T real\n",
      "eval )\n",
      "1896.07 ms\n",
      "+50%\n",
      "Load Time (Tload)\n",
      "4456.80 ms\n",
      "−14%\n",
      "Peak Memory (Meval)\n",
      "5.20 MB\n",
      "+50%\n",
      "Score\n",
      "2,063\n",
      "+52%\n",
      "5.3.5\n",
      "Iteration 5: Timestamp-based Vis-\n",
      "ited Tracking\n",
      "In the previous iteration, direct BFS implementation showed a\n",
      "new bottleneck: in evaluateUnbound(), we were allocating\n",
      "a fresh vector<bool>(V) bitvector for each active source\n",
      "to track visited nodes. With thousands of active sources per\n",
      "query, this meant thousands of O(V) allocations which caused\n",
      "a significant overhead on the query time.\n",
      "\n",
      "to track visited nodes. With thousands of active sources per\n",
      "query, this meant thousands of O(V) allocations which caused\n",
      "a significant overhead on the query time.\n",
      "• Removed Per-Source Bitvector Allocation: In the pre-\n",
      "vious iteration, the unbound evaluation allocated a sepa-\n",
      "rate vector<bool>(V) for each active source to track\n",
      "visited nodes. With thousands of active sources, this\n",
      "caused thousands of memory allocations per query, cre-\n",
      "ating significant overhead.\n",
      "• Shared Timestamp Array: We replaced per-source\n",
      "bitvectors with a single shared visitedTime array.\n",
      "Each source receives a unique timestamp via an incre-\n",
      "menting counter, and a node is considered visited if its\n",
      "timestamp matches the current value. This eliminates all\n",
      "per-source allocations and avoids the need to clear the\n",
      "array between sources, with the timestamp automatically\n",
      "invalidating previous visits.\n",
      "• Graph Finalization: We added a finalizeGraph()\n",
      "function called once after loading that sorts all adja-\n",
      "\n",
      "array between sources, with the timestamp automatically\n",
      "invalidating previous visits.\n",
      "• Graph Finalization: We added a finalizeGraph()\n",
      "function called once after loading that sorts all adja-\n",
      "cency lists by label and sorts label edge lists by source.\n",
      "This ensures edges with the same label are contiguous\n",
      "in memory, improving cache locality during traversal,\n",
      "and improves sequential access patterns during unbound\n",
      "evaluation.\n",
      "Result: The score improved from 2,063 to 1,395 (32% im-\n",
      "provement). The shared timestamp technique eliminated thou-\n",
      "sands of allocations per query with minimal overhead. With\n",
      "allocation bottlenecks resolved, experiments now showed that\n",
      "cache misses during graph traversal had become the major\n",
      "cost, leading us toward memory layout optimizations.\n",
      "Table 6: Iteration 5: Timestamp-based Visited Tracking\n",
      "Metric\n",
      "Value\n",
      "∆\n",
      "Preparation Time (Tprep)\n",
      "1469.60 ms\n",
      "−1%\n",
      "Eval Time Syn (T syn\n",
      "eval)\n",
      "50.18 ms\n",
      "+10%\n",
      "Eval Time Real (T real\n",
      "eval )\n",
      "1238.07 ms\n",
      "+35%\n",
      "Load Time (Tload)\n",
      "4428.61 ms\n",
      "\n",
      "Metric\n",
      "Value\n",
      "∆\n",
      "Preparation Time (Tprep)\n",
      "1469.60 ms\n",
      "−1%\n",
      "Eval Time Syn (T syn\n",
      "eval)\n",
      "50.18 ms\n",
      "+10%\n",
      "Eval Time Real (T real\n",
      "eval )\n",
      "1238.07 ms\n",
      "+35%\n",
      "Load Time (Tload)\n",
      "4428.61 ms\n",
      "+1%\n",
      "Peak Memory (Meval)\n",
      "5.00 MB\n",
      "+4%\n",
      "Score\n",
      "1,395\n",
      "+32%\n",
      "5.3.6\n",
      "Iteration 6: CSR Format, Graph Re-\n",
      "ordering, and Label-Indexed Struc-\n",
      "tures\n",
      "Benchmarking revealed that cache misses during graph traver-\n",
      "sal had emerged as the main bottleneck following the re-\n",
      "moval of allocation overhead. The adjacency list structure\n",
      "vector<vector<pair>> dispersed edges throughout mem-\n",
      "ory, resulting in frequent cache misses while attempting to\n",
      "access neighbors. Furthermore, nodes visited together during\n",
      "BFS frequently had distant IDs due to the arbitrary assign-\n",
      "ment of node IDs during graph loading, which resulted in\n",
      "poor spatial proximity.\n",
      "• Memory Pooling: We moved all BFS-related vectors\n",
      "(frontier, nextFrontier, visitedTime, etc.) to\n",
      "class members, pre-allocated once during prepare().\n",
      "\n",
      "poor spatial proximity.\n",
      "• Memory Pooling: We moved all BFS-related vectors\n",
      "(frontier, nextFrontier, visitedTime, etc.) to\n",
      "class members, pre-allocated once during prepare().\n",
      "Instead of allocating new vectors per query, we reuse\n",
      "them via swap() and clear(), eliminating allocation\n",
      "overhead from the query hot path.\n",
      "• Compressed Sparse Row (CSR) Format:\n",
      "We\n",
      "converted\n",
      "the\n",
      "adjacency\n",
      "list\n",
      "representation\n",
      "(vector<vector<pair>>)\n",
      "to\n",
      "CSR\n",
      "format\n",
      "with\n",
      "two contiguous arrays:\n",
      "offsets[V+1] storing the\n",
      "starting index of each node’s edges, and edges[E]\n",
      "\n",
      "Figure 1: Final architecture of the QuickSilver Query Evaluator. Iteration numbers indicate when each optimization was introduced.\n",
      "storing the edge data contiguously. This dramatically\n",
      "improves cache utilization during traversal.\n",
      "• BFS-based Graph Reordering: We implemented BFS\n",
      "based node ID reassignment, starting from the highest-\n",
      "degree node. This approach assigns consecutive IDs to\n",
      "the nodes visited together during traversal, eventually\n",
      "improving spatial locality when accessing adjacency\n",
      "data in the CSR arrays.\n",
      "• Node ID Translation: We maintained oldToNew[] and\n",
      "newToOld[] mapping arrays so that incoming queries\n",
      "using original node IDs are correctly translated to re-\n",
      "ordered IDs before evaluation.\n",
      "• Label-Indexed\n",
      "CSR:\n",
      "We\n",
      "created\n",
      "sep-\n",
      "arate\n",
      "CSR\n",
      "structures\n",
      "for\n",
      "each\n",
      "label\n",
      "(csr label offsets[label][node]\n",
      "and\n",
      "csr label targets[label][]). This eliminated the\n",
      "label comparison from the inner BFS loop entirely,\n",
      "facilitating direct iteration over the edges that are\n",
      "\n",
      "label\n",
      "(csr label offsets[label][node]\n",
      "and\n",
      "csr label targets[label][]). This eliminated the\n",
      "label comparison from the inner BFS loop entirely,\n",
      "facilitating direct iteration over the edges that are\n",
      "guaranteed to have the correct label instead of checking\n",
      "if (edge.label == targetLabel) for every edge.\n",
      "• Redundant Storage Removal: After implementing\n",
      "label-indexed CSR, the original generic CSR arrays were\n",
      "no longer used. We removed them, reducing memory\n",
      "footprint and improving cache efficiency.\n",
      "Result: Our final result was achieved when the score in-\n",
      "creased from 1,395 to 716 (49% improvement). The cache\n",
      "locality issue was fully solved by using a combination of\n",
      "label-indexed structures, graph reordering, and CSR format.\n",
      "The final results achieved after this iteration can be found\n",
      "using Table 7.\n",
      "5.4\n",
      "Results Summary\n",
      "Table 7 summarizes our complete optimization journey from\n",
      "baseline to final implementation.\n",
      "Table 7: Final Leaderboard Performance: After Iteration 6\n",
      "Metric\n",
      "Value\n",
      "\n",
      "using Table 7.\n",
      "5.4\n",
      "Results Summary\n",
      "Table 7 summarizes our complete optimization journey from\n",
      "baseline to final implementation.\n",
      "Table 7: Final Leaderboard Performance: After Iteration 6\n",
      "Metric\n",
      "Value\n",
      "Discord Username\n",
      "wah shampy\n",
      "Preparation Time (Tprep)\n",
      "1834.47 ms\n",
      "Evaluation Time T syn\n",
      "eval\n",
      "29.55 ms\n",
      "Evaluation Time T real\n",
      "eval\n",
      "564.33 ms\n",
      "Load Time (Tload)\n",
      "5053.52 ms\n",
      "Peak Memory Usage (Meval)\n",
      "5.67 MB\n",
      "Score\n",
      "716\n",
      "Table 8 summarizes our optimization journey from baseline\n",
      "to final implementation.\n",
      "5.5\n",
      "Conclusion: Part 2\n",
      "Through systematic optimization, we achieved a 94.8% reduc-\n",
      "tion in score (approximately 19,800 to 716), corresponding to\n",
      "a significant reduction in evaluation time. The most impactful\n",
      "optimizations were:\n",
      "\n",
      "Table 8: Overall Optimization Progress\n",
      "Iteration\n",
      "Type\n",
      "Score\n",
      "1. Baseline\n",
      "—\n",
      "19,800\n",
      "2. Bound-Aware Eval\n",
      "S\n",
      "7,248\n",
      "3. Compact IR\n",
      "F\n",
      "4,255\n",
      "4. Direct BFS\n",
      "S, Q\n",
      "2,063\n",
      "5. Timestamp Tracking\n",
      "Q, F\n",
      "1,395\n",
      "6. CSR + Reordering\n",
      "Q\n",
      "716\n",
      "S = Smart, Q = Quick, F = Frugal\n",
      "1. Label-indexed CSR: Eliminating the label check from\n",
      "the inner loop provided substantial gains by removing\n",
      "branch mispredictions and reducing the iteration count\n",
      "to only relevant edges.\n",
      "2. Generic CSR Format: Converting from vector-of-\n",
      "vectors to contiguous arrays dramatically improved\n",
      "cache locality and eliminated pointer chasing overhead.\n",
      "3. Memory Pooling: Pre-allocating and reusing vectors\n",
      "eliminated allocation overhead that previously domi-\n",
      "nated query processing time.\n",
      "4. Graph Reordering: BFS-based node ID reassignment\n",
      "improved spatial locality by ensuring frequently co-\n",
      "accessed nodes have nearby memory addresses.\n",
      "Failed Optimization Attempts: Our optimization jour-\n",
      "ney also included attempts that did not yield positive results,\n",
      "\n",
      "accessed nodes have nearby memory addresses.\n",
      "Failed Optimization Attempts: Our optimization jour-\n",
      "ney also included attempts that did not yield positive results,\n",
      "providing valuable lessons about the gap between theoretical\n",
      "improvements and practical performance.\n",
      "• We implemented a single-label fast path that added\n",
      "a conditional branch to bypass the label iteration loop\n",
      "when a query involved only one label. In theory, this\n",
      "should have reduced loop overhead for the majority of\n",
      "our queries which involved single labels. However, this\n",
      "branch was evaluated at every node during BFS traversal,\n",
      "and the CPU’s branch predictor struggled with the mixed\n",
      "true/false pattern, causing pipeline stalls that exceeded\n",
      "the time saved by skipping the loop.\n",
      "• We attempted frontier sorting, where we sorted the BFS\n",
      "frontier by node ID before processing each level, expect-\n",
      "ing that sequential memory access to CSR arrays would\n",
      "improve cache hit rates. While theoretically sound, the\n",
      "\n",
      "frontier by node ID before processing each level, expect-\n",
      "ing that sequential memory access to CSR arrays would\n",
      "improve cache hit rates. While theoretically sound, the\n",
      "O(n log n) sorting cost at every BFS level outweighed\n",
      "the cache benefits, particularly since our graph reorder-\n",
      "ing optimization had already improved spatial locality at\n",
      "preprocessing time, making runtime sorting redundant.\n",
      "These failures reinforced that micro-optimizations intro-\n",
      "ducing additional branches or per-level computations must be\n",
      "empirically validated, as their overhead can compound across\n",
      "millions of node visits in large-scale graph traversals.\n",
      "The final system achieves strong performance through a\n",
      "combination of memory efficiency, cache-aware data struc-\n",
      "tures, and algorithmic simplicity.\n",
      "These techniques are\n",
      "broadly applicable to graph processing systems where traver-\n",
      "sal performance is critical.\n",
      "6\n",
      "CONCLUSION\n",
      "This project addressed two fundamental challenges in graph\n",
      "\n",
      "These techniques are\n",
      "broadly applicable to graph processing systems where traver-\n",
      "sal performance is critical.\n",
      "6\n",
      "CONCLUSION\n",
      "This project addressed two fundamental challenges in graph\n",
      "database systems: cardinality estimation and efficient query\n",
      "evaluation.\n",
      "For cardinality estimation (Part 1), we developed a hybrid\n",
      "estimator combining end-biased sampling, stratified reser-\n",
      "voir sampling, correlated hashing, and XSEED-based simula-\n",
      "tion. This approach achieved competitive accuracy on both\n",
      "synthetic and real workloads while maintaining reasonable\n",
      "preparation and estimation times.\n",
      "For query evaluation (Part 2), we systematically optimized\n",
      "the Quicksilver system through memory pooling, CSR rep-\n",
      "resentations, graph reordering, and label-indexed data struc-\n",
      "tures. We learned that careful attention to memory access\n",
      "patterns and data structure design can yield dramatic perfor-\n",
      "mance gains.\n",
      "REFERENCES\n",
      "[1] B. Babcock and S. Chaudhuri. Towards a robust query optimizer: A principled\n",
      "\n",
      "patterns and data structure design can yield dramatic perfor-\n",
      "mance gains.\n",
      "REFERENCES\n",
      "[1] B. Babcock and S. Chaudhuri. Towards a robust query optimizer: A principled\n",
      "and practical approach. In Proceedings of the 2005 ACM SIGMOD International\n",
      "Conference on Management of Data, pages 119–130, Baltimore, MD, USA, 2005.\n",
      "ACM.\n",
      "[2] S. Beamer, K. Asanovic, and D. Patterson. Direction-optimizing breadth-first\n",
      "search. In Proceedings of the International Conference for High Performance\n",
      "Computing, Networking, Storage and Analysis (SC), Salt Lake City, Utah, USA,\n",
      "Nov. 2012. IEEE.\n",
      "[3] G. Cormode. Synopses for massive data: Samples, histograms, wavelets, sketches.\n",
      "Foundations and Trends in Databases, 4(1–3):1–294, 2011.\n",
      "[4] L. Dhulipala, J. Shi, T. Tseng, G. E. Blelloch, and J. Shun. The graph based\n",
      "benchmark suite (gbbs). In 3rd Joint International Workshop on Graph Data\n",
      "Management Experiences & Systems (GRADES) and Network Data Analytics\n",
      "(NDA) (GRADES-NDA’20), Portland, OR, USA, June 2020. ACM.\n",
      "\n",
      "Management Experiences & Systems (GRADES) and Network Data Analytics\n",
      "(NDA) (GRADES-NDA’20), Portland, OR, USA, June 2020. ACM.\n",
      "[5] C. Estan and J. F. Naughton. End-biased samples for join cardinality estima-\n",
      "tion. In Proceedings of the 22nd International Conference on Data Engineering\n",
      "(ICDE’06), page 20, Atlanta, GA, USA, 2006. IEEE.\n",
      "[6] F. Li, B. Wu, K. Yi, and Z. Zhao. Wander join: Online aggregation via random\n",
      "walks. In Proceedings of the 2016 ACM SIGMOD International Conference on\n",
      "Management of Data, pages 615–629, San Francisco, CA, USA, 2016. ACM.\n",
      "[7] T. Neumann and G. Moerkotte. Characteristic sets: Accurate cardinality esti-\n",
      "mation for RDF queries with multiple joins. In 2011 IEEE 27th International\n",
      "Conference on Data Engineering (ICDE 2011), pages 984–994, Hannover, Ger-\n",
      "many, Apr. 2011. IEEE.\n",
      "[8] J. Shun and G. E. Blelloch. Ligra: A lightweight graph processing framework\n",
      "for shared memory. In Proceedings of the 18th ACM SIGPLAN Symposium on\n",
      "\n",
      "many, Apr. 2011. IEEE.\n",
      "[8] J. Shun and G. E. Blelloch. Ligra: A lightweight graph processing framework\n",
      "for shared memory. In Proceedings of the 18th ACM SIGPLAN Symposium on\n",
      "Principles and Practice of Parallel Programming (PPoPP). ACM, 2013.\n",
      "[9] H. Wei, J. X. Yu, C. Lu, and X. Lin. Speedup graph processing by graph ordering.\n",
      "In Proceedings of the 2016 International Conference on Management of Data\n",
      "(SIGMOD), San Francisco, CA, USA, 2016. ACM.\n",
      "[10] M. Winter, D. Mlakar, R. Zayer, H.-P. Seidel, and M. Steinberger. faimgraph:\n",
      "High performance management of fully-dynamic graphs under tight memory\n",
      "constraints on the gpu. In Proceedings of SC18: International Conference for\n",
      "High Performance Computing, Networking, Storage and Analysis, Dallas, Texas,\n",
      "USA, Nov. 2018. IEEE.\n",
      "[11] N. Zhang, M. Ozsu, A. Aboulnaga, and I. Ilyas. Xseed: Accurate and fast\n",
      "cardinality estimation for xpath queries. In 22nd International Conference on\n",
      "Data Engineering (ICDE’06), pages 61–61, 2006.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for document in documents:\n",
    "    print(document.page_content, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd121ca2-6f2d-4451-8728-ac421449e5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd377957-99b9-4ee4-b521-580be6a77055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b94176ab-60ca-4b55-8666-edbf049db156",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b59b9a9-e645-4065-9fba-9ec15f2bf159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37d7e5b4-8f47-45fe-a746-e245579b63e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(documents=documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c97583a-13ef-4d6b-a946-83d9ee67b2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536, 58)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index.d, db.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fd310fc-59b5-4e0c-8559-ea493566f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"With cardinality on paths, the query path is split into simpler segments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec12c23-b6cb-418c-8727-85c91028963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents  = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29fc97b2-c554-4622-a5fb-31d0a3f410f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2026-02-01T15:53:53+00:00', 'source': './Data/PDFs/Report.pdf', 'file_path': './Data/PDFs/Report.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'VGTC Special Issue Paper for TVCG', 'keywords': '', 'moddate': '2026-02-01T15:53:53+00:00', 'trapped': '', 'modDate': 'D:20260201155353Z', 'creationDate': 'D:20260201155353Z', 'page': 1}, page_content='of the results produced by a query. In this project, we are\\ndealing with RDF databases that differ from the typical re-\\nlational schemas because of their structure, which leads to\\ndifferent assumptions and decisions made when estimating\\nthe cardinality of a query. In query optimisation, synopses are\\nprecomputed summaries of database data (like histograms,\\nsketches, samples) that database systems use to quickly es-\\ntimate the cost of different query execution plans. Bonifati\\net al. survey cardinality estimation techniques specifically\\ndesigned for graph query languages. The two approaches\\ndiscussed are cardinality of paths and cardinality on patterns.\\nWith cardinality on paths, the query path is split into simpler\\nsegments and predefined formulas are used for concentra-\\ntion, union and inversion are combined to estimate the result.\\nCardinality on patterns, on the other hand focuses on graph\\npatterns with characteristic sets or graph summaries. With'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2026-02-01T15:53:53+00:00', 'source': './Data/PDFs/Report.pdf', 'file_path': './Data/PDFs/Report.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'VGTC Special Issue Paper for TVCG', 'keywords': '', 'moddate': '2026-02-01T15:53:53+00:00', 'trapped': '', 'modDate': 'D:20260201155353Z', 'creationDate': 'D:20260201155353Z', 'page': 1}, page_content='tion, union and inversion are combined to estimate the result.\\nCardinality on patterns, on the other hand focuses on graph\\npatterns with characteristic sets or graph summaries. With\\ncharacteristic sets, each node is summarised by the set of\\noutgoing edge labels it has, and these “characteristic sets” are\\nused to estimate how many nodes (and joins) participate in\\na given pattern. They are discussed in a study by Neumann\\nand Moerkotte, where they observe that in RDF databases,\\nmany subjects share the same set of outgoing predicates, and\\ndefine a characteristic set as exactly this set of predicates\\nattached to a subject [7]. With graph summarizers, the graph\\nis compressed into a smaller summary (by grouping similar\\nnodes/edges), and cardinalities for complex path or subgraph\\npatterns are estimated by evaluating the query on this sum-\\nmary instead of on the full graph.\\nTwo statistical methods for dealing with that are sampling\\nand histograms. Estan and Naughton propose end-biased'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2026-02-01T15:53:53+00:00', 'source': './Data/PDFs/Report.pdf', 'file_path': './Data/PDFs/Report.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'VGTC Special Issue Paper for TVCG', 'keywords': '', 'moddate': '2026-02-01T15:53:53+00:00', 'trapped': '', 'modDate': 'D:20260201155353Z', 'creationDate': 'D:20260201155353Z', 'page': 1}, page_content='mary instead of on the full graph.\\nTwo statistical methods for dealing with that are sampling\\nand histograms. Estan and Naughton propose end-biased\\nsamples for join cardinality estimation in relational databases.\\nThe method retains tuples with probability proportional to\\ntheir frequency in join columns, so that high-frequency val-\\nues (responsible for most join output) are over-represented.\\nThis yields much more accurate join size estimates than uni-\\nform sampling at comparable sample sizes [5]. Babcock and\\nChaudhuri introduce a technique based on Bayesian inference\\nover precomputed random samples, which models uncertainty\\nand produces estimates together with confidence information.\\nTheir method captures multi-dimensional correlations without\\nassuming attribute independence and remains time-efficient.\\nImportantly, it supports user- or application-specified trade-\\noffs between performance and predictability, contributing to\\nthe design of more robust query optimisers [1].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2026-02-01T15:53:53+00:00', 'source': './Data/PDFs/Report.pdf', 'file_path': './Data/PDFs/Report.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': 'VGTC Special Issue Paper for TVCG', 'keywords': '', 'moddate': '2026-02-01T15:53:53+00:00', 'trapped': '', 'modDate': 'D:20260201155353Z', 'creationDate': 'D:20260201155353Z', 'page': 1}, page_content='Importantly, it supports user- or application-specified trade-\\noffs between performance and predictability, contributing to\\nthe design of more robust query optimisers [1].\\nHistograms are one of the most widely used methods for\\ncardinality estimation in relational DBMS. The basic idea\\nis to partition the domain of an attribute into buckets and\\nstore, for each bucket, the number of tuples (and sometimes\\nadditional stats such as distinct values). Query selectivities\\nare then approximated by assuming a uniform distribution of\\nvalues inside each bucket and summing contributions from\\nthe buckets that overlap the predicate. A detailed survey\\nof histogram-based and related synopsis techniques is given\\nby Cormode, who reviews equi-width, equi-depth and more\\nadvanced frequency-based histograms, as well as their multi-\\ndimensional extensions and maintenance trade-offs [3]. Joins\\ncan be very expensive, and to try to bridge the gap between')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieved_documents\n",
    "retrieved_documents = documents[4:8]\n",
    "retrieved_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e7dd0c-d10b-4849-8615-71bc15239dc8",
   "metadata": {},
   "source": [
    "### Retrieval Chain and Document Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7b7caef-1561-4e32-a10f-7ca7a185b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9be023a9-8706-48b0-ab1e-0258cf1fd2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb2c9507-8655-4fad-9c36-7fe786884912",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9c94f35-96d1-4349-a08f-7eb3fff47e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following questions based only of the provided context::\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df1e04b8-eab3-4e8e-9283-746f038de7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain has three main strategies to feed documents to an LLM:\n",
    "\n",
    "# 1 Stuff → put all documents into one prompt\n",
    "# 2 Map → run the LLM on each document separately, then combine outputs\n",
    "# 3 Refine → run LLM iteratively, refining previous answers\n",
    "\n",
    "# create_stuff_documents_chain → uses the stuffing strategy (all docs at once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3d815a5-8a1a-4ef1-a8c1-7e905994fec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the following questions based only of the provided context::\\n    <context>\\n    {context}\\n    </context>\\n    '), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x1197d4430>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11ab1f400>, root_client=<openai.OpenAI object at 0x119e6caf0>, root_async_client=<openai.AsyncOpenAI object at 0x1197d4ac0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82c5058e-bc27-48ea-9d5e-eb279d4d2345",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = document_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"Importantly, it supports user- or application-specified trade-\\noffs between performance and predictability, contributing to\\nthe design of more robust query optimisers [1].\\nHistograms are one of the most widely used methods for\\ncardinality estimation in relational DBMS.\",\n",
    "        \"context\": retrieved_documents\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f3f1ee0-bc21-4818-afec-5f02e92e3c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, here are the answers to some potential questions:\\n\\n1. **What are RDF databases, and how do they differ from typical relational databases?**\\n   RDF databases have a different structure compared to typical relational schemas. This structural difference affects assumptions and decisions in estimating query cardinality.\\n\\n2. **What are synopses in query optimization?**\\n   Synopses are precomputed summaries of database data, such as histograms, sketches, or samples, used by database systems to quickly estimate the cost of different query execution plans.\\n\\n3. **What are the two cardinality estimation approaches specifically designed for graph query languages discussed by Bonifati et al.?**\\n   The two approaches are: \\n   - Cardinality on paths, which involves splitting query paths into simpler segments and using predefined formulas for concentration, union, and inversion to estimate results.\\n   - Cardinality on patterns, which focuses on graph patterns using characteristic sets or graph summaries.\\n\\n4. **What is a 'characteristic set' in RDF databases?**\\n   A characteristic set is defined as the set of outgoing predicates attached to a subject, representing the outgoing edge labels of a node. This is used to estimate participation in given patterns.\\n\\n5. **How does sampling improve join cardinality estimation according to Estan and Naughton?**\\n   End-biased sampling retains tuples with probability proportional to their frequency in join columns. This over-represents high-frequency values, yielding more accurate join size estimates than uniform sampling at similar sample sizes.\\n\\n6. **What technique do Babcock and Chaudhuri introduce for cardinality estimation?**\\n   They introduce a technique based on Bayesian inference over precomputed random samples. This method models uncertainty and produces estimates with confidence information, capturing multi-dimensional correlations without assuming attribute independence.\\n\\n7. **What are histograms in the context of database cardinality estimation?**\\n   Histograms are a common method for cardinality estimation in relational DBMSs. They involve partitioning an attribute's domain into buckets and storing the number of tuples and sometimes additional stats for each bucket. Query selectivities are approximated by assuming uniform distribution within buckets and summing up overlapping bucket contributions.\\n\\n8. **What does Cormode's survey focus on?**\\n   Cormode's survey reviews histogram-based and related synopsis techniques, such as equi-width, equi-depth, advanced frequency-based histograms, and their multidimensional extensions, along with maintenance trade-offs.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42c56157-762f-46a5-8969-4a6b2e9dcbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2843748e-235b-444f-90d7-b3cab872dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the database as a retriever allows you to answer any query that falls within the context of the documents\n",
    "# stored in the vector store. When a query is made, the retriever dynamically searches the database for the most \n",
    "# relevant documents, which are then passed to the document chain and processed by the LLM to generate a natural \n",
    "# language answer. This makes the system fully flexible, as it can handle any question covered by the stored content. \n",
    "\n",
    "\n",
    "# In contrast, if you use a fixed set of retrieved documents, the system can only generate answers based on those specific \n",
    "# documents. Even if a question is relevant to the overall database, the LLM will only see the limited fixed context and \n",
    "# cannot access the rest of the information. The key difference is that a dynamic retriever provides context based on the \n",
    "# query, enabling a true retrieval-augmented generation workflow, while a fixed set of documents limits the LLM to a narrow, \n",
    "# preselected scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea9da923-0e7a-4909-8413-635530ede84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever → already knows how to query FAISS and fetch relevant documents\n",
    "# document_chain → knows how to feed those documents to the LLM\n",
    "# retriever_chain → combines them in one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1421ec6-be05-494c-ada7-03ead3fddebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "# document_chain provides the context information\n",
    "retriever_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c1015-1379-4110-88c8-c15dd8d50dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retriever_chain.invoke({\"input\": \"What is the technique invoked by Babcock and Chaudhuri?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195733f-da2d-4d46-a0bd-467f2355fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
