{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b17eac7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3708190-4ffb-4848-a688-a8eee6ba9373",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Reading Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "81bd610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "45726d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_loader = TextLoader(\"./Data/Text/speech.txt\")\n",
    "# txt_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "59c3ef23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './Data/Text/speech.txt'}, page_content='Ladies and gentlemen, friends, and fellow builders,\\n\\nWe live in an era where change no longer knocks on the door. It breaks it down. Every year, technology moves faster, systems grow more complex, and decisions that once took decades now take months, sometimes weeks. And in the middle of all this acceleration, there is one uncomfortable truth we must face: progress without direction is not innovation—it is chaos.\\n\\nFor generations, we measured success by what we could build. Bigger machines. Faster networks. Smarter systems. But today, the real challenge is not whether we can build something. It is whether we should, and if we do, whether we understand the consequences of what we are putting into the world.\\n\\nTechnology is not neutral. Every line of code carries assumptions. Every model reflects priorities. Every system amplifies certain voices while muting others. When we say we are “just engineers” or “just researchers,” we ignore the fact that our work shapes how people live, learn, communicate, and even think.\\n\\nLook around us. Algorithms decide what news we see. Automated systems influence who gets a loan, who gets a job interview, and who is flagged as a risk. These systems are efficient, scalable, and impressive. But efficiency without accountability can quietly erode trust. And trust, once lost, is far harder to rebuild than any system we deploy.\\n\\nThe future we are building will not be defined by our tools alone. It will be defined by the values we embed into them. If we optimize only for speed, we will get speed. If we optimize only for profit, we will get profit. But if we fail to optimize for fairness, transparency, and human dignity, we should not be surprised when the systems we create feel cold, distant, and unfair.')]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_text = txt_loader.load()\n",
    "loaded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2dcad-5074-4517-b17d-92b133c20371",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Reading PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "99aaa80c-1d05-4ec3-906b-cfb8b07b553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e38840-0904-47f7-bba7-4c3e63376357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "12abb5c6-efb9-4e68-9f2f-2dda68cb31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(\"./Data/PDFs/Report.pdf\")\n",
    "# pdf_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a1d38bc3-d745-4942-9f61-f7c2a5e458ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2026-02-01T15:53:53+00:00', 'author': '', 'keywords': '', 'moddate': '2026-02-01T15:53:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': 'VGTC Special Issue Paper for TVCG', 'title': '', 'trapped': '/False', 'source': './Data/PDFs/Report.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}, page_content='Engineering Data Intensive Systems - 2IMD10\\nEDS - PROJECTREPORT\\nTeam Number - 16\\nFull Name Discord Username Email\\nDivyansh Purohit wah shampy d.purohit@student.tue.nl\\nLikhit Vesalapu likhit7. l.vesalapu@student.tue.nl\\nPrathamesh Samal viper 101 p.samal@student.tue.nl\\nElena Terzieva ellie218388 e.e.terzieva@student.tue.nl\\nEindhoven, February 1, 2026'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2026-02-01T15:53:53+00:00', 'author': '', 'keywords': '', 'moddate': '2026-02-01T15:53:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': 'VGTC Special Issue Paper for TVCG', 'title': '', 'trapped': '/False', 'source': './Data/PDFs/Report.pdf', 'total_pages': 10, 'page': 1, 'page_label': '1'}, page_content='1 ABSTRACT\\nAccurate cardinality estimation is fundamental to query opti-\\nmization in graph databases, enabling the selection of efficient\\nexecution plans for regular path queries. In this report, we\\npresent a hybrid cardinality estimator that combines multi-\\nple statistical synopses including per-label statistics, pairwise\\nlabel correlations, and characteristic sets with a weighted\\nand stratified sampling strategy for complex queries. Our ap-\\nproach balances estimation accuracy against preparation time\\nand memory overhead, achieving competitive performance\\non both synthetic and real-world workloads.\\n2 INTRODUCTION\\nThe efficiency of query processing is an important topic in\\nthe domain of databases, where cardinality estimation plays\\na central role. Modern graph and XML database systems\\nrely on cardinality estimators as a core component of their\\ncost based query optimizers. A cardinality estimator is a\\nstatistical module that, given an abstract description of a query\\n(a labeled path pattern starting from some node and ending\\nat another), predicts how many results this query is likely to\\nproduce. These predictions are crucial for choosing efficient\\nexecution plans, selecting join orders, and deciding which\\nphysical operators to use. Without an accurate cardinality\\nestimator, an optimizer can easily choose plans that are orders\\nof magnitude slower than necessary.\\nTraditional cardinality estimators for path queries are based\\non compact summaries of structural synopses that record\\nper-label statistics, degree distributions, and limited informa-\\ntion about how labels coincide. Although such synopses can\\nbe very effective for short, simple path patterns, their accuracy\\nrapidly degrades for long paths, patterns with repetition, or\\ndata with strong structural correlations. In these cases, naive\\nindependence assumptions break down and purely synopsis\\nbased estimators can produce large errors.\\nTo address these challenges, our work investigates a sam-\\npling based cardinality estimator that involves four com-\\nplementary approaches: Stratified reservoir sampling, End-\\nbiased sampling, Correlated sampling, XSEED based Simula-\\ntion.\\n3 LITERATURESURVEY\\nThe topic of cardinality estimation continues to be one of the\\nmost important in query optimization. There are different\\ntechniques that have been developed for estimating the size\\nof the results produced by a query. In this project, we are\\ndealing with RDF databases that differ from the typical re-\\nlational schemas because of their structure, which leads to\\ndifferent assumptions and decisions made when estimating\\nthe cardinality of a query. In query optimisation, synopses are\\nprecomputed summaries of database data (like histograms,\\nsketches, samples) that database systems use to quickly es-\\ntimate the cost of different query execution plans. Bonifati\\net al. survey cardinality estimation techniques specifically\\ndesigned for graph query languages. The two approaches\\ndiscussed are cardinality of paths and cardinality on patterns.\\nWith cardinality on paths, the query path is split into simpler\\nsegments and predefined formulas are used for concentra-\\ntion, union and inversion are combined to estimate the result.\\nCardinality on patterns, on the other hand focuses on graph\\npatterns with characteristic sets or graph summaries. With\\ncharacteristic sets, each node is summarised by the set of\\noutgoing edge labels it has, and these “characteristic sets” are\\nused to estimate how many nodes (and joins) participate in\\na given pattern. They are discussed in a study by Neumann\\nand Moerkotte, where they observe that in RDF databases,\\nmany subjects share the same set of outgoing predicates, and\\ndefine a characteristic set as exactly this set of predicates\\nattached to a subject [7]. With graph summarizers, the graph\\nis compressed into a smaller summary (by grouping similar\\nnodes/edges), and cardinalities for complex path or subgraph\\npatterns are estimated by evaluating the query on this sum-\\nmary instead of on the full graph.\\nTwo statistical methods for dealing with that are sampling\\nand histograms. Estan and Naughton propose end-biased\\nsamples for join cardinality estimation in relational databases.\\nThe method retains tuples with probability proportional to\\ntheir frequency in join columns, so that high-frequency val-\\nues (responsible for most join output) are over-represented.\\nThis yields much more accurate join size estimates than uni-\\nform sampling at comparable sample sizes [5]. Babcock and\\nChaudhuri introduce a technique based on Bayesian inference\\nover precomputed random samples, which models uncertainty\\nand produces estimates together with confidence information.\\nTheir method captures multi-dimensional correlations without\\nassuming attribute independence and remains time-efficient.\\nImportantly, it supports user- or application-specified trade-\\noffs between performance and predictability, contributing to\\nthe design of more robust query optimisers [1].\\nHistograms are one of the most widely used methods for\\ncardinality estimation in relational DBMS. The basic idea\\nis to partition the domain of an attribute into buckets and\\nstore, for each bucket, the number of tuples (and sometimes\\nadditional stats such as distinct values). Query selectivities\\nare then approximated by assuming a uniform distribution of\\nvalues inside each bucket and summing contributions from\\nthe buckets that overlap the predicate. A detailed survey\\nof histogram-based and related synopsis techniques is given\\nby Cormode, who reviews equi-width, equi-depth and more\\nadvanced frequency-based histograms, as well as their multi-\\ndimensional extensions and maintenance trade-offs [3]. Joins\\ncan be very expensive, and to try to bridge the gap between\\nquery efficiency and accuracy, Li et al. introduced the new\\nalgorithm Wander Join [6]. Wander Join is a join estimation\\nmethod that treats a multi-table join like a graph and then\\nperforms random walks through the graph instead of com-\\nputing the full join. Since it only touches a small number\\nof data points and does not need to compute statistics for\\nthe full data, the approximation results are produced quickly\\nand more accurately with the increase in performed random'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2026-02-01T15:53:53+00:00', 'author': '', 'keywords': '', 'moddate': '2026-02-01T15:53:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': 'VGTC Special Issue Paper for TVCG', 'title': '', 'trapped': '/False', 'source': './Data/PDFs/Report.pdf', 'total_pages': 10, 'page': 2, 'page_label': '2'}, page_content='walks.\\n4 IMPLEMENTATION\\n4.1 Approach and Planning\\n4.1.1 Problem Analysis\\nIn order to address the problem of cardinality estimation for\\npath queries, our team first examined the basic properties of\\ngraph data. Especially in networks, graph data shows heavy\\ntailed, power-law distribution, variable-length traversals and\\nKleene closures in comparison to relational tables where data\\nusually follows uniform distribution. The scoring metric for\\nthis project emphasizes the accuracy on real workloads with\\na weight of two and also penalizing the run times. This\\ntuned our strategy to get accurate estimates without expensive\\ncomputation.\\nThe provided graph contained 10,000 nodes, 22,122 edges,\\nand 36 distinct edge labels. With an average out-degree of\\napproximately 2.92 and a maximum out-degree of 43, the\\ngraph is relatively sparse (density approx 0.02%). The de-\\ngree distribution follows a classic power-law pattern: 37% of\\nnodes have out-degree 1, while only a handful of hub nodes\\nexceed degree 20. This skewed distribution means that a small\\nnumber of high-degree nodes disproportionately contribute\\nto path counts, making uniform sampling ineffective.\\nThe label distribution is similarly skewed, with label 8\\nappearing in over 5,000 edges (23% of all edges), while\\n12 labels appear fewer than 50 times each. This imbalance\\naffects join selectivity and makes certain label combinations\\nfar more common than others. The query workload spanned\\ndiverse patterns:\\n•Single predicates: e.g.,,1>,,,8>,\\n• Concatenations: e.g., ,1>/8>,, ,1>/8>/9>/8>, (up\\nto length 5)\\n•Kleene closures: e.g.,,(1>)+,,,(8>)+,\\n•Mixed patterns: e.g.,,1>/(8>)+/(9>)+,\\n•Label unions: e.g.,,(0>|1>|2>|3>|4>)+,\\n• Bound source queries: e.g., 200,1>/8>,,\\n227,(1>)+/(8>)+,\\nThe scoring metric for this project emphasizes accuracy on\\nreal workloads with a weight of two while also penalizing\\npreparation and estimation time. This guided our strategy: we\\nneeded accurate estimates without expensive runtime compu-\\ntation.\\n4.1.2 Technique Selection Based on Liter-\\nature Survey\\nBased on our literature review, we selected the following\\napproaches for our hybrid estimation model:\\n• End-biased Sampling:This strategy was chosen in\\norder to address the variation brought on by power-law\\ndistributions. We reasoned that probabilistic estimate\\nis not appropriate for frequent values (hubs), since they\\nhave the greatest influence on join size and variance. We\\neliminate the main cause of instability in the estimator\\nby explicitly isolating and tracking the top- K highest-\\ndegree nodes using exact logic, leaving a lower-variance\\npopulation (the tail) that is easier to sample.\\n• Stratified Reservoir Sampling:Rare edge labels are\\nnot guaranteed to be covered by a global random sam-\\nple. To make sure that every potential transition in the\\ngraph has representative start nodes, we decided to use\\nstratified reservoirs, which allocate a predetermined sam-\\nple budget per unique label. In order to reduce endless\\nfailures on queries with rare predicates, this choice was\\ncrucial.\\n• Correlated Sampling by Hashing:Since independent\\nrandom sampling statistically fails to capture the inter-\\nsection of joins in sparse samples, we ignored it for the\\ntail. We make sure that sample choices are consistent\\nacross various query contexts by using a deterministic\\nhashing approach as suggested by Estan et al. given as\\nfollows:\\nh(v)≤p v (1)\\nwhere pV is the sampling probability for a given value\\nof v and h is the hashing function which maps values in\\nthe range[0,1].\\n• XSEED-based Seed Simulation:In clustered graphs,\\nthe assumption of independence between path steps is\\nbroken by standard histograms. The idea of employing\\nbound nodes as ”seeds” for cardinality estimation is pre-\\nsented in the XSEED paper [11]. We incorporated this\\nidea into our Sampling Model, treating our Stratified\\nSamples as ”seeds” and executing the query path from\\nthem. This enables us to capture the structural relation-\\nships in the graph that are missed by static statistical\\nmethods.\\nWe decided to pursue with a Hybrid Cardinality Estimator\\nthat adjusts to the structural skew of the graph by combining\\nthe above mentioned approaches. By dynamically replicat-\\ning graph traversals on a subgraph, this synergy enables the\\nsystem to get around the drawbacks of static statistics and\\nbalance the accuracy with the runtime efficiency of sampling.\\n4.1.3 Expected Challenges\\nWe anticipated several challenges:\\n• Balancing accuracy and speed: More sophisticated\\nstatistics and complex simulations improve accuracy\\nbut increase both preparation and estimation time. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2026-02-01T15:53:53+00:00', 'author': '', 'keywords': '', 'moddate': '2026-02-01T15:53:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': 'VGTC Special Issue Paper for TVCG', 'title': '', 'trapped': '/False', 'source': './Data/PDFs/Report.pdf', 'total_pages': 10, 'page': 3, 'page_label': '3'}, page_content='addition, reservoir sizes and the estimation time penalty\\nhad to be carefully balanced. We anticipated that wander\\njoin approaches, which use random walks to estimate\\njoin cardinalities, would greatly benefit accuracy but\\nadversely affect estimation time.\\n• Handling Kleene closures: Single predicate queries\\nlike 1 and concatenation queries like 1 /2 were straight-\\nforward and could be directly computed from our stored\\nlabel statistics and join synopses. However, transitive\\nclosure queries can produce result sets ranging from\\nempty to quadratic in graph size, making estimation\\ninherently difficult.\\n• Correlation capture: Independence assumptions be-\\ntween path segments would lead to systematic over or\\nunder estimation, but capturing accurate and extensive\\ncorrelations would require exhaustive precomputations.\\n• Memory constraints: Storing detailed statistics for all\\nlabel pairs (pairwise correlations) would scale quadrati-\\ncally with the number of labels in the graph.\\n4.2 Implementation Progress and Results\\nWe used an iterative life cycle for our implementation. Before\\nfinalizing our ultimate End-Biased Stratified architecture, we\\nran our experiments with a number of approaches, including\\na computationally costly Random Walk attempt.\\n4.2.1 Iteration 1: Basic Synopsis\\nWe began by implementing foundational statistics computed\\nin a single pass over the graph. This included edge counts\\nper label and a matrix to store pairwise correlations between\\nlabels. By adding up the product of degrees, we were\\nable to determine the precise number of pathways for any\\nlength-2 label combination. A join statistics matrix was also\\nintroduced to capture the number of two-hop paths between\\nall label pairs, providing exact estimates for length-2 queries.\\nResult:This baseline achieved reasonable accuracy\\non simple queries but struggled with complex patterns and\\nshowed high variance on longer paths.\\n4.2.2 Iteration 2: Wander Joins and Ran-\\ndom Walks\\nWe implemented a Wander Join estimator to increase accuracy\\nfor Kleene closures and long pathways. In order to naturally\\napproximate the path distribution for each query, we con-\\nducted random walks beginning from a collection of sampled\\nnodes. To further refine this approach, we explored several ad-\\nvanced adaptive sampling techniques: the Filtering-Sampling\\nApproach (FaSTest), that dynamically adjusted sample sizes\\nbased on query characteristics, along with an adaptive stop-\\nping criteria that terminated walks once the 95% confidence\\ninterval around the Horvitz-Thompson estimate fell below a\\n5% error threshold, thereby saving time on dense queries.\\nResult:Since this method physically traversed the graph\\nstructure to capture correlations, this method produced re-\\nmarkable accuracy, however estimation time (Test) grew sig-\\nnificantly. Even with our optimizations, a large number of\\nwalks were required in order to get stable estimates, which\\nwent against the time constraints set up for this project. In-\\nstead, we opted for a pre calculated sampling strategy.\\n4.2.3 Iteration 3: Hybrid Approach\\nOur estimating model was developed through an ordered\\nbreakdown of the problem rather than being chosen as a sin-\\ngle technique. In order to address particular data features, we\\nintegrated various approaches in three logical steps.\\nStep 1: Decomposition by DegreeThe existence of high-\\ndegree nodes is the main cause of errors in graph estima-\\ntion. We developed a split-execution model based on the End-\\nBiased Sampling concepts. We proposed treating the graph\\nG as Ghead and Gtail , into two separate subgraphs. Since any\\nprobabilistic variance here would propagate quadratically dur-\\ning joins, we concluded that Ghead (the top-K nodes) must\\nbe effectively retained in memory and queried accurately. In\\nGtail , where the variance is inherently lower, the probabilistic\\nestimation is strongly restricted.\\nStep 2: Handling SparsityStandard sampling is ineffec-\\ntive for rare predicates in path queries, despite Estan et al.’s\\nsuggestion to weight by frequency. A global sample will be\\nunable to capture a label L if it is present in only 0.1% of the\\ngraph. The sampling logic was expanded to be stratified. Each\\nedge label is treated as a separate population strata. Regard-\\nless of how uncommon the edge label is, we mathematically\\nguarantee that the probability of providing a ”zero-estimate”\\nfor a valid query path is reduced by guaranteeing a constant\\nreservoir size for each label.\\nStep 3: Correlation PreservationIn essence, a path query\\nX→Y→Z is an intersection problem. The fundamental cor-\\nrelations needed to compute these crossings are eliminated by\\nindependent random sampling. We decided to use Correlated\\nHashing. In case, a node exists in the reservoir for label A (as\\na target), it will also be present in the reservoir for label B (as\\na source), as long as it satisfies the probability threshold, by\\nmaking the inclusion of a node into a reservoir a deterministic\\nfunction of its ID (h(v)≤p v). This maintains the linkability.\\nStep 4: Simulation-Based EstimationFinally, we used the\\nSeed-Based approach from the XSEED paper to solve for\\ncomplex query types that defy static formulas, such as Kleene\\nclosures (+). In addition to being statistical representatives,\\nwe thought of our stratified samples as ”active seeds.” Rather\\nthan merging histograms, the estimator measures reachability\\nby physically running the query from these seeds and scal-\\ning the outcome by the inverse of the inclusion probability\\n(1/p v).\\nResult:This approach achieved the best trade-off between\\naccuracy and estimation time on simple as well as complex\\nqueries. queries. The only drawback of this approach was that\\nit occupied significantly more memory, which was further'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2026-02-01T15:53:53+00:00', 'author': '', 'keywords': '', 'moddate': '2026-02-01T15:53:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': 'VGTC Special Issue Paper for TVCG', 'title': '', 'trapped': '/False', 'source': './Data/PDFs/Report.pdf', 'total_pages': 10, 'page': 4, 'page_label': '4'}, page_content='refined in the next iteration.\\n4.2.4 Iteration 4: Optimization\\nThe final iteration focused on performance optimization with-\\nout sacrificing on the accuracy. This iteration was done in\\norder to reduce Estimation Time (Test) and Preparation Time\\n(Tprep).\\n• Binary Search for Adjacency:It was inefficient to\\nsearch adjacency lists linearly for edges that matched a\\ngiven start label for the high-degree top-k nodes. Using\\nbinary search on the sorted edge lists, we optimized the\\nsampling algorithm. This resulted in a targeted speedup\\nfor the most frequently used nodes by lowering the edge\\nlookup difficulty from linear to logarithmic.\\n• Memory Reuse:Our code initially generated fresh tem-\\nporary lists for each node it handled during the prepa-\\nration stage. This required more memory and was slow\\ndue to large number of nodes. These list creations were\\nrelocated outside of the main loop. The same memory\\nblock is reused repeatedly since we only need to gener-\\nate the list once and then clear it after processing each\\nnode. As a result, the overhead of continuous memory\\nallocation was significantly reduced.\\n4.2.5 Results\\nOur final hybrid estimator shows a good trade-off between\\nruntime and estimation accuracy. The leaderboard’s perfor-\\nmance metrics are as follows\\nT able 1: Final Leaderboard Performance\\nMetric Value\\nDiscord Username viper 101\\nSynthetic Accuracy (Eqsyn) 1.18\\nReal-Workload Accuracy (Eqreal) 8.86\\nPreparation Time (Tprep) 153.88 ms\\nEstimation Time (Test) 32.58 ms\\nPeak Memory Usage (M) 24.07 MB\\nFinal Score 49.63\\n5 PART2: EFFICIENTQUERYEVALUATION\\nIn Part 2 of the project, we shift focus from cardinality es-\\ntimation to efficient query evaluation. The goal is to make\\nQuicksilverSQUIFfy: Smart, Quick, and Frugal. We focused\\nheavily on balancing the trade-offs between execution speed,\\npreparation time, and strict memory limits. We didn’t just\\nwant it to be fast; we wanted it to be robust and memory\\nefficient as per Frugal requirement.\\n5.1 Strategy\\nOur optimization strategy followed a systematic approach\\ninspired by research into high-performance graph processing\\nsystems. We categorized our efforts into three pillars: Smart\\n(choosing optimal algorithms), Quick (reducing computation\\ntime), and Frugal (minimizing memory overhead).\\n5.1.1 Smart\\nWe made informed decisions based on query patterns and\\nempirical benchmarking:\\n• Separate Bound Source/Target Handlers:We used\\nforward BFS traversal for queries with bound sources\\nand backward BFS from the target for queries with\\nbound targets. This method helped us to avoid full-graph\\ntraversals and significantly reduced the search space.\\n• Node ID Mapping:We maintained a translation table\\nto map the source and target node IDs from original to\\nreordered IDs at evaluation time so that when the graph\\nis reordered (discussed in Quick) and nodes get new\\nIDs, the evaluator continues work with consistent node\\nidentifiers.\\n5.1.2 Quick: Reducing Computation Time\\nHere our primary focus was eliminating redundant work and\\nimproving cache efficiency:\\n• Label-indexed CSR:We pre-partition edges by label\\nduring graph loading instead of filtering them during\\nBFS, which eliminated the label check from the hottest\\ncode path. This reduced branch mispredictions and im-\\nproved instruction level parallelism.\\n• Graph Reordering:We implemented BFS based node\\nID reassignment, starting from the highest-degree node.\\nThis approach assigns consecutive IDs to the nodes vis-\\nited together during traversal, eventually improving spa-\\ntial locality when accessing adjacency data in the CSR\\narrays.\\n• Tracking visited nodes using timestamp:Rather than\\nclearing the visited array between queries (O(V) oper-\\nation), we used a monotonically increasing timestamp\\ncounter to indicate the status of a node. We marked a\\nnode with the current timestamp when visiting it, and\\nthe node is considered unvisited if its timestamp doesn’t\\nmatch the current query’s timestamp (O(1) operation).\\n5.1.3 Frugal: Memory Efficiency\\nHere we focused on minimizing dynamic memory operations\\nbecause memory allocation and deallocation are expensive\\noperations.\\n• Memory Pooling:All vectors used during BFS traversal\\nwere pre-allocated once during the prepare phase and\\nreused across all queries via swap() operations rather\\nthan repeated allocation and deallocation.\\n• CSR Format:We converted adjacency lists to Com-\\npressed Sparse Row format, storing edges in contiguous\\narrays indexed by node ID, to reduce memory fragmen-\\ntation and improve cache utilization.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2026-02-01T15:53:53+00:00', 'author': '', 'keywords': '', 'moddate': '2026-02-01T15:53:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': 'VGTC Special Issue Paper for TVCG', 'title': '', 'trapped': '/False', 'source': './Data/PDFs/Report.pdf', 'total_pages': 10, 'page': 5, 'page_label': '5'}, page_content='5.2 Literature Survey\\nWork on fast graph query evaluation repeatedly shows that\\nperformance depends less on high-level query language rules\\nand more on how traversal is executed: reducing wasted\\nexploration, using cache-friendly graph layouts, and avoiding\\noverhead inside the traversal loop. Shared-memory graph\\nsystems and RDF engines especially emphasize contiguous\\nadjacency storage, locality-aware preprocessing, and access-\\npath choices that prevent repeated filtering during evaluation.\\nIn direction-optimizing BFS, Beamer et al. show that\\nswitching between traversal styles can dramatically reduce\\nthe number of edges examined on real “small-world” graphs,\\nwhich is exactly the problem when a naive traversal expands\\ntoo broadly. Their key message is that how you traverse (direc-\\ntion/strategy decisions) can matter as much as raw low-level\\noptimization, because it directly shrinks the explored search\\nspace [2]. Moreover, when graph preprocessing changes ver-\\ntex IDs (for example after reordering), systems typically need\\na stable way to preserve semantics across representations.\\nIn the graph ordering work by Wei et al., the goal is to re-\\narrange vertex order to improve cache behavior. However,\\nthis implicitly creates a “logical graph” (original IDs) and a\\n“physical graph” (reordered IDs). The paper motivates the\\npractical need to keep query evaluation correct even when\\nthe physical layout changes — conceptually the same reason\\nmany systems maintain an ID mapping layer around reordered\\ngraphs [9].\\nFor traversal-heavy workloads, research shows that perfor-\\nmance is often memory-bound, so the fastest solutions focus\\non contiguous layouts and minimizing work in the tight loop.\\nLigra is a well-known shared-memory framework demon-\\nstrating that graph traversal becomes significantly faster when\\nadjacency is stored and processed in cache-friendly ways,\\nenabling efficient neighbor scans and reducing overhead from\\npointer-heavy structures. Although Ligra is a framework, its\\nresults strongly support the idea that CSR-like representations\\nand streamlined traversal primitives are a practical baseline\\nfor high throughput BFS-style processing [8]. A second line\\nof work targets locality directly via graph reordering. Wei et\\nal. show that reordering vertices can reduce CPU cache miss\\nratios across several graph algorithms, producing speedups\\nwithout changing the underlying algorithm. Their contribu-\\ntion is important for query evaluation because it treats graph\\nlayout as an optimization target: by reassigning IDs so that\\nvertices accessed close in time are also close in memory, the\\ncost of adjacency and metadata access drops [9]. Visited-\\nstate handling is another recurring bottleneck in fast traversal.\\nBeamer et al. discuss optimizations such as using compact\\nvisited representations (for example bitmaps) to reduce costly\\nrandom accesses and keep visited checks efficient on shared-\\nmemory machines. The point is that even “small” bookkeep-\\ning operations can dominate runtime when done at scale, so\\nvisited tracking must be engineered carefully [2]. A large\\npart of the memory efficiency in graph query engines comes\\nfrom keeping data structures compact and avoiding repeated\\nallocations during query execution. Many systems therefore\\nrely on two practical ideas such as storing adjacency in a con-\\ntiguous sparse format (typically CSR), and pre-allocating and\\nreusing working buffers instead of allocating/freeing mem-\\nory for every query. Several systems make this point even\\nmore explicitly by standardizing around CSR as the baseline\\nrepresentation due to its compactness and predictable access\\npattern. Dhulipala et al. (GBBS) state that graphs in the suite\\nare stored in Compressed Sparse Row (CSR), where neigh-\\nbor lists live in contiguous arrays with an offset array per\\nnode. This design reduces fragmentation and supports cache-\\nfriendly scans. The key motivation is that pointer-heavy adja-\\ncency lists create overhead and irregular access, while CSR\\nimproves both memory footprint and traversal throughput [4].\\nMany high-performance systems reduce memory overhead\\nby minimizing dynamic allocations during execution. Winter\\net al. (faimGraph) present a memory-management approach\\ndesigned for fully-dynamic graphs on the GPU. While faim-\\nGraph is GPU-focused, its core principle—using one large\\npre-allocation to avoid the overhead and fragmentation of\\nrepeated memory allocation or vector resizing—maps cleanly\\nto CPU implementations. We adopted this by pre-allocating\\nour working buffers once to keep runtime memory behavior\\nstable. [10].\\n5.3 Implementation Progress\\nWe followed an iterative approach for optimizing our per-\\nformance, implementing and benchmarking each technique\\nindependently to measure its overall impact. In the sections\\nbelow, we explain each optimization iteration experiments.\\nWe also report the performance metrics achieved in each itera-\\ntion, along with a improvement percentage from the previous\\niteration (represented by ∆). A positive value indicates im-\\nprovement in the performance parameter whereas a negative\\nvalue represents decrease in the performance.\\n5.3.1 Iteration 1: Baseline Analysis\\nThe baseline implementation used standard adjacency lists\\nwith vectors of (label, target) pairs. Each query allocated\\nfresh vectors for BFS traversal and used a boolean visited\\narray that was cleared between queries. The simple evaluator\\nperformed linear scans through the adjacency lists, checking\\neach edge’s label against the query predicate.\\nResults:Profiling revealed several bottlenecks: re-\\npeated memory allocation for each query, O(V) cost to clear\\nthe visited array, poor cache locality due to scattered memory\\naccess, and redundant label checks in the inner traversal loop.\\n5.3.2 Iteration 2: Bound-Aware Evaluation\\nand Semi-Naive Transitive Closure\\nAfter the experiments on our baseline, w e found that queries\\nwith bound sources or targets continued to unnecessarily tra-\\nverse the whole graph. Furthermore, Kleene star searches'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2026-02-01T15:53:53+00:00', 'author': '', 'keywords': '', 'moddate': '2026-02-01T15:53:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': 'VGTC Special Issue Paper for TVCG', 'title': '', 'trapped': '/False', 'source': './Data/PDFs/Report.pdf', 'total_pages': 10, 'page': 6, 'page_label': '6'}, page_content='T able 2: Iteration 1: Baseline Performance\\nMetric Value\\nPreparation Time (Tprep) 145.87 ms\\nEvaluation TimeT syn\\neval 1213.70 ms\\nEvaluation TimeT real\\neval 17993.72 ms\\nLoad Time (Tload ) 3985.82 ms\\nPeak Memory Usage (Meval) 29.48 MB\\nScore ∼19,800\\nwere creating duplicate work by recalculating every reachable\\nnode at each cycle.\\n• Bound-Aware Evaluation:We implemented separate\\nindependent evaluation handlers for queries with bound\\nsources and bound targets. Instead of traversing the\\nentire graph, we start a breadth first search from the\\nsingle known node, significantly reducing the search\\nspace for constrained queries.\\n• Semi-Naive Transitive Closure:We replaced the naive\\ntransitive closure algorithm with a semi-naive approach\\nfor Kleene star queries. Instead of recalculating all\\nreachable nodes at each iteration, we process only newly\\ndiscovered edges, avoiding redundant computation and\\nsignificantly improving performance for recursive path\\npatterns.\\nResults:The score improved from ˜19,800 to 7,248 (63%\\nimprovement). This confirmed that algorithmic issues were\\nthe major bottleneck. However, this also revealed that signifi-\\ncant time was still being spent on constructing and managing\\nintermediate result graphs, hinting us towards data structure\\noptimizations.\\nT able 3: Iteration 2: Bound-Aware Evaluation\\nMetric Value ∆\\nPreparation Time (Tprep) 1505.31 ms −932%\\nEval Time Syn (Tsyn\\neval) 341.26 ms +72%\\nEval Time Real (Treal\\neval ) 6403.16 ms +64%\\nLoad Time (Tload ) 4092.68 ms −3%\\nPeak Memory (Meval) 24.87 MB +16%\\nScore 7,248 +63%\\n5.3.3 Iteration 3: Compact Intermediate\\nResults\\nAfter the algorithmic changes were implemented, profiling\\nshowed that creating and maintaining intermediate result\\ngraphs took a considerable amount of time. We observed\\nthat these intermediary structures kept duplicate data: reverse\\nadjacency lists were kept even if joins only traveled forward,\\nand labels were kept even though edges had already been\\nfiltered. To subside these issues, we used the following ap-\\nproaches:\\n• CompactIR Data Structure:We introduced a\\nlightweight intermediate result structure that stores only\\nsource-to-target mappings. We removed labels from in-\\ntermediate results since once edges are filtered by label\\nduring selection, storing the label becomes redundant.\\nWe also eliminated the reverse adjacency list because\\njoin operations only traverse forward, never requiring\\nbackward lookups on intermediate data.\\n• Deferred Deduplication:We also removed per-insert\\nduplicate checking, which required expensive lookups\\non every edge addition. Instead, we allowed duplicates\\nduring construction and performed a batch sort-and-\\ndeduplicate operation at the end by using finalize().\\nThis approach is more cache-friendly and significantly\\nreduces computation time.\\n• Hash-based Existence Checking:For transitive closure\\ncomputation, we replaced linear search with hash sets\\nfor tracking visited edges, reducing existence checks\\nfrom O(N) to O(1) and speeding up Kleene star queries.\\nResult:The score improved from 7,248 to 4,255 (41% im-\\nprovement). Memory usage and computation time were de-\\ncreased by eliminating redundant storage and delaying dedu-\\nplication.\\nT able 4: Iteration 3: Compact Intermediate Results\\nMetric Value ∆\\nPreparation Time (Tprep) 1476.81 ms +2%\\nEval Time Syn (Tsyn\\neval) 212.29 ms +38%\\nEval Time Real (Treal\\neval ) 3828.77 ms +40%\\nLoad Time (Tload ) 3911.26 ms +4%\\nPeak Memory (Meval) 10.40 MB +58%\\nScore 4,255 +41%\\n5.3.4 Iteration 4: Direct BFS Traversal\\nIn the previous iteration, our approach continued to materi-\\nalize intermediate result graphs and carry out explicit join\\noperations in spite of the CompactIR enhancements. Since\\nwe only want the final reachable nodes for path queries, not\\nthe intermediary edges, we thought if this abstraction was\\neven essential. As a result, we completely stopped using\\nintermediate graph building.\\n• Removed Intermediate Graph Construction:We\\neliminated the CompactIR data structure and join op-\\nerations entirely. Instead of building intermediate result\\ngraphs and joining them, we now traverse the query path\\ndirectly while maintaining only the current frontier. This\\navoids materializing all intermediate edges, significantly\\nreducing memory usage and computation time.\\n• Specialized Evaluation Paths:We implemented three\\nseparate evaluation functions: forward BFS for bound-\\nsource queries, backward BFS for bound-target queries,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2026-02-01T15:53:53+00:00', 'author': '', 'keywords': '', 'moddate': '2026-02-01T15:53:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': 'VGTC Special Issue Paper for TVCG', 'title': '', 'trapped': '/False', 'source': './Data/PDFs/Report.pdf', 'total_pages': 10, 'page': 7, 'page_label': '7'}, page_content='and multi-source traversal for unbound queries. Each\\npath is optimized for its specific access pattern.\\n• Bitvector-based Tracking:We replaced hash sets with\\nbitvectors for tracking frontier membership and visited\\nnodes. For dense node ID spaces, bitvectors provide\\nO(1) lookups with better cache locality than hash-based\\nstructures.\\n• Label-indexed Edge Lists:Precomputed lists were\\nadded that provide direct access to all edges with a given\\nlabel, eliminating the need to scan entire adjacency lists\\nand filter by label during unbound evaluation.\\n• Memory Optimizations:vector pre-reservation were\\nadded to reduce reallocations during BFS and used\\nmove semantics to avoid unnecessary vector copies when\\nswapping frontiers.\\nResult:The score improved from 4,255 to 2,063 (52%\\nimprovement). Eliminating intermediate graph construction\\nwas a major win, confirming that the join-based abstraction\\nwas unnecessary overhead. However, experiments showed\\na new bottleneck: the unbound evaluation was allocating\\nthousands of bitvectors per query, one for each active source.\\nT able 5: Iteration 4: Direct BFS T raversal\\nMetric Value ∆\\nPreparation Time (Tprep) 1451.41 ms +2%\\nEval Time Syn (Tsyn\\neval) 55.70 ms +74%\\nEval Time Real (Treal\\neval ) 1896.07 ms +50%\\nLoad Time (Tload ) 4456.80 ms −14%\\nPeak Memory (Meval) 5.20 MB +50%\\nScore 2,063 +52%\\n5.3.5 Iteration 5: Timestamp-based Vis-\\nited Tracking\\nIn the previous iteration, direct BFS implementation showed a\\nnew bottleneck: in evaluateUnbound(), we were allocating\\na fresh vector<bool>(V) bitvector for each active source\\nto track visited nodes. With thousands of active sources per\\nquery, this meant thousands of O(V) allocations which caused\\na significant overhead on the query time.\\n• Removed Per-Source Bitvector Allocation:In the pre-\\nvious iteration, the unbound evaluation allocated a sepa-\\nrate vector<bool>(V) for each active source to track\\nvisited nodes. With thousands of active sources, this\\ncaused thousands of memory allocations per query, cre-\\nating significant overhead.\\n• Shared Timestamp Array:We replaced per-source\\nbitvectors with a single shared visitedTime array.\\nEach source receives a unique timestamp via an incre-\\nmenting counter, and a node is considered visited if its\\ntimestamp matches the current value. This eliminates all\\nper-source allocations and avoids the need to clear the\\narray between sources, with the timestamp automatically\\ninvalidating previous visits.\\n• Graph Finalization:We added a finalizeGraph()\\nfunction called once after loading that sorts all adja-\\ncency lists by label and sorts label edge lists by source.\\nThis ensures edges with the same label are contiguous\\nin memory, improving cache locality during traversal,\\nand improves sequential access patterns during unbound\\nevaluation.\\nResult:The score improved from 2,063 to 1,395 (32% im-\\nprovement). The shared timestamp technique eliminated thou-\\nsands of allocations per query with minimal overhead. With\\nallocation bottlenecks resolved, experiments now showed that\\ncache misses during graph traversal had become the major\\ncost, leading us toward memory layout optimizations.\\nT able 6: Iteration 5: Timestamp-based Visited T racking\\nMetric Value ∆\\nPreparation Time (Tprep) 1469.60 ms −1%\\nEval Time Syn (Tsyn\\neval) 50.18 ms +10%\\nEval Time Real (Treal\\neval ) 1238.07 ms +35%\\nLoad Time (Tload ) 4428.61 ms +1%\\nPeak Memory (Meval) 5.00 MB +4%\\nScore 1,395 +32%\\n5.3.6 Iteration 6: CSR Format, Graph Re-\\nordering, and Label-Indexed Struc-\\ntures\\nBenchmarking revealed that cache misses during graph traver-\\nsal had emerged as the main bottleneck following the re-\\nmoval of allocation overhead. The adjacency list structure\\nvector<vector<pair>> dispersed edges throughout mem-\\nory, resulting in frequent cache misses while attempting to\\naccess neighbors. Furthermore, nodes visited together during\\nBFS frequently had distant IDs due to the arbitrary assign-\\nment of node IDs during graph loading, which resulted in\\npoor spatial proximity.\\n• Memory Pooling:We moved all BFS-related vectors\\n(frontier, nextFrontier, visitedTime, etc.) to\\nclass members, pre-allocated once during prepare().\\nInstead of allocating new vectors per query, we reuse\\nthem via swap() and clear(), eliminating allocation\\noverhead from the query hot path.\\n• Compressed Sparse Row (CSR) Format:We\\nconverted the adjacency list representation\\n(vector<vector<pair>>) to CSR format with\\ntwo contiguous arrays: offsets[V+1] storing the\\nstarting index of each node’s edges, and edges[E]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2026-02-01T15:53:53+00:00', 'author': '', 'keywords': '', 'moddate': '2026-02-01T15:53:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': 'VGTC Special Issue Paper for TVCG', 'title': '', 'trapped': '/False', 'source': './Data/PDFs/Report.pdf', 'total_pages': 10, 'page': 8, 'page_label': '8'}, page_content='Figure 1: Final architecture of the QuickSilver Query Evaluator. Iteration numbers indicate when each optimization was introduced.\\nstoring the edge data contiguously. This dramatically\\nimproves cache utilization during traversal.\\n• BFS-based Graph Reordering:We implemented BFS\\nbased node ID reassignment, starting from the highest-\\ndegree node. This approach assigns consecutive IDs to\\nthe nodes visited together during traversal, eventually\\nimproving spatial locality when accessing adjacency\\ndata in the CSR arrays.\\n• Node ID Translation:We maintained oldToNew[] and\\nnewToOld[] mapping arrays so that incoming queries\\nusing original node IDs are correctly translated to re-\\nordered IDs before evaluation.\\n• Label-Indexed CSR:We created sep-\\narate CSR structures for each label\\n(csr label offsets[label][node] and\\ncsr label targets[label][]). This eliminated the\\nlabel comparison from the inner BFS loop entirely,\\nfacilitating direct iteration over the edges that are\\nguaranteed to have the correct label instead of checking\\nif (edge.label == targetLabel)for every edge.\\n• Redundant Storage Removal:After implementing\\nlabel-indexed CSR, the original generic CSR arrays were\\nno longer used. We removed them, reducing memory\\nfootprint and improving cache efficiency.\\nResult:Our final result was achieved when the score in-\\ncreased from 1,395 to 716 (49% improvement). The cache\\nlocality issue was fully solved by using a combination of\\nlabel-indexed structures, graph reordering, and CSR format.\\nThe final results achieved after this iteration can be found\\nusing Table 7.\\n5.4 Results Summary\\nTable 7 summarizes our complete optimization journey from\\nbaseline to final implementation.\\nT able 7: Final Leaderboard Performance: After Iteration 6\\nMetric Value\\nDiscord Username wah shampy\\nPreparation Time (Tprep) 1834.47 ms\\nEvaluation TimeT syn\\neval 29.55 ms\\nEvaluation TimeT real\\neval 564.33 ms\\nLoad Time (Tload ) 5053.52 ms\\nPeak Memory Usage (Meval) 5.67 MB\\nScore 716\\nTable 8 summarizes our optimization journey from baseline\\nto final implementation.\\n5.5 Conclusion: Part 2\\nThrough systematic optimization, we achieved a 94.8% reduc-\\ntion in score (approximately 19,800 to 716), corresponding to\\na significant reduction in evaluation time. The most impactful\\noptimizations were:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2026-02-01T15:53:53+00:00', 'author': '', 'keywords': '', 'moddate': '2026-02-01T15:53:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': 'VGTC Special Issue Paper for TVCG', 'title': '', 'trapped': '/False', 'source': './Data/PDFs/Report.pdf', 'total_pages': 10, 'page': 9, 'page_label': '9'}, page_content='T able 8: Overall Optimization Progress\\nIteration Type Score\\n1. Baseline — 19,800\\n2. Bound-Aware Eval S 7,248\\n3. Compact IR F 4,255\\n4. Direct BFS S, Q 2,063\\n5. Timestamp Tracking Q, F 1,395\\n6. CSR + Reordering Q 716\\nS = Smart, Q = Quick, F = Frugal\\n1. Label-indexed CSR:Eliminating the label check from\\nthe inner loop provided substantial gains by removing\\nbranch mispredictions and reducing the iteration count\\nto only relevant edges.\\n2. Generic CSR Format:Converting from vector-of-\\nvectors to contiguous arrays dramatically improved\\ncache locality and eliminated pointer chasing overhead.\\n3. Memory Pooling:Pre-allocating and reusing vectors\\neliminated allocation overhead that previously domi-\\nnated query processing time.\\n4. Graph Reordering:BFS-based node ID reassignment\\nimproved spatial locality by ensuring frequently co-\\naccessed nodes have nearby memory addresses.\\nFailed Optimization Attempts:Our optimization jour-\\nney also included attempts that did not yield positive results,\\nproviding valuable lessons about the gap between theoretical\\nimprovements and practical performance.\\n• We implemented asingle-label fast paththat added\\na conditional branch to bypass the label iteration loop\\nwhen a query involved only one label. In theory, this\\nshould have reduced loop overhead for the majority of\\nour queries which involved single labels. However, this\\nbranch was evaluated at every node during BFS traversal,\\nand the CPU’s branch predictor struggled with the mixed\\ntrue/false pattern, causing pipeline stalls that exceeded\\nthe time saved by skipping the loop.\\n• We attemptedfrontier sorting, where we sorted the BFS\\nfrontier by node ID before processing each level, expect-\\ning that sequential memory access to CSR arrays would\\nimprove cache hit rates. While theoretically sound, the\\nO(n log n) sorting cost at every BFS level outweighed\\nthe cache benefits, particularly since our graph reorder-\\ning optimization had already improved spatial locality at\\npreprocessing time, making runtime sorting redundant.\\nThese failures reinforced that micro-optimizations intro-\\nducing additional branches or per-level computations must be\\nempirically validated, as their overhead can compound across\\nmillions of node visits in large-scale graph traversals.\\nThe final system achieves strong performance through a\\ncombination of memory efficiency, cache-aware data struc-\\ntures, and algorithmic simplicity. These techniques are\\nbroadly applicable to graph processing systems where traver-\\nsal performance is critical.\\n6 CONCLUSION\\nThis project addressed two fundamental challenges in graph\\ndatabase systems: cardinality estimation and efficient query\\nevaluation.\\nFor cardinality estimation (Part 1), we developed a hybrid\\nestimator combining end-biased sampling, stratified reser-\\nvoir sampling, correlated hashing, and XSEED-based simula-\\ntion. This approach achieved competitive accuracy on both\\nsynthetic and real workloads while maintaining reasonable\\npreparation and estimation times.\\nFor query evaluation (Part 2), we systematically optimized\\nthe Quicksilver system through memory pooling, CSR rep-\\nresentations, graph reordering, and label-indexed data struc-\\ntures. We learned that careful attention to memory access\\npatterns and data structure design can yield dramatic perfor-\\nmance gains.\\nREFERENCES\\n[1] B. Babcock and S. Chaudhuri. Towards a robust query optimizer: A principled\\nand practical approach. InProceedings of the 2005 ACM SIGMOD International\\nConference on Management of Data, pages 119–130, Baltimore, MD, USA, 2005.\\nACM.\\n[2] S. Beamer, K. Asanovic, and D. Patterson. Direction-optimizing breadth-first\\nsearch. InProceedings of the International Conference for High Performance\\nComputing, Networking, Storage and Analysis (SC), Salt Lake City, Utah, USA,\\nNov. 2012. IEEE.\\n[3] G. Cormode. Synopses for massive data: Samples, histograms, wavelets, sketches.\\nF oundations and Trends in Databases, 4(1–3):1–294, 2011.\\n[4] L. Dhulipala, J. Shi, T. Tseng, G. E. Blelloch, and J. Shun. The graph based\\nbenchmark suite (gbbs). In3rd Joint International Workshop on Graph Data\\nManagement Experiences & Systems (GRADES) and Network Data Analytics\\n(NDA) (GRADES-NDA’20), Portland, OR, USA, June 2020. ACM.\\n[5] C. Estan and J. F. Naughton. End-biased samples for join cardinality estima-\\ntion. InProceedings of the 22nd International Conference on Data Engineering\\n(ICDE’06), page 20, Atlanta, GA, USA, 2006. IEEE.\\n[6] F. Li, B. Wu, K. Yi, and Z. Zhao. Wander join: Online aggregation via random\\nwalks. InProceedings of the 2016 ACM SIGMOD International Conference on\\nManagement of Data, pages 615–629, San Francisco, CA, USA, 2016. ACM.\\n[7] T. Neumann and G. Moerkotte. Characteristic sets: Accurate cardinality esti-\\nmation for RDF queries with multiple joins. In2011 IEEE 27th International\\nConference on Data Engineering (ICDE 2011), pages 984–994, Hannover, Ger-\\nmany, Apr. 2011. IEEE.\\n[8] J. Shun and G. E. Blelloch. Ligra: A lightweight graph processing framework\\nfor shared memory. InProceedings of the 18th ACM SIGPLAN Symposium on\\nPrinciples and Practice of Parallel Programming (PPoPP). ACM, 2013.\\n[9] H. Wei, J. X. Yu, C. Lu, and X. Lin. Speedup graph processing by graph ordering.\\nInProceedings of the 2016 International Conference on Management of Data\\n(SIGMOD), San Francisco, CA, USA, 2016. ACM.\\n[10] M. Winter, D. Mlakar, R. Zayer, H.-P. Seidel, and M. Steinberger. faimgraph:\\nHigh performance management of fully-dynamic graphs under tight memory\\nconstraints on the gpu. InProceedings of SC18: International Conference for\\nHigh Performance Computing, Networking, Storage and Analysis, Dallas, Texas,\\nUSA, Nov. 2018. IEEE.\\n[11] N. Zhang, M. Ozsu, A. Aboulnaga, and I. Ilyas. Xseed: Accurate and fast\\ncardinality estimation for xpath queries. In22nd International Conference on\\nData Engineering (ICDE’06), pages 61–61, 2006.')]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_pdf = pdf_loader.load()\n",
    "loaded_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "9c95ca11-255a-4abc-ae8f-6d7aeea68393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of documents\n",
    "type(loaded_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "50a770a9-b41b-4124-9652-f1317cf6fd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering Data Intensive Systems - 2IMD10\n",
      "EDS - PROJECTREPORT\n",
      "Team Number - 16\n",
      "Full Name Discord Username Email\n",
      "Divyansh Purohit wah shampy d.purohit@student.tue.nl\n",
      "Likhit Vesalapu likhit7. l.vesalapu@student.tue.nl\n",
      "Prathamesh Samal viper 101 p.samal@student.tue.nl\n",
      "Elena Terzieva ellie218388 e.e.terzieva@student.tue.nl\n",
      "Eindhoven, February 1, 2026\n",
      "\n",
      "1 ABSTRACT\n",
      "Accurate cardinality estimation is fundamental to query opti-\n",
      "mization in graph databases, enabling the selection of efficient\n",
      "execution plans for regular path queries. In this report, we\n",
      "present a hybrid cardinality estimator that combines multi-\n",
      "ple statistical synopses including per-label statistics, pairwise\n",
      "label correlations, and characteristic sets with a weighted\n",
      "and stratified sampling strategy for complex queries. Our ap-\n",
      "proach balances estimation accuracy against preparation time\n",
      "and memory overhead, achieving competitive performance\n",
      "on both synthetic and real-world workloads.\n",
      "2 INTRODUCTION\n",
      "The efficiency of query processing is an important topic in\n",
      "the domain of databases, where cardinality estimation plays\n",
      "a central role. Modern graph and XML database systems\n",
      "rely on cardinality estimators as a core component of their\n",
      "cost based query optimizers. A cardinality estimator is a\n",
      "statistical module that, given an abstract description of a query\n",
      "(a labeled path pattern starting from some node and ending\n",
      "at another), predicts how many results this query is likely to\n",
      "produce. These predictions are crucial for choosing efficient\n",
      "execution plans, selecting join orders, and deciding which\n",
      "physical operators to use. Without an accurate cardinality\n",
      "estimator, an optimizer can easily choose plans that are orders\n",
      "of magnitude slower than necessary.\n",
      "Traditional cardinality estimators for path queries are based\n",
      "on compact summaries of structural synopses that record\n",
      "per-label statistics, degree distributions, and limited informa-\n",
      "tion about how labels coincide. Although such synopses can\n",
      "be very effective for short, simple path patterns, their accuracy\n",
      "rapidly degrades for long paths, patterns with repetition, or\n",
      "data with strong structural correlations. In these cases, naive\n",
      "independence assumptions break down and purely synopsis\n",
      "based estimators can produce large errors.\n",
      "To address these challenges, our work investigates a sam-\n",
      "pling based cardinality estimator that involves four com-\n",
      "plementary approaches: Stratified reservoir sampling, End-\n",
      "biased sampling, Correlated sampling, XSEED based Simula-\n",
      "tion.\n",
      "3 LITERATURESURVEY\n",
      "The topic of cardinality estimation continues to be one of the\n",
      "most important in query optimization. There are different\n",
      "techniques that have been developed for estimating the size\n",
      "of the results produced by a query. In this project, we are\n",
      "dealing with RDF databases that differ from the typical re-\n",
      "lational schemas because of their structure, which leads to\n",
      "different assumptions and decisions made when estimating\n",
      "the cardinality of a query. In query optimisation, synopses are\n",
      "precomputed summaries of database data (like histograms,\n",
      "sketches, samples) that database systems use to quickly es-\n",
      "timate the cost of different query execution plans. Bonifati\n",
      "et al. survey cardinality estimation techniques specifically\n",
      "designed for graph query languages. The two approaches\n",
      "discussed are cardinality of paths and cardinality on patterns.\n",
      "With cardinality on paths, the query path is split into simpler\n",
      "segments and predefined formulas are used for concentra-\n",
      "tion, union and inversion are combined to estimate the result.\n",
      "Cardinality on patterns, on the other hand focuses on graph\n",
      "patterns with characteristic sets or graph summaries. With\n",
      "characteristic sets, each node is summarised by the set of\n",
      "outgoing edge labels it has, and these “characteristic sets” are\n",
      "used to estimate how many nodes (and joins) participate in\n",
      "a given pattern. They are discussed in a study by Neumann\n",
      "and Moerkotte, where they observe that in RDF databases,\n",
      "many subjects share the same set of outgoing predicates, and\n",
      "define a characteristic set as exactly this set of predicates\n",
      "attached to a subject [7]. With graph summarizers, the graph\n",
      "is compressed into a smaller summary (by grouping similar\n",
      "nodes/edges), and cardinalities for complex path or subgraph\n",
      "patterns are estimated by evaluating the query on this sum-\n",
      "mary instead of on the full graph.\n",
      "Two statistical methods for dealing with that are sampling\n",
      "and histograms. Estan and Naughton propose end-biased\n",
      "samples for join cardinality estimation in relational databases.\n",
      "The method retains tuples with probability proportional to\n",
      "their frequency in join columns, so that high-frequency val-\n",
      "ues (responsible for most join output) are over-represented.\n",
      "This yields much more accurate join size estimates than uni-\n",
      "form sampling at comparable sample sizes [5]. Babcock and\n",
      "Chaudhuri introduce a technique based on Bayesian inference\n",
      "over precomputed random samples, which models uncertainty\n",
      "and produces estimates together with confidence information.\n",
      "Their method captures multi-dimensional correlations without\n",
      "assuming attribute independence and remains time-efficient.\n",
      "Importantly, it supports user- or application-specified trade-\n",
      "offs between performance and predictability, contributing to\n",
      "the design of more robust query optimisers [1].\n",
      "Histograms are one of the most widely used methods for\n",
      "cardinality estimation in relational DBMS. The basic idea\n",
      "is to partition the domain of an attribute into buckets and\n",
      "store, for each bucket, the number of tuples (and sometimes\n",
      "additional stats such as distinct values). Query selectivities\n",
      "are then approximated by assuming a uniform distribution of\n",
      "values inside each bucket and summing contributions from\n",
      "the buckets that overlap the predicate. A detailed survey\n",
      "of histogram-based and related synopsis techniques is given\n",
      "by Cormode, who reviews equi-width, equi-depth and more\n",
      "advanced frequency-based histograms, as well as their multi-\n",
      "dimensional extensions and maintenance trade-offs [3]. Joins\n",
      "can be very expensive, and to try to bridge the gap between\n",
      "query efficiency and accuracy, Li et al. introduced the new\n",
      "algorithm Wander Join [6]. Wander Join is a join estimation\n",
      "method that treats a multi-table join like a graph and then\n",
      "performs random walks through the graph instead of com-\n",
      "puting the full join. Since it only touches a small number\n",
      "of data points and does not need to compute statistics for\n",
      "the full data, the approximation results are produced quickly\n",
      "and more accurately with the increase in performed random\n",
      "\n",
      "walks.\n",
      "4 IMPLEMENTATION\n",
      "4.1 Approach and Planning\n",
      "4.1.1 Problem Analysis\n",
      "In order to address the problem of cardinality estimation for\n",
      "path queries, our team first examined the basic properties of\n",
      "graph data. Especially in networks, graph data shows heavy\n",
      "tailed, power-law distribution, variable-length traversals and\n",
      "Kleene closures in comparison to relational tables where data\n",
      "usually follows uniform distribution. The scoring metric for\n",
      "this project emphasizes the accuracy on real workloads with\n",
      "a weight of two and also penalizing the run times. This\n",
      "tuned our strategy to get accurate estimates without expensive\n",
      "computation.\n",
      "The provided graph contained 10,000 nodes, 22,122 edges,\n",
      "and 36 distinct edge labels. With an average out-degree of\n",
      "approximately 2.92 and a maximum out-degree of 43, the\n",
      "graph is relatively sparse (density approx 0.02%). The de-\n",
      "gree distribution follows a classic power-law pattern: 37% of\n",
      "nodes have out-degree 1, while only a handful of hub nodes\n",
      "exceed degree 20. This skewed distribution means that a small\n",
      "number of high-degree nodes disproportionately contribute\n",
      "to path counts, making uniform sampling ineffective.\n",
      "The label distribution is similarly skewed, with label 8\n",
      "appearing in over 5,000 edges (23% of all edges), while\n",
      "12 labels appear fewer than 50 times each. This imbalance\n",
      "affects join selectivity and makes certain label combinations\n",
      "far more common than others. The query workload spanned\n",
      "diverse patterns:\n",
      "•Single predicates: e.g.,,1>,,,8>,\n",
      "• Concatenations: e.g., ,1>/8>,, ,1>/8>/9>/8>, (up\n",
      "to length 5)\n",
      "•Kleene closures: e.g.,,(1>)+,,,(8>)+,\n",
      "•Mixed patterns: e.g.,,1>/(8>)+/(9>)+,\n",
      "•Label unions: e.g.,,(0>|1>|2>|3>|4>)+,\n",
      "• Bound source queries: e.g., 200,1>/8>,,\n",
      "227,(1>)+/(8>)+,\n",
      "The scoring metric for this project emphasizes accuracy on\n",
      "real workloads with a weight of two while also penalizing\n",
      "preparation and estimation time. This guided our strategy: we\n",
      "needed accurate estimates without expensive runtime compu-\n",
      "tation.\n",
      "4.1.2 Technique Selection Based on Liter-\n",
      "ature Survey\n",
      "Based on our literature review, we selected the following\n",
      "approaches for our hybrid estimation model:\n",
      "• End-biased Sampling:This strategy was chosen in\n",
      "order to address the variation brought on by power-law\n",
      "distributions. We reasoned that probabilistic estimate\n",
      "is not appropriate for frequent values (hubs), since they\n",
      "have the greatest influence on join size and variance. We\n",
      "eliminate the main cause of instability in the estimator\n",
      "by explicitly isolating and tracking the top- K highest-\n",
      "degree nodes using exact logic, leaving a lower-variance\n",
      "population (the tail) that is easier to sample.\n",
      "• Stratified Reservoir Sampling:Rare edge labels are\n",
      "not guaranteed to be covered by a global random sam-\n",
      "ple. To make sure that every potential transition in the\n",
      "graph has representative start nodes, we decided to use\n",
      "stratified reservoirs, which allocate a predetermined sam-\n",
      "ple budget per unique label. In order to reduce endless\n",
      "failures on queries with rare predicates, this choice was\n",
      "crucial.\n",
      "• Correlated Sampling by Hashing:Since independent\n",
      "random sampling statistically fails to capture the inter-\n",
      "section of joins in sparse samples, we ignored it for the\n",
      "tail. We make sure that sample choices are consistent\n",
      "across various query contexts by using a deterministic\n",
      "hashing approach as suggested by Estan et al. given as\n",
      "follows:\n",
      "h(v)≤p v (1)\n",
      "where pV is the sampling probability for a given value\n",
      "of v and h is the hashing function which maps values in\n",
      "the range[0,1].\n",
      "• XSEED-based Seed Simulation:In clustered graphs,\n",
      "the assumption of independence between path steps is\n",
      "broken by standard histograms. The idea of employing\n",
      "bound nodes as ”seeds” for cardinality estimation is pre-\n",
      "sented in the XSEED paper [11]. We incorporated this\n",
      "idea into our Sampling Model, treating our Stratified\n",
      "Samples as ”seeds” and executing the query path from\n",
      "them. This enables us to capture the structural relation-\n",
      "ships in the graph that are missed by static statistical\n",
      "methods.\n",
      "We decided to pursue with a Hybrid Cardinality Estimator\n",
      "that adjusts to the structural skew of the graph by combining\n",
      "the above mentioned approaches. By dynamically replicat-\n",
      "ing graph traversals on a subgraph, this synergy enables the\n",
      "system to get around the drawbacks of static statistics and\n",
      "balance the accuracy with the runtime efficiency of sampling.\n",
      "4.1.3 Expected Challenges\n",
      "We anticipated several challenges:\n",
      "• Balancing accuracy and speed: More sophisticated\n",
      "statistics and complex simulations improve accuracy\n",
      "but increase both preparation and estimation time. In\n",
      "\n",
      "addition, reservoir sizes and the estimation time penalty\n",
      "had to be carefully balanced. We anticipated that wander\n",
      "join approaches, which use random walks to estimate\n",
      "join cardinalities, would greatly benefit accuracy but\n",
      "adversely affect estimation time.\n",
      "• Handling Kleene closures: Single predicate queries\n",
      "like 1 and concatenation queries like 1 /2 were straight-\n",
      "forward and could be directly computed from our stored\n",
      "label statistics and join synopses. However, transitive\n",
      "closure queries can produce result sets ranging from\n",
      "empty to quadratic in graph size, making estimation\n",
      "inherently difficult.\n",
      "• Correlation capture: Independence assumptions be-\n",
      "tween path segments would lead to systematic over or\n",
      "under estimation, but capturing accurate and extensive\n",
      "correlations would require exhaustive precomputations.\n",
      "• Memory constraints: Storing detailed statistics for all\n",
      "label pairs (pairwise correlations) would scale quadrati-\n",
      "cally with the number of labels in the graph.\n",
      "4.2 Implementation Progress and Results\n",
      "We used an iterative life cycle for our implementation. Before\n",
      "finalizing our ultimate End-Biased Stratified architecture, we\n",
      "ran our experiments with a number of approaches, including\n",
      "a computationally costly Random Walk attempt.\n",
      "4.2.1 Iteration 1: Basic Synopsis\n",
      "We began by implementing foundational statistics computed\n",
      "in a single pass over the graph. This included edge counts\n",
      "per label and a matrix to store pairwise correlations between\n",
      "labels. By adding up the product of degrees, we were\n",
      "able to determine the precise number of pathways for any\n",
      "length-2 label combination. A join statistics matrix was also\n",
      "introduced to capture the number of two-hop paths between\n",
      "all label pairs, providing exact estimates for length-2 queries.\n",
      "Result:This baseline achieved reasonable accuracy\n",
      "on simple queries but struggled with complex patterns and\n",
      "showed high variance on longer paths.\n",
      "4.2.2 Iteration 2: Wander Joins and Ran-\n",
      "dom Walks\n",
      "We implemented a Wander Join estimator to increase accuracy\n",
      "for Kleene closures and long pathways. In order to naturally\n",
      "approximate the path distribution for each query, we con-\n",
      "ducted random walks beginning from a collection of sampled\n",
      "nodes. To further refine this approach, we explored several ad-\n",
      "vanced adaptive sampling techniques: the Filtering-Sampling\n",
      "Approach (FaSTest), that dynamically adjusted sample sizes\n",
      "based on query characteristics, along with an adaptive stop-\n",
      "ping criteria that terminated walks once the 95% confidence\n",
      "interval around the Horvitz-Thompson estimate fell below a\n",
      "5% error threshold, thereby saving time on dense queries.\n",
      "Result:Since this method physically traversed the graph\n",
      "structure to capture correlations, this method produced re-\n",
      "markable accuracy, however estimation time (Test) grew sig-\n",
      "nificantly. Even with our optimizations, a large number of\n",
      "walks were required in order to get stable estimates, which\n",
      "went against the time constraints set up for this project. In-\n",
      "stead, we opted for a pre calculated sampling strategy.\n",
      "4.2.3 Iteration 3: Hybrid Approach\n",
      "Our estimating model was developed through an ordered\n",
      "breakdown of the problem rather than being chosen as a sin-\n",
      "gle technique. In order to address particular data features, we\n",
      "integrated various approaches in three logical steps.\n",
      "Step 1: Decomposition by DegreeThe existence of high-\n",
      "degree nodes is the main cause of errors in graph estima-\n",
      "tion. We developed a split-execution model based on the End-\n",
      "Biased Sampling concepts. We proposed treating the graph\n",
      "G as Ghead and Gtail , into two separate subgraphs. Since any\n",
      "probabilistic variance here would propagate quadratically dur-\n",
      "ing joins, we concluded that Ghead (the top-K nodes) must\n",
      "be effectively retained in memory and queried accurately. In\n",
      "Gtail , where the variance is inherently lower, the probabilistic\n",
      "estimation is strongly restricted.\n",
      "Step 2: Handling SparsityStandard sampling is ineffec-\n",
      "tive for rare predicates in path queries, despite Estan et al.’s\n",
      "suggestion to weight by frequency. A global sample will be\n",
      "unable to capture a label L if it is present in only 0.1% of the\n",
      "graph. The sampling logic was expanded to be stratified. Each\n",
      "edge label is treated as a separate population strata. Regard-\n",
      "less of how uncommon the edge label is, we mathematically\n",
      "guarantee that the probability of providing a ”zero-estimate”\n",
      "for a valid query path is reduced by guaranteeing a constant\n",
      "reservoir size for each label.\n",
      "Step 3: Correlation PreservationIn essence, a path query\n",
      "X→Y→Z is an intersection problem. The fundamental cor-\n",
      "relations needed to compute these crossings are eliminated by\n",
      "independent random sampling. We decided to use Correlated\n",
      "Hashing. In case, a node exists in the reservoir for label A (as\n",
      "a target), it will also be present in the reservoir for label B (as\n",
      "a source), as long as it satisfies the probability threshold, by\n",
      "making the inclusion of a node into a reservoir a deterministic\n",
      "function of its ID (h(v)≤p v). This maintains the linkability.\n",
      "Step 4: Simulation-Based EstimationFinally, we used the\n",
      "Seed-Based approach from the XSEED paper to solve for\n",
      "complex query types that defy static formulas, such as Kleene\n",
      "closures (+). In addition to being statistical representatives,\n",
      "we thought of our stratified samples as ”active seeds.” Rather\n",
      "than merging histograms, the estimator measures reachability\n",
      "by physically running the query from these seeds and scal-\n",
      "ing the outcome by the inverse of the inclusion probability\n",
      "(1/p v).\n",
      "Result:This approach achieved the best trade-off between\n",
      "accuracy and estimation time on simple as well as complex\n",
      "queries. queries. The only drawback of this approach was that\n",
      "it occupied significantly more memory, which was further\n",
      "\n",
      "refined in the next iteration.\n",
      "4.2.4 Iteration 4: Optimization\n",
      "The final iteration focused on performance optimization with-\n",
      "out sacrificing on the accuracy. This iteration was done in\n",
      "order to reduce Estimation Time (Test) and Preparation Time\n",
      "(Tprep).\n",
      "• Binary Search for Adjacency:It was inefficient to\n",
      "search adjacency lists linearly for edges that matched a\n",
      "given start label for the high-degree top-k nodes. Using\n",
      "binary search on the sorted edge lists, we optimized the\n",
      "sampling algorithm. This resulted in a targeted speedup\n",
      "for the most frequently used nodes by lowering the edge\n",
      "lookup difficulty from linear to logarithmic.\n",
      "• Memory Reuse:Our code initially generated fresh tem-\n",
      "porary lists for each node it handled during the prepa-\n",
      "ration stage. This required more memory and was slow\n",
      "due to large number of nodes. These list creations were\n",
      "relocated outside of the main loop. The same memory\n",
      "block is reused repeatedly since we only need to gener-\n",
      "ate the list once and then clear it after processing each\n",
      "node. As a result, the overhead of continuous memory\n",
      "allocation was significantly reduced.\n",
      "4.2.5 Results\n",
      "Our final hybrid estimator shows a good trade-off between\n",
      "runtime and estimation accuracy. The leaderboard’s perfor-\n",
      "mance metrics are as follows\n",
      "T able 1: Final Leaderboard Performance\n",
      "Metric Value\n",
      "Discord Username viper 101\n",
      "Synthetic Accuracy (Eqsyn) 1.18\n",
      "Real-Workload Accuracy (Eqreal) 8.86\n",
      "Preparation Time (Tprep) 153.88 ms\n",
      "Estimation Time (Test) 32.58 ms\n",
      "Peak Memory Usage (M) 24.07 MB\n",
      "Final Score 49.63\n",
      "5 PART2: EFFICIENTQUERYEVALUATION\n",
      "In Part 2 of the project, we shift focus from cardinality es-\n",
      "timation to efficient query evaluation. The goal is to make\n",
      "QuicksilverSQUIFfy: Smart, Quick, and Frugal. We focused\n",
      "heavily on balancing the trade-offs between execution speed,\n",
      "preparation time, and strict memory limits. We didn’t just\n",
      "want it to be fast; we wanted it to be robust and memory\n",
      "efficient as per Frugal requirement.\n",
      "5.1 Strategy\n",
      "Our optimization strategy followed a systematic approach\n",
      "inspired by research into high-performance graph processing\n",
      "systems. We categorized our efforts into three pillars: Smart\n",
      "(choosing optimal algorithms), Quick (reducing computation\n",
      "time), and Frugal (minimizing memory overhead).\n",
      "5.1.1 Smart\n",
      "We made informed decisions based on query patterns and\n",
      "empirical benchmarking:\n",
      "• Separate Bound Source/Target Handlers:We used\n",
      "forward BFS traversal for queries with bound sources\n",
      "and backward BFS from the target for queries with\n",
      "bound targets. This method helped us to avoid full-graph\n",
      "traversals and significantly reduced the search space.\n",
      "• Node ID Mapping:We maintained a translation table\n",
      "to map the source and target node IDs from original to\n",
      "reordered IDs at evaluation time so that when the graph\n",
      "is reordered (discussed in Quick) and nodes get new\n",
      "IDs, the evaluator continues work with consistent node\n",
      "identifiers.\n",
      "5.1.2 Quick: Reducing Computation Time\n",
      "Here our primary focus was eliminating redundant work and\n",
      "improving cache efficiency:\n",
      "• Label-indexed CSR:We pre-partition edges by label\n",
      "during graph loading instead of filtering them during\n",
      "BFS, which eliminated the label check from the hottest\n",
      "code path. This reduced branch mispredictions and im-\n",
      "proved instruction level parallelism.\n",
      "• Graph Reordering:We implemented BFS based node\n",
      "ID reassignment, starting from the highest-degree node.\n",
      "This approach assigns consecutive IDs to the nodes vis-\n",
      "ited together during traversal, eventually improving spa-\n",
      "tial locality when accessing adjacency data in the CSR\n",
      "arrays.\n",
      "• Tracking visited nodes using timestamp:Rather than\n",
      "clearing the visited array between queries (O(V) oper-\n",
      "ation), we used a monotonically increasing timestamp\n",
      "counter to indicate the status of a node. We marked a\n",
      "node with the current timestamp when visiting it, and\n",
      "the node is considered unvisited if its timestamp doesn’t\n",
      "match the current query’s timestamp (O(1) operation).\n",
      "5.1.3 Frugal: Memory Efficiency\n",
      "Here we focused on minimizing dynamic memory operations\n",
      "because memory allocation and deallocation are expensive\n",
      "operations.\n",
      "• Memory Pooling:All vectors used during BFS traversal\n",
      "were pre-allocated once during the prepare phase and\n",
      "reused across all queries via swap() operations rather\n",
      "than repeated allocation and deallocation.\n",
      "• CSR Format:We converted adjacency lists to Com-\n",
      "pressed Sparse Row format, storing edges in contiguous\n",
      "arrays indexed by node ID, to reduce memory fragmen-\n",
      "tation and improve cache utilization.\n",
      "\n",
      "5.2 Literature Survey\n",
      "Work on fast graph query evaluation repeatedly shows that\n",
      "performance depends less on high-level query language rules\n",
      "and more on how traversal is executed: reducing wasted\n",
      "exploration, using cache-friendly graph layouts, and avoiding\n",
      "overhead inside the traversal loop. Shared-memory graph\n",
      "systems and RDF engines especially emphasize contiguous\n",
      "adjacency storage, locality-aware preprocessing, and access-\n",
      "path choices that prevent repeated filtering during evaluation.\n",
      "In direction-optimizing BFS, Beamer et al. show that\n",
      "switching between traversal styles can dramatically reduce\n",
      "the number of edges examined on real “small-world” graphs,\n",
      "which is exactly the problem when a naive traversal expands\n",
      "too broadly. Their key message is that how you traverse (direc-\n",
      "tion/strategy decisions) can matter as much as raw low-level\n",
      "optimization, because it directly shrinks the explored search\n",
      "space [2]. Moreover, when graph preprocessing changes ver-\n",
      "tex IDs (for example after reordering), systems typically need\n",
      "a stable way to preserve semantics across representations.\n",
      "In the graph ordering work by Wei et al., the goal is to re-\n",
      "arrange vertex order to improve cache behavior. However,\n",
      "this implicitly creates a “logical graph” (original IDs) and a\n",
      "“physical graph” (reordered IDs). The paper motivates the\n",
      "practical need to keep query evaluation correct even when\n",
      "the physical layout changes — conceptually the same reason\n",
      "many systems maintain an ID mapping layer around reordered\n",
      "graphs [9].\n",
      "For traversal-heavy workloads, research shows that perfor-\n",
      "mance is often memory-bound, so the fastest solutions focus\n",
      "on contiguous layouts and minimizing work in the tight loop.\n",
      "Ligra is a well-known shared-memory framework demon-\n",
      "strating that graph traversal becomes significantly faster when\n",
      "adjacency is stored and processed in cache-friendly ways,\n",
      "enabling efficient neighbor scans and reducing overhead from\n",
      "pointer-heavy structures. Although Ligra is a framework, its\n",
      "results strongly support the idea that CSR-like representations\n",
      "and streamlined traversal primitives are a practical baseline\n",
      "for high throughput BFS-style processing [8]. A second line\n",
      "of work targets locality directly via graph reordering. Wei et\n",
      "al. show that reordering vertices can reduce CPU cache miss\n",
      "ratios across several graph algorithms, producing speedups\n",
      "without changing the underlying algorithm. Their contribu-\n",
      "tion is important for query evaluation because it treats graph\n",
      "layout as an optimization target: by reassigning IDs so that\n",
      "vertices accessed close in time are also close in memory, the\n",
      "cost of adjacency and metadata access drops [9]. Visited-\n",
      "state handling is another recurring bottleneck in fast traversal.\n",
      "Beamer et al. discuss optimizations such as using compact\n",
      "visited representations (for example bitmaps) to reduce costly\n",
      "random accesses and keep visited checks efficient on shared-\n",
      "memory machines. The point is that even “small” bookkeep-\n",
      "ing operations can dominate runtime when done at scale, so\n",
      "visited tracking must be engineered carefully [2]. A large\n",
      "part of the memory efficiency in graph query engines comes\n",
      "from keeping data structures compact and avoiding repeated\n",
      "allocations during query execution. Many systems therefore\n",
      "rely on two practical ideas such as storing adjacency in a con-\n",
      "tiguous sparse format (typically CSR), and pre-allocating and\n",
      "reusing working buffers instead of allocating/freeing mem-\n",
      "ory for every query. Several systems make this point even\n",
      "more explicitly by standardizing around CSR as the baseline\n",
      "representation due to its compactness and predictable access\n",
      "pattern. Dhulipala et al. (GBBS) state that graphs in the suite\n",
      "are stored in Compressed Sparse Row (CSR), where neigh-\n",
      "bor lists live in contiguous arrays with an offset array per\n",
      "node. This design reduces fragmentation and supports cache-\n",
      "friendly scans. The key motivation is that pointer-heavy adja-\n",
      "cency lists create overhead and irregular access, while CSR\n",
      "improves both memory footprint and traversal throughput [4].\n",
      "Many high-performance systems reduce memory overhead\n",
      "by minimizing dynamic allocations during execution. Winter\n",
      "et al. (faimGraph) present a memory-management approach\n",
      "designed for fully-dynamic graphs on the GPU. While faim-\n",
      "Graph is GPU-focused, its core principle—using one large\n",
      "pre-allocation to avoid the overhead and fragmentation of\n",
      "repeated memory allocation or vector resizing—maps cleanly\n",
      "to CPU implementations. We adopted this by pre-allocating\n",
      "our working buffers once to keep runtime memory behavior\n",
      "stable. [10].\n",
      "5.3 Implementation Progress\n",
      "We followed an iterative approach for optimizing our per-\n",
      "formance, implementing and benchmarking each technique\n",
      "independently to measure its overall impact. In the sections\n",
      "below, we explain each optimization iteration experiments.\n",
      "We also report the performance metrics achieved in each itera-\n",
      "tion, along with a improvement percentage from the previous\n",
      "iteration (represented by ∆). A positive value indicates im-\n",
      "provement in the performance parameter whereas a negative\n",
      "value represents decrease in the performance.\n",
      "5.3.1 Iteration 1: Baseline Analysis\n",
      "The baseline implementation used standard adjacency lists\n",
      "with vectors of (label, target) pairs. Each query allocated\n",
      "fresh vectors for BFS traversal and used a boolean visited\n",
      "array that was cleared between queries. The simple evaluator\n",
      "performed linear scans through the adjacency lists, checking\n",
      "each edge’s label against the query predicate.\n",
      "Results:Profiling revealed several bottlenecks: re-\n",
      "peated memory allocation for each query, O(V) cost to clear\n",
      "the visited array, poor cache locality due to scattered memory\n",
      "access, and redundant label checks in the inner traversal loop.\n",
      "5.3.2 Iteration 2: Bound-Aware Evaluation\n",
      "and Semi-Naive Transitive Closure\n",
      "After the experiments on our baseline, w e found that queries\n",
      "with bound sources or targets continued to unnecessarily tra-\n",
      "verse the whole graph. Furthermore, Kleene star searches\n",
      "\n",
      "T able 2: Iteration 1: Baseline Performance\n",
      "Metric Value\n",
      "Preparation Time (Tprep) 145.87 ms\n",
      "Evaluation TimeT syn\n",
      "eval 1213.70 ms\n",
      "Evaluation TimeT real\n",
      "eval 17993.72 ms\n",
      "Load Time (Tload ) 3985.82 ms\n",
      "Peak Memory Usage (Meval) 29.48 MB\n",
      "Score ∼19,800\n",
      "were creating duplicate work by recalculating every reachable\n",
      "node at each cycle.\n",
      "• Bound-Aware Evaluation:We implemented separate\n",
      "independent evaluation handlers for queries with bound\n",
      "sources and bound targets. Instead of traversing the\n",
      "entire graph, we start a breadth first search from the\n",
      "single known node, significantly reducing the search\n",
      "space for constrained queries.\n",
      "• Semi-Naive Transitive Closure:We replaced the naive\n",
      "transitive closure algorithm with a semi-naive approach\n",
      "for Kleene star queries. Instead of recalculating all\n",
      "reachable nodes at each iteration, we process only newly\n",
      "discovered edges, avoiding redundant computation and\n",
      "significantly improving performance for recursive path\n",
      "patterns.\n",
      "Results:The score improved from ˜19,800 to 7,248 (63%\n",
      "improvement). This confirmed that algorithmic issues were\n",
      "the major bottleneck. However, this also revealed that signifi-\n",
      "cant time was still being spent on constructing and managing\n",
      "intermediate result graphs, hinting us towards data structure\n",
      "optimizations.\n",
      "T able 3: Iteration 2: Bound-Aware Evaluation\n",
      "Metric Value ∆\n",
      "Preparation Time (Tprep) 1505.31 ms −932%\n",
      "Eval Time Syn (Tsyn\n",
      "eval) 341.26 ms +72%\n",
      "Eval Time Real (Treal\n",
      "eval ) 6403.16 ms +64%\n",
      "Load Time (Tload ) 4092.68 ms −3%\n",
      "Peak Memory (Meval) 24.87 MB +16%\n",
      "Score 7,248 +63%\n",
      "5.3.3 Iteration 3: Compact Intermediate\n",
      "Results\n",
      "After the algorithmic changes were implemented, profiling\n",
      "showed that creating and maintaining intermediate result\n",
      "graphs took a considerable amount of time. We observed\n",
      "that these intermediary structures kept duplicate data: reverse\n",
      "adjacency lists were kept even if joins only traveled forward,\n",
      "and labels were kept even though edges had already been\n",
      "filtered. To subside these issues, we used the following ap-\n",
      "proaches:\n",
      "• CompactIR Data Structure:We introduced a\n",
      "lightweight intermediate result structure that stores only\n",
      "source-to-target mappings. We removed labels from in-\n",
      "termediate results since once edges are filtered by label\n",
      "during selection, storing the label becomes redundant.\n",
      "We also eliminated the reverse adjacency list because\n",
      "join operations only traverse forward, never requiring\n",
      "backward lookups on intermediate data.\n",
      "• Deferred Deduplication:We also removed per-insert\n",
      "duplicate checking, which required expensive lookups\n",
      "on every edge addition. Instead, we allowed duplicates\n",
      "during construction and performed a batch sort-and-\n",
      "deduplicate operation at the end by using finalize().\n",
      "This approach is more cache-friendly and significantly\n",
      "reduces computation time.\n",
      "• Hash-based Existence Checking:For transitive closure\n",
      "computation, we replaced linear search with hash sets\n",
      "for tracking visited edges, reducing existence checks\n",
      "from O(N) to O(1) and speeding up Kleene star queries.\n",
      "Result:The score improved from 7,248 to 4,255 (41% im-\n",
      "provement). Memory usage and computation time were de-\n",
      "creased by eliminating redundant storage and delaying dedu-\n",
      "plication.\n",
      "T able 4: Iteration 3: Compact Intermediate Results\n",
      "Metric Value ∆\n",
      "Preparation Time (Tprep) 1476.81 ms +2%\n",
      "Eval Time Syn (Tsyn\n",
      "eval) 212.29 ms +38%\n",
      "Eval Time Real (Treal\n",
      "eval ) 3828.77 ms +40%\n",
      "Load Time (Tload ) 3911.26 ms +4%\n",
      "Peak Memory (Meval) 10.40 MB +58%\n",
      "Score 4,255 +41%\n",
      "5.3.4 Iteration 4: Direct BFS Traversal\n",
      "In the previous iteration, our approach continued to materi-\n",
      "alize intermediate result graphs and carry out explicit join\n",
      "operations in spite of the CompactIR enhancements. Since\n",
      "we only want the final reachable nodes for path queries, not\n",
      "the intermediary edges, we thought if this abstraction was\n",
      "even essential. As a result, we completely stopped using\n",
      "intermediate graph building.\n",
      "• Removed Intermediate Graph Construction:We\n",
      "eliminated the CompactIR data structure and join op-\n",
      "erations entirely. Instead of building intermediate result\n",
      "graphs and joining them, we now traverse the query path\n",
      "directly while maintaining only the current frontier. This\n",
      "avoids materializing all intermediate edges, significantly\n",
      "reducing memory usage and computation time.\n",
      "• Specialized Evaluation Paths:We implemented three\n",
      "separate evaluation functions: forward BFS for bound-\n",
      "source queries, backward BFS for bound-target queries,\n",
      "\n",
      "and multi-source traversal for unbound queries. Each\n",
      "path is optimized for its specific access pattern.\n",
      "• Bitvector-based Tracking:We replaced hash sets with\n",
      "bitvectors for tracking frontier membership and visited\n",
      "nodes. For dense node ID spaces, bitvectors provide\n",
      "O(1) lookups with better cache locality than hash-based\n",
      "structures.\n",
      "• Label-indexed Edge Lists:Precomputed lists were\n",
      "added that provide direct access to all edges with a given\n",
      "label, eliminating the need to scan entire adjacency lists\n",
      "and filter by label during unbound evaluation.\n",
      "• Memory Optimizations:vector pre-reservation were\n",
      "added to reduce reallocations during BFS and used\n",
      "move semantics to avoid unnecessary vector copies when\n",
      "swapping frontiers.\n",
      "Result:The score improved from 4,255 to 2,063 (52%\n",
      "improvement). Eliminating intermediate graph construction\n",
      "was a major win, confirming that the join-based abstraction\n",
      "was unnecessary overhead. However, experiments showed\n",
      "a new bottleneck: the unbound evaluation was allocating\n",
      "thousands of bitvectors per query, one for each active source.\n",
      "T able 5: Iteration 4: Direct BFS T raversal\n",
      "Metric Value ∆\n",
      "Preparation Time (Tprep) 1451.41 ms +2%\n",
      "Eval Time Syn (Tsyn\n",
      "eval) 55.70 ms +74%\n",
      "Eval Time Real (Treal\n",
      "eval ) 1896.07 ms +50%\n",
      "Load Time (Tload ) 4456.80 ms −14%\n",
      "Peak Memory (Meval) 5.20 MB +50%\n",
      "Score 2,063 +52%\n",
      "5.3.5 Iteration 5: Timestamp-based Vis-\n",
      "ited Tracking\n",
      "In the previous iteration, direct BFS implementation showed a\n",
      "new bottleneck: in evaluateUnbound(), we were allocating\n",
      "a fresh vector<bool>(V) bitvector for each active source\n",
      "to track visited nodes. With thousands of active sources per\n",
      "query, this meant thousands of O(V) allocations which caused\n",
      "a significant overhead on the query time.\n",
      "• Removed Per-Source Bitvector Allocation:In the pre-\n",
      "vious iteration, the unbound evaluation allocated a sepa-\n",
      "rate vector<bool>(V) for each active source to track\n",
      "visited nodes. With thousands of active sources, this\n",
      "caused thousands of memory allocations per query, cre-\n",
      "ating significant overhead.\n",
      "• Shared Timestamp Array:We replaced per-source\n",
      "bitvectors with a single shared visitedTime array.\n",
      "Each source receives a unique timestamp via an incre-\n",
      "menting counter, and a node is considered visited if its\n",
      "timestamp matches the current value. This eliminates all\n",
      "per-source allocations and avoids the need to clear the\n",
      "array between sources, with the timestamp automatically\n",
      "invalidating previous visits.\n",
      "• Graph Finalization:We added a finalizeGraph()\n",
      "function called once after loading that sorts all adja-\n",
      "cency lists by label and sorts label edge lists by source.\n",
      "This ensures edges with the same label are contiguous\n",
      "in memory, improving cache locality during traversal,\n",
      "and improves sequential access patterns during unbound\n",
      "evaluation.\n",
      "Result:The score improved from 2,063 to 1,395 (32% im-\n",
      "provement). The shared timestamp technique eliminated thou-\n",
      "sands of allocations per query with minimal overhead. With\n",
      "allocation bottlenecks resolved, experiments now showed that\n",
      "cache misses during graph traversal had become the major\n",
      "cost, leading us toward memory layout optimizations.\n",
      "T able 6: Iteration 5: Timestamp-based Visited T racking\n",
      "Metric Value ∆\n",
      "Preparation Time (Tprep) 1469.60 ms −1%\n",
      "Eval Time Syn (Tsyn\n",
      "eval) 50.18 ms +10%\n",
      "Eval Time Real (Treal\n",
      "eval ) 1238.07 ms +35%\n",
      "Load Time (Tload ) 4428.61 ms +1%\n",
      "Peak Memory (Meval) 5.00 MB +4%\n",
      "Score 1,395 +32%\n",
      "5.3.6 Iteration 6: CSR Format, Graph Re-\n",
      "ordering, and Label-Indexed Struc-\n",
      "tures\n",
      "Benchmarking revealed that cache misses during graph traver-\n",
      "sal had emerged as the main bottleneck following the re-\n",
      "moval of allocation overhead. The adjacency list structure\n",
      "vector<vector<pair>> dispersed edges throughout mem-\n",
      "ory, resulting in frequent cache misses while attempting to\n",
      "access neighbors. Furthermore, nodes visited together during\n",
      "BFS frequently had distant IDs due to the arbitrary assign-\n",
      "ment of node IDs during graph loading, which resulted in\n",
      "poor spatial proximity.\n",
      "• Memory Pooling:We moved all BFS-related vectors\n",
      "(frontier, nextFrontier, visitedTime, etc.) to\n",
      "class members, pre-allocated once during prepare().\n",
      "Instead of allocating new vectors per query, we reuse\n",
      "them via swap() and clear(), eliminating allocation\n",
      "overhead from the query hot path.\n",
      "• Compressed Sparse Row (CSR) Format:We\n",
      "converted the adjacency list representation\n",
      "(vector<vector<pair>>) to CSR format with\n",
      "two contiguous arrays: offsets[V+1] storing the\n",
      "starting index of each node’s edges, and edges[E]\n",
      "\n",
      "Figure 1: Final architecture of the QuickSilver Query Evaluator. Iteration numbers indicate when each optimization was introduced.\n",
      "storing the edge data contiguously. This dramatically\n",
      "improves cache utilization during traversal.\n",
      "• BFS-based Graph Reordering:We implemented BFS\n",
      "based node ID reassignment, starting from the highest-\n",
      "degree node. This approach assigns consecutive IDs to\n",
      "the nodes visited together during traversal, eventually\n",
      "improving spatial locality when accessing adjacency\n",
      "data in the CSR arrays.\n",
      "• Node ID Translation:We maintained oldToNew[] and\n",
      "newToOld[] mapping arrays so that incoming queries\n",
      "using original node IDs are correctly translated to re-\n",
      "ordered IDs before evaluation.\n",
      "• Label-Indexed CSR:We created sep-\n",
      "arate CSR structures for each label\n",
      "(csr label offsets[label][node] and\n",
      "csr label targets[label][]). This eliminated the\n",
      "label comparison from the inner BFS loop entirely,\n",
      "facilitating direct iteration over the edges that are\n",
      "guaranteed to have the correct label instead of checking\n",
      "if (edge.label == targetLabel)for every edge.\n",
      "• Redundant Storage Removal:After implementing\n",
      "label-indexed CSR, the original generic CSR arrays were\n",
      "no longer used. We removed them, reducing memory\n",
      "footprint and improving cache efficiency.\n",
      "Result:Our final result was achieved when the score in-\n",
      "creased from 1,395 to 716 (49% improvement). The cache\n",
      "locality issue was fully solved by using a combination of\n",
      "label-indexed structures, graph reordering, and CSR format.\n",
      "The final results achieved after this iteration can be found\n",
      "using Table 7.\n",
      "5.4 Results Summary\n",
      "Table 7 summarizes our complete optimization journey from\n",
      "baseline to final implementation.\n",
      "T able 7: Final Leaderboard Performance: After Iteration 6\n",
      "Metric Value\n",
      "Discord Username wah shampy\n",
      "Preparation Time (Tprep) 1834.47 ms\n",
      "Evaluation TimeT syn\n",
      "eval 29.55 ms\n",
      "Evaluation TimeT real\n",
      "eval 564.33 ms\n",
      "Load Time (Tload ) 5053.52 ms\n",
      "Peak Memory Usage (Meval) 5.67 MB\n",
      "Score 716\n",
      "Table 8 summarizes our optimization journey from baseline\n",
      "to final implementation.\n",
      "5.5 Conclusion: Part 2\n",
      "Through systematic optimization, we achieved a 94.8% reduc-\n",
      "tion in score (approximately 19,800 to 716), corresponding to\n",
      "a significant reduction in evaluation time. The most impactful\n",
      "optimizations were:\n",
      "\n",
      "T able 8: Overall Optimization Progress\n",
      "Iteration Type Score\n",
      "1. Baseline — 19,800\n",
      "2. Bound-Aware Eval S 7,248\n",
      "3. Compact IR F 4,255\n",
      "4. Direct BFS S, Q 2,063\n",
      "5. Timestamp Tracking Q, F 1,395\n",
      "6. CSR + Reordering Q 716\n",
      "S = Smart, Q = Quick, F = Frugal\n",
      "1. Label-indexed CSR:Eliminating the label check from\n",
      "the inner loop provided substantial gains by removing\n",
      "branch mispredictions and reducing the iteration count\n",
      "to only relevant edges.\n",
      "2. Generic CSR Format:Converting from vector-of-\n",
      "vectors to contiguous arrays dramatically improved\n",
      "cache locality and eliminated pointer chasing overhead.\n",
      "3. Memory Pooling:Pre-allocating and reusing vectors\n",
      "eliminated allocation overhead that previously domi-\n",
      "nated query processing time.\n",
      "4. Graph Reordering:BFS-based node ID reassignment\n",
      "improved spatial locality by ensuring frequently co-\n",
      "accessed nodes have nearby memory addresses.\n",
      "Failed Optimization Attempts:Our optimization jour-\n",
      "ney also included attempts that did not yield positive results,\n",
      "providing valuable lessons about the gap between theoretical\n",
      "improvements and practical performance.\n",
      "• We implemented asingle-label fast paththat added\n",
      "a conditional branch to bypass the label iteration loop\n",
      "when a query involved only one label. In theory, this\n",
      "should have reduced loop overhead for the majority of\n",
      "our queries which involved single labels. However, this\n",
      "branch was evaluated at every node during BFS traversal,\n",
      "and the CPU’s branch predictor struggled with the mixed\n",
      "true/false pattern, causing pipeline stalls that exceeded\n",
      "the time saved by skipping the loop.\n",
      "• We attemptedfrontier sorting, where we sorted the BFS\n",
      "frontier by node ID before processing each level, expect-\n",
      "ing that sequential memory access to CSR arrays would\n",
      "improve cache hit rates. While theoretically sound, the\n",
      "O(n log n) sorting cost at every BFS level outweighed\n",
      "the cache benefits, particularly since our graph reorder-\n",
      "ing optimization had already improved spatial locality at\n",
      "preprocessing time, making runtime sorting redundant.\n",
      "These failures reinforced that micro-optimizations intro-\n",
      "ducing additional branches or per-level computations must be\n",
      "empirically validated, as their overhead can compound across\n",
      "millions of node visits in large-scale graph traversals.\n",
      "The final system achieves strong performance through a\n",
      "combination of memory efficiency, cache-aware data struc-\n",
      "tures, and algorithmic simplicity. These techniques are\n",
      "broadly applicable to graph processing systems where traver-\n",
      "sal performance is critical.\n",
      "6 CONCLUSION\n",
      "This project addressed two fundamental challenges in graph\n",
      "database systems: cardinality estimation and efficient query\n",
      "evaluation.\n",
      "For cardinality estimation (Part 1), we developed a hybrid\n",
      "estimator combining end-biased sampling, stratified reser-\n",
      "voir sampling, correlated hashing, and XSEED-based simula-\n",
      "tion. This approach achieved competitive accuracy on both\n",
      "synthetic and real workloads while maintaining reasonable\n",
      "preparation and estimation times.\n",
      "For query evaluation (Part 2), we systematically optimized\n",
      "the Quicksilver system through memory pooling, CSR rep-\n",
      "resentations, graph reordering, and label-indexed data struc-\n",
      "tures. We learned that careful attention to memory access\n",
      "patterns and data structure design can yield dramatic perfor-\n",
      "mance gains.\n",
      "REFERENCES\n",
      "[1] B. Babcock and S. Chaudhuri. Towards a robust query optimizer: A principled\n",
      "and practical approach. InProceedings of the 2005 ACM SIGMOD International\n",
      "Conference on Management of Data, pages 119–130, Baltimore, MD, USA, 2005.\n",
      "ACM.\n",
      "[2] S. Beamer, K. Asanovic, and D. Patterson. Direction-optimizing breadth-first\n",
      "search. InProceedings of the International Conference for High Performance\n",
      "Computing, Networking, Storage and Analysis (SC), Salt Lake City, Utah, USA,\n",
      "Nov. 2012. IEEE.\n",
      "[3] G. Cormode. Synopses for massive data: Samples, histograms, wavelets, sketches.\n",
      "F oundations and Trends in Databases, 4(1–3):1–294, 2011.\n",
      "[4] L. Dhulipala, J. Shi, T. Tseng, G. E. Blelloch, and J. Shun. The graph based\n",
      "benchmark suite (gbbs). In3rd Joint International Workshop on Graph Data\n",
      "Management Experiences & Systems (GRADES) and Network Data Analytics\n",
      "(NDA) (GRADES-NDA’20), Portland, OR, USA, June 2020. ACM.\n",
      "[5] C. Estan and J. F. Naughton. End-biased samples for join cardinality estima-\n",
      "tion. InProceedings of the 22nd International Conference on Data Engineering\n",
      "(ICDE’06), page 20, Atlanta, GA, USA, 2006. IEEE.\n",
      "[6] F. Li, B. Wu, K. Yi, and Z. Zhao. Wander join: Online aggregation via random\n",
      "walks. InProceedings of the 2016 ACM SIGMOD International Conference on\n",
      "Management of Data, pages 615–629, San Francisco, CA, USA, 2016. ACM.\n",
      "[7] T. Neumann and G. Moerkotte. Characteristic sets: Accurate cardinality esti-\n",
      "mation for RDF queries with multiple joins. In2011 IEEE 27th International\n",
      "Conference on Data Engineering (ICDE 2011), pages 984–994, Hannover, Ger-\n",
      "many, Apr. 2011. IEEE.\n",
      "[8] J. Shun and G. E. Blelloch. Ligra: A lightweight graph processing framework\n",
      "for shared memory. InProceedings of the 18th ACM SIGPLAN Symposium on\n",
      "Principles and Practice of Parallel Programming (PPoPP). ACM, 2013.\n",
      "[9] H. Wei, J. X. Yu, C. Lu, and X. Lin. Speedup graph processing by graph ordering.\n",
      "InProceedings of the 2016 International Conference on Management of Data\n",
      "(SIGMOD), San Francisco, CA, USA, 2016. ACM.\n",
      "[10] M. Winter, D. Mlakar, R. Zayer, H.-P. Seidel, and M. Steinberger. faimgraph:\n",
      "High performance management of fully-dynamic graphs under tight memory\n",
      "constraints on the gpu. InProceedings of SC18: International Conference for\n",
      "High Performance Computing, Networking, Storage and Analysis, Dallas, Texas,\n",
      "USA, Nov. 2018. IEEE.\n",
      "[11] N. Zhang, M. Ozsu, A. Aboulnaga, and I. Ilyas. Xseed: Accurate and fast\n",
      "cardinality estimation for xpath queries. In22nd International Conference on\n",
      "Data Engineering (ICDE’06), pages 61–61, 2006.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in loaded_pdf:\n",
    "    print(item.page_content, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccaf322-8f99-4716-b6dc-1e503ac3bae8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Reading from Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8f6eeb07-361c-4394-a21b-f50064387d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "794dc72a-56bb-444e-b6cc-580e6e57e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "a59cb8f3-8441-412f-820f-4e3bcf166dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_loader = WebBaseLoader(web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\"), \n",
    "                          bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                              class_=(\"post-title\", \"post-content\", \"post-header\")\n",
    "                              # Inspect Element for these classes\n",
    "                          )))\n",
    "# without bs_kwargs, entire content will be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "76e5e954-fa53-41a8-aa56-1c6e9ecc1533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\n\\nExamples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\n\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\n\\nIllustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\n\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\n\\nExperiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\n\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\n\\nAfter fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\n\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\n\\nIllustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\n\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\n\\nComparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\n\\nComparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\n\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\n\\nA picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\n\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\n\\nIllustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\n\\nPseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\n\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\\n\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_web_documents = web_loader.load()\n",
    "loaded_web_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "1ec1606e-6d9d-4808-8058-8cdd30384007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "\n",
      "Tool use\n",
      "\n",
      "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overview of a LLM-powered autonomous agent system.\n",
      "\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\n",
      "ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\n",
      "The ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\n",
      "Thought: ...\n",
      "Action: ...\n",
      "Observation: ...\n",
      "... (Repeated many times)\n",
      "\n",
      "\n",
      "Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\n",
      "\n",
      "In both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\n",
      "Reflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\n",
      "\n",
      "\n",
      "Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "\n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\n",
      "\n",
      "\n",
      "Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\n",
      "\n",
      "Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\n",
      "To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\n",
      "The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\n",
      "\n",
      "\n",
      "After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\n",
      "\n",
      "The idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\n",
      "\n",
      "\n",
      "Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\n",
      "\n",
      "The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\n",
      "In reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\n",
      "In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\n",
      "\n",
      "\n",
      "Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\n",
      "\n",
      "Component Two: Memory#\n",
      "(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\n",
      "Types of Memory#\n",
      "Memory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\n",
      "\n",
      "\n",
      "Sensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\n",
      "\n",
      "\n",
      "Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\n",
      "\n",
      "\n",
      "Long-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\n",
      "\n",
      "Explicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\n",
      "Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Categorization of human memory.\n",
      "\n",
      "We can roughly consider the following mappings:\n",
      "\n",
      "Sensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\n",
      "Short-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\n",
      "Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\n",
      "\n",
      "Maximum Inner Product Search (MIPS)#\n",
      "The external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)​ algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\n",
      "A couple common choices of ANN algorithms for fast MIPS:\n",
      "\n",
      "LSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\n",
      "ANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\n",
      "HNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\n",
      "FAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\n",
      "ScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\tilde{x}_i$ such that the inner product $\\langle q, x_i \\rangle$ is as similar to the original distance of $\\angle q, \\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\n",
      "\n",
      "\n",
      "\n",
      "Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\n",
      "\n",
      "Check more MIPS algorithms and performance comparison in ann-benchmarks.com.\n",
      "Component Three: Tool Use#\n",
      "Tool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\n",
      "\n",
      "\n",
      "A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\n",
      "\n",
      "MRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\n",
      "They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\n",
      "Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\n",
      "ChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\n",
      "HuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\n",
      "\n",
      "\n",
      "Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\n",
      "\n",
      "The system comprises of 4 stages:\n",
      "(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\n",
      "Instruction:\n",
      "\n",
      "The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\n",
      "\n",
      "(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\n",
      "Instruction:\n",
      "\n",
      "Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\n",
      "\n",
      "(3) Task execution: Expert models execute on the specific tasks and log results.\n",
      "Instruction:\n",
      "\n",
      "With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\n",
      "\n",
      "(4) Response generation: LLM receives the execution results and provides summarized results to users.\n",
      "To put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\n",
      "API-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\n",
      "\n",
      "\n",
      "Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\n",
      "\n",
      "In the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\n",
      "\n",
      "Whether an API call is needed.\n",
      "Identify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\n",
      "Response based on the API results: the model can choose to refine and call again if results are not satisfied.\n",
      "\n",
      "This benchmark evaluates the agent’s tool use capabilities at three levels:\n",
      "\n",
      "Level-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\n",
      "Level-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\n",
      "Level-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\n",
      "\n",
      "Case Studies#\n",
      "Scientific Discovery Agent#\n",
      "ChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\n",
      "\n",
      "The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\n",
      "It is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\n",
      "\n",
      "One interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\n",
      "Boiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\n",
      "For example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\n",
      "\n",
      "inquired about current trends in anticancer drug discovery;\n",
      "selected a target;\n",
      "requested a scaffold targeting these compounds;\n",
      "Once the compound was identified, the model attempted its synthesis.\n",
      "\n",
      "They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\n",
      "Generative Agents Simulation#\n",
      "Generative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\n",
      "The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\n",
      "\n",
      "Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\n",
      "\n",
      "Each element is an observation, an event directly provided by the agent.\n",
      "- Inter-agent communication can trigger new natural language statements.\n",
      "\n",
      "\n",
      "Retrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\n",
      "\n",
      "Recency: recent events have higher scores\n",
      "Importance: distinguish mundane from core memories. Ask LM directly.\n",
      "Relevance: based on how related it is to the current situation / query.\n",
      "\n",
      "\n",
      "Reflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\n",
      "\n",
      "Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\n",
      "\n",
      "\n",
      "Planning & Reacting: translate the reflections and the environment information into actions\n",
      "\n",
      "Planning is essentially in order to optimize believability at the moment vs in time.\n",
      "Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\n",
      "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
      "Environment information is present in a tree structure.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The generative agent architecture. (Image source: Park et al. 2023)\n",
      "\n",
      "This fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\n",
      "Proof-of-Concept Examples#\n",
      "AutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\n",
      "Here is the system message used by AutoGPT, where {{...}} are user inputs:\n",
      "You are {{ai-name}}, {{user-provided AI bot description}}.\n",
      "Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
      "\n",
      "GOALS:\n",
      "\n",
      "1. {{user-provided goal 1}}\n",
      "2. {{user-provided goal 2}}\n",
      "3. ...\n",
      "4. ...\n",
      "5. ...\n",
      "\n",
      "Constraints:\n",
      "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
      "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
      "3. No user assistance\n",
      "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
      "5. Use subprocesses for commands that will not terminate within a few minutes\n",
      "\n",
      "Commands:\n",
      "1. Google Search: \"google\", args: \"input\": \"<search>\"\n",
      "2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\n",
      "3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\n",
      "4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\n",
      "5. List GPT Agents: \"list_agents\", args:\n",
      "6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\n",
      "7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\n",
      "8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n",
      "9. Read file: \"read_file\", args: \"file\": \"<file>\"\n",
      "10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n",
      "11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\n",
      "12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\n",
      "13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\n",
      "14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\n",
      "15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\n",
      "16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\n",
      "17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\n",
      "18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\n",
      "19. Do Nothing: \"do_nothing\", args:\n",
      "20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\n",
      "\n",
      "Resources:\n",
      "1. Internet access for searches and information gathering.\n",
      "2. Long Term memory management.\n",
      "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
      "4. File output.\n",
      "\n",
      "Performance Evaluation:\n",
      "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
      "2. Constructively self-criticize your big-picture behavior constantly.\n",
      "3. Reflect on past decisions and strategies to refine your approach.\n",
      "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
      "\n",
      "You should only respond in JSON format as described below\n",
      "Response Format:\n",
      "{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"thought\",\n",
      "        \"reasoning\": \"reasoning\",\n",
      "        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n",
      "        \"criticism\": \"constructive self-criticism\",\n",
      "        \"speak\": \"thoughts summary to say to user\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"command name\",\n",
      "        \"args\": {\n",
      "            \"arg name\": \"value\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Ensure the response can be parsed by Python json.loads\n",
      "GPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\n",
      "Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\nThen you will pick one clarifying question, and wait for an answer from the user.\\n\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\n\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Summary of areas that need clarification:\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\n2. Details about the MVC components (e.g. which components are in each file)\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\n\\nClarifying question:\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\n\\nIs anything else unclear? If yes, only answer in the form:\\n{remaining unclear areas} remaining questions.\\n{Next question}\\nIf everything is sufficiently clear, only answer \\\"Nothing more to clarify.\\\".\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Remaining unclear areas: 2 remaining questions.\\nCan you provide more information about how the MVC components are split into separate files?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\n",
      "  }\n",
      "]\n",
      "Then after these clarification, the agent moved into the code writing mode with a different system message.\n",
      "System message:\n",
      "\n",
      "You will get instructions for code to write.\n",
      "You will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\n",
      "Make sure that every detail of the architecture is, in the end, implemented as code.\n",
      "Think step by step and reason yourself to the right decisions to make sure we get it right.\n",
      "You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\n",
      "Then you will output the content of each file including ALL code.\n",
      "Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that\n",
      "FILENAME is the lowercase file name including the file extension,\n",
      "LANG is the markup code block language for the code’s language, and CODE is the code:\n",
      "FILENAME\n",
      "CODE\n",
      "You will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\n",
      "Please note that the code should be fully functional. No placeholders.\n",
      "Follow a language and framework appropriate best practice file naming convention.\n",
      "Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\n",
      "Ensure to implement all code, if you are unsure, write a plausible implementation.\n",
      "Include module dependency or package manager dependency definition file.\n",
      "Before you finish, double check that all parts of the architecture is present in the files.\n",
      "Useful to know:\n",
      "You almost always put different classes in different files.\n",
      "For Python, you always create an appropriate requirements.txt file.\n",
      "For NodeJS, you always create an appropriate package.json file.\n",
      "You always add a comment briefly describing the purpose of the function definition.\n",
      "You try to add comments explaining very complex bits of logic.\n",
      "You always follow the best practices for the requested languages in terms of describing the code written as a defined\n",
      "package/project.\n",
      "Python toolbelt preferences:\n",
      "\n",
      "pytest\n",
      "dataclasses\n",
      "\n",
      "\n",
      "Conversatin samples:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\n\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\n\\n\\nPython toolbelt preferences:\\n- pytest\\n- dataclasses\\n\"\n",
      "  },\n",
      " #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Assumptions:\\n1. Model: The model will contain the game's data, such as level information, character states, and enemy positions.\\n2. View: The view will handle the game's visuals, including rendering the game objects, backgrounds, and updating the display.\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\n\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Please now remember the steps:\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nPlease note that the code should be fully functional. No placeholders.\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\"\n",
      "  }\n",
      "]\n",
      "Challenges#\n",
      "After going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\n",
      "\n",
      "\n",
      "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n",
      "\n",
      "\n",
      "Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\n",
      "\n",
      "\n",
      "Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\n",
      "\n",
      "\n",
      "Citation#\n",
      "Cited as:\n",
      "\n",
      "Weng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\n",
      "\n",
      "Or\n",
      "@article{weng2023agent,\n",
      "  title   = \"LLM-powered Autonomous Agents\",\n",
      "  author  = \"Weng, Lilian\",\n",
      "  journal = \"lilianweng.github.io\",\n",
      "  year    = \"2023\",\n",
      "  month   = \"Jun\",\n",
      "  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
      "}\n",
      "References#\n",
      "[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\n",
      "[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\n",
      "[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\n",
      "“ arXiv preprint arXiv:2302.02676 (2023).\n",
      "[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\n",
      "[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\n",
      "[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\n",
      "[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n",
      "[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\n",
      "[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\n",
      "[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\n",
      "[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\n",
      "[12] Parisi et al. “TALM: Tool Augmented Language Models”\n",
      "[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\n",
      "[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\n",
      "[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\n",
      "[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\n",
      "[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\n",
      "[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\n",
      "[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\n",
      "[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\n",
      "[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in loaded_web_documents:\n",
    "    print(item.page_content, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b350fef0-9891-4db9-9875-c739682c9689",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Reading from Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad5a72-cd04-403f-9225-1339e699dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arxiv pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14564ba2-a27b-46b3-8d94-36d8d0eb6b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6756097f-722d-41ed-8a47-f5ab4670cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each paper has its own unique arxiv query number\n",
    "# example: attention is all you need -> 1706.03762v7 \n",
    "arxiv_loader = ArxivLoader(query=(\"1706.03762v7\"), laod_max_docs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "5e469dfc-78bc-4b88-858e-4af77aca3dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_arxiv = arxiv_loader.load()\n",
    "loaded_arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b9ee2-0d4c-4e06-9eb2-579d74c421a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Reading from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "afe0028a-255a-4154-bf56-b7fbe5275590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "840338bf-18f0-4ac5-9b2c-960eac44387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77ea2e93-bb0d-4f8c-9b98-5a8e8d36a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_loader = WikipediaLoader(query=\"India\", load_max_docs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "638c89d9-7351-483d-825f-0f1b308a4c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_wiki_documents = wiki_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "e0ce5bb9-631b-4b1f-a56b-fce1d94131cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='India, officially the Republic of India, is a country in South Asia.  It is the seventh-largest country by area; the most populous country since 2023; and, since its independence in 1947, the world's most populous democracy. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is near Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Myanmar, Thailand, and Indonesia.\n",
      "Modern humans arrived on the Indian subcontinent from Africa no later than 55,000 years ago. Their long occupation, predominantly in isolation as hunter-gatherers, has made the region highly diverse. Settled life emerged on the subcontinent in the western margins of the Indus river basin 9,000 years ago, evolving gradually into the Indus Valley Civilisation of the third millennium BCE. By 1200 BCE, an archaic form of Sanskrit, an Indo-European language, had diffused into India from the northwest. Its hymns recorded the early dawnings of Hinduism in India. India's pre-existing Dravidian languages were supplanted in the northern regions. By 400 BCE, caste had emerged within Hinduism, and Buddhism and Jainism had arisen, proclaiming social orders unlinked to heredity. Early political consolidations gave rise to the loose-knit Maurya and Gupta Empires. This era was noted for creativity in art, architecture, and writing, but the status of women declined, and untouchability became an organised belief. In South India, the Middle kingdoms exported Dravidian language scripts and religious cultures to the kingdoms of Southeast Asia.\n",
      "In the 1st millennium, Islam, Christianity, Judaism, and Zoroastrianism became established on India's southern and western coasts. In the early centuries of the 2nd millennium Muslim armies from Central Asia intermittently overran India's northern plains. The resulting Delhi Sultanate drew northern India into the cosmopolitan networks of medieval Islam. In south India, the Vijayanagara Empire created a long-lasting composite Hindu culture. In the Punjab, Sikhism emerged, rejecting institutionalised religion. The Mughal Empire ushered in two centuries of economic expansion and relative peace, and left a rich architectural legacy. Gradually expanding rule of the British East India Company turned India into a colonial economy but consolidated its sovereignty. British Crown rule began in 1858. The rights promised to Indians were granted slowly, but technological changes were introduced, and modern ideas of education and the public life took root. A nationalist movement emerged in India, the first in the non-European British Empire and an influence on other nationalist movements. Noted for nonviolent resistance after 1920, it became the primary factor in ending British rule. In 1947, the British Indian Empire was partitioned into two independent dominions, a Hindu-majority dominion of India and a Muslim-majority dominion of Pakistan. A large-scale loss of life and an unprecedented migration accompanied the partition.\n",
      "India has been a federal republic since 1950, governed through a democratic parliamentary system. It is a pluralistic, multilingual and multi-ethnic society. India's population grew from 361 million in 1951 to over 1.4 billion in 2023. During this time, its nominal per capita income increased from US$64 annually to US$2,601, and its literacy rate from 16.6% to 74%. A comparatively destitute country in 1951, India has become a fast-growing major economy and a hub for information technology services, with an expanding middle class. India has reduced its poverty rate, though at the cost of increasing economic inequality. It is a nuclear-weapon state that ranks high in military expenditure. It has disputes over Kashmir with its neighbours, Pakistan and China, unresolved since the mid-20th' metadata={'title': 'India', 'summary': \"India, officially the Republic of India, is a country in South Asia.  It is the seventh-largest country by area; the most populous country since 2023; and, since its independence in 1947, the world's most populous democracy. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is near Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Myanmar, Thailand, and Indonesia.\\nModern humans arrived on the Indian subcontinent from Africa no later than 55,000 years ago. Their long occupation, predominantly in isolation as hunter-gatherers, has made the region highly diverse. Settled life emerged on the subcontinent in the western margins of the Indus river basin 9,000 years ago, evolving gradually into the Indus Valley Civilisation of the third millennium BCE. By 1200 BCE, an archaic form of Sanskrit, an Indo-European language, had diffused into India from the northwest. Its hymns recorded the early dawnings of Hinduism in India. India's pre-existing Dravidian languages were supplanted in the northern regions. By 400 BCE, caste had emerged within Hinduism, and Buddhism and Jainism had arisen, proclaiming social orders unlinked to heredity. Early political consolidations gave rise to the loose-knit Maurya and Gupta Empires. This era was noted for creativity in art, architecture, and writing, but the status of women declined, and untouchability became an organised belief. In South India, the Middle kingdoms exported Dravidian language scripts and religious cultures to the kingdoms of Southeast Asia.\\nIn the 1st millennium, Islam, Christianity, Judaism, and Zoroastrianism became established on India's southern and western coasts. In the early centuries of the 2nd millennium Muslim armies from Central Asia intermittently overran India's northern plains. The resulting Delhi Sultanate drew northern India into the cosmopolitan networks of medieval Islam. In south India, the Vijayanagara Empire created a long-lasting composite Hindu culture. In the Punjab, Sikhism emerged, rejecting institutionalised religion. The Mughal Empire ushered in two centuries of economic expansion and relative peace, and left a rich architectural legacy. Gradually expanding rule of the British East India Company turned India into a colonial economy but consolidated its sovereignty. British Crown rule began in 1858. The rights promised to Indians were granted slowly, but technological changes were introduced, and modern ideas of education and the public life took root. A nationalist movement emerged in India, the first in the non-European British Empire and an influence on other nationalist movements. Noted for nonviolent resistance after 1920, it became the primary factor in ending British rule. In 1947, the British Indian Empire was partitioned into two independent dominions, a Hindu-majority dominion of India and a Muslim-majority dominion of Pakistan. A large-scale loss of life and an unprecedented migration accompanied the partition.\\nIndia has been a federal republic since 1950, governed through a democratic parliamentary system. It is a pluralistic, multilingual and multi-ethnic society. India's population grew from 361 million in 1951 to over 1.4 billion in 2023. During this time, its nominal per capita income increased from US$64 annually to US$2,601, and its literacy rate from 16.6% to 74%. A comparatively destitute country in 1951, India has become a fast-growing major economy and a hub for information technology services, with an expanding middle class. India has reduced its poverty rate, though at the cost of increasing economic inequality. It is a nuclear-weapon state that ranks high in military expenditure. It has disputes over Kashmir with its neighbours, Pakistan and China, unresolved since the mid-20th century. Among the socio-economic challenges India faces are gender inequality, child malnutrition, and rising levels of air pollution. India's land is megadiverse with four biodiversity hotspots. India's wildlife, which has traditionally been viewed with tolerance in its culture, is supported in protected habitats.\", 'source': 'https://en.wikipedia.org/wiki/India'}\n",
      "PAGE_END\n",
      "page_content='The Indian National Developmental Inclusive Alliance (INDIA) is a big tent multi-party political alliance of several political parties in India led by the country's largest opposition party, the Indian National Congress. The alliance is against the ideology and governance of the ruling National Democratic Alliance (NDA) government led by the Bharatiya Janata Party (BJP) in the 2024 Indian general elections. In the 2024 general election, the alliance won 234 seats, gaining more than 100 seats in relation to its size before dissolution, and the majority of seats in states like Uttar Pradesh, Maharashtra, West Bengal, Tamil Nadu, Kerala and Punjab.\n",
      "\n",
      "\n",
      "== Etymology ==\n",
      "The Indian National Developmental Inclusive Alliance, commonly known by its backronym I.N.D.I.A. is an opposition front announced by the leaders of 28 parties to contest the 2024 Lok Sabha elections. The name was proposed during a meeting in Bengaluru and was unanimously adopted by the 28 participating parties. While some sources attribute the suggestion of the name to Rahul Gandhi, the leader face of the Indian National Congress (INC), others mention that it was suggested by Mamata Banerjee, the Trinamool Congress (TMC) supremo and chief minister of West Bengal.\n",
      "\n",
      "\n",
      "== History ==\n",
      "On September 25, 2022, Indian National Lok Dal (INLD) supremo Om Prakash Chautala hosted a rally in Fatehabad on the occasion of former Deputy Prime Minister Devi Lal's birth anniversary. During this rally, the ideas of a national alliance were first openly called on stage. Chief Minister of Bihar Nitish Kumar had said \"I'll urge all parties, including Congress, to get together and then they (BJP) will lose badly\". JD(U) spokesperson K. C. Tyagi had also stated that the foundation of the alliance was during this same rally.\n",
      "The first major Opposition parties' meeting, held in Patna, Bihar, was chaired by Nitish Kumar on 23 June 2023, when the proposal for a new alliance was put on the table. The meeting was attended by 16 Opposition parties. INLD was not included in this meeting.\n",
      "The second meeting, was held in Bengaluru, Karnataka on 17–18 July. It was chaired by UPA Chairperson Sonia Gandhi when the proposal for an alliance was accepted and ten more parties were added to the list. The alliance's name was finalized and given the name Indian National Developmental Inclusive Alliance.\n",
      "The third meeting was held in Mumbai, Maharashtra from 31 August to 1 September. The meeting was hosted by Shiv Sena (UBT) President Uddhav Thackeray and saw Sonia Gandhi, Rahul Gandhi, and chief ministers of 5 states in attendance. Over the two-day deliberations, the alliance discussed major electoral issues for the upcoming general elections, carved out the coordination committee, and passed a three-point resolution to fight 2024 Indian general elections together 'as far as possible'.\n",
      "The fourth meeting was held in New Delhi on 19 December. The meeting was primarily held to discuss seat-sharing, joint rallies, and the prime ministerial face and/or convenor of the alliance. The alliance adopted a resolution to ensure maximum use of VVPATs in upcoming elections. \"To enhance confidence in elections, VVPAT slips should be directly given to voters to self-verify and place in a separate box, instead of them falling into the main box. Eventually, all VVPAT slips must be 100% counted, ensuring truly free and fair elections,\" read the resolution passed by the alliance at the meeting. Seat sharing was also to be done by either 31 December 2023 or mid-January 2024. It was also decided that protests will be held across the country on 22 December 2023 against the suspensions of opposition MPs in the Indian Parliament. Some leaders said that the alliance would hold a grand joint rally at Patna on 30 January 2024, the death anniversary of Mahatma Gandhi, although this was not officially announced.\n",
      "The alliance held its 5th meeting virtually with some leaders not attending. Following the meeting, the Indian National Congress Pr' metadata={'title': 'Indian National Developmental Inclusive Alliance', 'summary': \"The Indian National Developmental Inclusive Alliance (INDIA) is a big tent multi-party political alliance of several political parties in India led by the country's largest opposition party, the Indian National Congress. The alliance is against the ideology and governance of the ruling National Democratic Alliance (NDA) government led by the Bharatiya Janata Party (BJP) in the 2024 Indian general elections. In the 2024 general election, the alliance won 234 seats, gaining more than 100 seats in relation to its size before dissolution, and the majority of seats in states like Uttar Pradesh, Maharashtra, West Bengal, Tamil Nadu, Kerala and Punjab.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/Indian_National_Developmental_Inclusive_Alliance'}\n",
      "PAGE_END\n"
     ]
    }
   ],
   "source": [
    "for page in loaded_wiki_documents:\n",
    "    print(page, end=\"\\nPAGE_END\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627aeecf-a31d-46de-87f8-c12b4c49815f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Splitting Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e3519-93c6-41fd-8f64-73735b583ba7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Recursively splitting text into characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7f161126-f850-4e0d-850d-7b2d58353332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4953b6ba-7227-4ff6-ad02-23fa96145509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2da5d98-2518-4406-897a-a921d42f9822",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "# each chunk of text has a maximum chunk size of 500\n",
    "# there can be an overlap of 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "9424c5f7-0f1d-4ae3-8d91-8cf031e48ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_documents\n",
    "pdf_documents = text_splitter.split_documents(loaded_pdf)\n",
    "# loaded_pdf is already a list of documents, therefore we are using split_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e8f883e2-2101-49af-ae43-de23c4ed0c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering Data Intensive Systems - 2IMD10\n",
      "EDS - PROJECTREPORT\n",
      "Team Number - 16\n",
      "Full Name Discord Username Email\n",
      "Divyansh Purohit wah shampy d.purohit@student.tue.nl\n",
      "Likhit Vesalapu likhit7. l.vesalapu@student.tue.nl\n",
      "Prathamesh Samal viper 101 p.samal@student.tue.nl\n",
      "Elena Terzieva ellie218388 e.e.terzieva@student.tue.nl\n",
      "Eindhoven, February 1, 2026\n",
      "\n",
      "\n",
      "1 ABSTRACT\n",
      "Accurate cardinality estimation is fundamental to query opti-\n",
      "mization in graph databases, enabling the selection of efficient\n",
      "execution plans for regular path queries. In this report, we\n",
      "present a hybrid cardinality estimator that combines multi-\n",
      "ple statistical synopses including per-label statistics, pairwise\n",
      "label correlations, and characteristic sets with a weighted\n",
      "and stratified sampling strategy for complex queries. Our ap-\n",
      "\n",
      "\n",
      "and stratified sampling strategy for complex queries. Our ap-\n",
      "proach balances estimation accuracy against preparation time\n",
      "and memory overhead, achieving competitive performance\n",
      "on both synthetic and real-world workloads.\n",
      "2 INTRODUCTION\n",
      "The efficiency of query processing is an important topic in\n",
      "the domain of databases, where cardinality estimation plays\n",
      "a central role. Modern graph and XML database systems\n",
      "rely on cardinality estimators as a core component of their\n",
      "\n",
      "\n",
      "rely on cardinality estimators as a core component of their\n",
      "cost based query optimizers. A cardinality estimator is a\n",
      "statistical module that, given an abstract description of a query\n",
      "(a labeled path pattern starting from some node and ending\n",
      "at another), predicts how many results this query is likely to\n",
      "produce. These predictions are crucial for choosing efficient\n",
      "execution plans, selecting join orders, and deciding which\n",
      "physical operators to use. Without an accurate cardinality\n",
      "\n",
      "\n",
      "physical operators to use. Without an accurate cardinality\n",
      "estimator, an optimizer can easily choose plans that are orders\n",
      "of magnitude slower than necessary.\n",
      "Traditional cardinality estimators for path queries are based\n",
      "on compact summaries of structural synopses that record\n",
      "per-label statistics, degree distributions, and limited informa-\n",
      "tion about how labels coincide. Although such synopses can\n",
      "be very effective for short, simple path patterns, their accuracy\n",
      "\n",
      "\n",
      "be very effective for short, simple path patterns, their accuracy\n",
      "rapidly degrades for long paths, patterns with repetition, or\n",
      "data with strong structural correlations. In these cases, naive\n",
      "independence assumptions break down and purely synopsis\n",
      "based estimators can produce large errors.\n",
      "To address these challenges, our work investigates a sam-\n",
      "pling based cardinality estimator that involves four com-\n",
      "plementary approaches: Stratified reservoir sampling, End-\n",
      "\n",
      "\n",
      "plementary approaches: Stratified reservoir sampling, End-\n",
      "biased sampling, Correlated sampling, XSEED based Simula-\n",
      "tion.\n",
      "3 LITERATURESURVEY\n",
      "The topic of cardinality estimation continues to be one of the\n",
      "most important in query optimization. There are different\n",
      "techniques that have been developed for estimating the size\n",
      "of the results produced by a query. In this project, we are\n",
      "dealing with RDF databases that differ from the typical re-\n",
      "\n",
      "\n",
      "dealing with RDF databases that differ from the typical re-\n",
      "lational schemas because of their structure, which leads to\n",
      "different assumptions and decisions made when estimating\n",
      "the cardinality of a query. In query optimisation, synopses are\n",
      "precomputed summaries of database data (like histograms,\n",
      "sketches, samples) that database systems use to quickly es-\n",
      "timate the cost of different query execution plans. Bonifati\n",
      "et al. survey cardinality estimation techniques specifically\n",
      "\n",
      "\n",
      "et al. survey cardinality estimation techniques specifically\n",
      "designed for graph query languages. The two approaches\n",
      "discussed are cardinality of paths and cardinality on patterns.\n",
      "With cardinality on paths, the query path is split into simpler\n",
      "segments and predefined formulas are used for concentra-\n",
      "tion, union and inversion are combined to estimate the result.\n",
      "Cardinality on patterns, on the other hand focuses on graph\n",
      "patterns with characteristic sets or graph summaries. With\n",
      "\n",
      "\n",
      "patterns with characteristic sets or graph summaries. With\n",
      "characteristic sets, each node is summarised by the set of\n",
      "outgoing edge labels it has, and these “characteristic sets” are\n",
      "used to estimate how many nodes (and joins) participate in\n",
      "a given pattern. They are discussed in a study by Neumann\n",
      "and Moerkotte, where they observe that in RDF databases,\n",
      "many subjects share the same set of outgoing predicates, and\n",
      "define a characteristic set as exactly this set of predicates\n",
      "\n",
      "\n",
      "define a characteristic set as exactly this set of predicates\n",
      "attached to a subject [7]. With graph summarizers, the graph\n",
      "is compressed into a smaller summary (by grouping similar\n",
      "nodes/edges), and cardinalities for complex path or subgraph\n",
      "patterns are estimated by evaluating the query on this sum-\n",
      "mary instead of on the full graph.\n",
      "Two statistical methods for dealing with that are sampling\n",
      "and histograms. Estan and Naughton propose end-biased\n",
      "\n",
      "\n",
      "and histograms. Estan and Naughton propose end-biased\n",
      "samples for join cardinality estimation in relational databases.\n",
      "The method retains tuples with probability proportional to\n",
      "their frequency in join columns, so that high-frequency val-\n",
      "ues (responsible for most join output) are over-represented.\n",
      "This yields much more accurate join size estimates than uni-\n",
      "form sampling at comparable sample sizes [5]. Babcock and\n",
      "Chaudhuri introduce a technique based on Bayesian inference\n",
      "\n",
      "\n",
      "Chaudhuri introduce a technique based on Bayesian inference\n",
      "over precomputed random samples, which models uncertainty\n",
      "and produces estimates together with confidence information.\n",
      "Their method captures multi-dimensional correlations without\n",
      "assuming attribute independence and remains time-efficient.\n",
      "Importantly, it supports user- or application-specified trade-\n",
      "offs between performance and predictability, contributing to\n",
      "the design of more robust query optimisers [1].\n",
      "\n",
      "\n",
      "the design of more robust query optimisers [1].\n",
      "Histograms are one of the most widely used methods for\n",
      "cardinality estimation in relational DBMS. The basic idea\n",
      "is to partition the domain of an attribute into buckets and\n",
      "store, for each bucket, the number of tuples (and sometimes\n",
      "additional stats such as distinct values). Query selectivities\n",
      "are then approximated by assuming a uniform distribution of\n",
      "values inside each bucket and summing contributions from\n",
      "\n",
      "\n",
      "values inside each bucket and summing contributions from\n",
      "the buckets that overlap the predicate. A detailed survey\n",
      "of histogram-based and related synopsis techniques is given\n",
      "by Cormode, who reviews equi-width, equi-depth and more\n",
      "advanced frequency-based histograms, as well as their multi-\n",
      "dimensional extensions and maintenance trade-offs [3]. Joins\n",
      "can be very expensive, and to try to bridge the gap between\n",
      "query efficiency and accuracy, Li et al. introduced the new\n",
      "\n",
      "\n",
      "query efficiency and accuracy, Li et al. introduced the new\n",
      "algorithm Wander Join [6]. Wander Join is a join estimation\n",
      "method that treats a multi-table join like a graph and then\n",
      "performs random walks through the graph instead of com-\n",
      "puting the full join. Since it only touches a small number\n",
      "of data points and does not need to compute statistics for\n",
      "the full data, the approximation results are produced quickly\n",
      "and more accurately with the increase in performed random\n",
      "\n",
      "\n",
      "walks.\n",
      "4 IMPLEMENTATION\n",
      "4.1 Approach and Planning\n",
      "4.1.1 Problem Analysis\n",
      "In order to address the problem of cardinality estimation for\n",
      "path queries, our team first examined the basic properties of\n",
      "graph data. Especially in networks, graph data shows heavy\n",
      "tailed, power-law distribution, variable-length traversals and\n",
      "Kleene closures in comparison to relational tables where data\n",
      "usually follows uniform distribution. The scoring metric for\n",
      "\n",
      "\n",
      "usually follows uniform distribution. The scoring metric for\n",
      "this project emphasizes the accuracy on real workloads with\n",
      "a weight of two and also penalizing the run times. This\n",
      "tuned our strategy to get accurate estimates without expensive\n",
      "computation.\n",
      "The provided graph contained 10,000 nodes, 22,122 edges,\n",
      "and 36 distinct edge labels. With an average out-degree of\n",
      "approximately 2.92 and a maximum out-degree of 43, the\n",
      "graph is relatively sparse (density approx 0.02%). The de-\n",
      "\n",
      "\n",
      "graph is relatively sparse (density approx 0.02%). The de-\n",
      "gree distribution follows a classic power-law pattern: 37% of\n",
      "nodes have out-degree 1, while only a handful of hub nodes\n",
      "exceed degree 20. This skewed distribution means that a small\n",
      "number of high-degree nodes disproportionately contribute\n",
      "to path counts, making uniform sampling ineffective.\n",
      "The label distribution is similarly skewed, with label 8\n",
      "appearing in over 5,000 edges (23% of all edges), while\n",
      "\n",
      "\n",
      "appearing in over 5,000 edges (23% of all edges), while\n",
      "12 labels appear fewer than 50 times each. This imbalance\n",
      "affects join selectivity and makes certain label combinations\n",
      "far more common than others. The query workload spanned\n",
      "diverse patterns:\n",
      "•Single predicates: e.g.,,1>,,,8>,\n",
      "• Concatenations: e.g., ,1>/8>,, ,1>/8>/9>/8>, (up\n",
      "to length 5)\n",
      "•Kleene closures: e.g.,,(1>)+,,,(8>)+,\n",
      "•Mixed patterns: e.g.,,1>/(8>)+/(9>)+,\n",
      "•Label unions: e.g.,,(0>|1>|2>|3>|4>)+,\n",
      "\n",
      "\n",
      "•Mixed patterns: e.g.,,1>/(8>)+/(9>)+,\n",
      "•Label unions: e.g.,,(0>|1>|2>|3>|4>)+,\n",
      "• Bound source queries: e.g., 200,1>/8>,,\n",
      "227,(1>)+/(8>)+,\n",
      "The scoring metric for this project emphasizes accuracy on\n",
      "real workloads with a weight of two while also penalizing\n",
      "preparation and estimation time. This guided our strategy: we\n",
      "needed accurate estimates without expensive runtime compu-\n",
      "tation.\n",
      "4.1.2 Technique Selection Based on Liter-\n",
      "ature Survey\n",
      "Based on our literature review, we selected the following\n",
      "\n",
      "\n",
      "ature Survey\n",
      "Based on our literature review, we selected the following\n",
      "approaches for our hybrid estimation model:\n",
      "• End-biased Sampling:This strategy was chosen in\n",
      "order to address the variation brought on by power-law\n",
      "distributions. We reasoned that probabilistic estimate\n",
      "is not appropriate for frequent values (hubs), since they\n",
      "have the greatest influence on join size and variance. We\n",
      "eliminate the main cause of instability in the estimator\n",
      "\n",
      "\n",
      "eliminate the main cause of instability in the estimator\n",
      "by explicitly isolating and tracking the top- K highest-\n",
      "degree nodes using exact logic, leaving a lower-variance\n",
      "population (the tail) that is easier to sample.\n",
      "• Stratified Reservoir Sampling:Rare edge labels are\n",
      "not guaranteed to be covered by a global random sam-\n",
      "ple. To make sure that every potential transition in the\n",
      "graph has representative start nodes, we decided to use\n",
      "stratified reservoirs, which allocate a predetermined sam-\n",
      "\n",
      "\n",
      "stratified reservoirs, which allocate a predetermined sam-\n",
      "ple budget per unique label. In order to reduce endless\n",
      "failures on queries with rare predicates, this choice was\n",
      "crucial.\n",
      "• Correlated Sampling by Hashing:Since independent\n",
      "random sampling statistically fails to capture the inter-\n",
      "section of joins in sparse samples, we ignored it for the\n",
      "tail. We make sure that sample choices are consistent\n",
      "across various query contexts by using a deterministic\n",
      "\n",
      "\n",
      "across various query contexts by using a deterministic\n",
      "hashing approach as suggested by Estan et al. given as\n",
      "follows:\n",
      "h(v)≤p v (1)\n",
      "where pV is the sampling probability for a given value\n",
      "of v and h is the hashing function which maps values in\n",
      "the range[0,1].\n",
      "• XSEED-based Seed Simulation:In clustered graphs,\n",
      "the assumption of independence between path steps is\n",
      "broken by standard histograms. The idea of employing\n",
      "bound nodes as ”seeds” for cardinality estimation is pre-\n",
      "\n",
      "\n",
      "bound nodes as ”seeds” for cardinality estimation is pre-\n",
      "sented in the XSEED paper [11]. We incorporated this\n",
      "idea into our Sampling Model, treating our Stratified\n",
      "Samples as ”seeds” and executing the query path from\n",
      "them. This enables us to capture the structural relation-\n",
      "ships in the graph that are missed by static statistical\n",
      "methods.\n",
      "We decided to pursue with a Hybrid Cardinality Estimator\n",
      "that adjusts to the structural skew of the graph by combining\n",
      "\n",
      "\n",
      "that adjusts to the structural skew of the graph by combining\n",
      "the above mentioned approaches. By dynamically replicat-\n",
      "ing graph traversals on a subgraph, this synergy enables the\n",
      "system to get around the drawbacks of static statistics and\n",
      "balance the accuracy with the runtime efficiency of sampling.\n",
      "4.1.3 Expected Challenges\n",
      "We anticipated several challenges:\n",
      "• Balancing accuracy and speed: More sophisticated\n",
      "statistics and complex simulations improve accuracy\n",
      "\n",
      "\n",
      "statistics and complex simulations improve accuracy\n",
      "but increase both preparation and estimation time. In\n",
      "\n",
      "\n",
      "addition, reservoir sizes and the estimation time penalty\n",
      "had to be carefully balanced. We anticipated that wander\n",
      "join approaches, which use random walks to estimate\n",
      "join cardinalities, would greatly benefit accuracy but\n",
      "adversely affect estimation time.\n",
      "• Handling Kleene closures: Single predicate queries\n",
      "like 1 and concatenation queries like 1 /2 were straight-\n",
      "forward and could be directly computed from our stored\n",
      "label statistics and join synopses. However, transitive\n",
      "\n",
      "\n",
      "label statistics and join synopses. However, transitive\n",
      "closure queries can produce result sets ranging from\n",
      "empty to quadratic in graph size, making estimation\n",
      "inherently difficult.\n",
      "• Correlation capture: Independence assumptions be-\n",
      "tween path segments would lead to systematic over or\n",
      "under estimation, but capturing accurate and extensive\n",
      "correlations would require exhaustive precomputations.\n",
      "• Memory constraints: Storing detailed statistics for all\n",
      "\n",
      "\n",
      "• Memory constraints: Storing detailed statistics for all\n",
      "label pairs (pairwise correlations) would scale quadrati-\n",
      "cally with the number of labels in the graph.\n",
      "4.2 Implementation Progress and Results\n",
      "We used an iterative life cycle for our implementation. Before\n",
      "finalizing our ultimate End-Biased Stratified architecture, we\n",
      "ran our experiments with a number of approaches, including\n",
      "a computationally costly Random Walk attempt.\n",
      "4.2.1 Iteration 1: Basic Synopsis\n",
      "\n",
      "\n",
      "a computationally costly Random Walk attempt.\n",
      "4.2.1 Iteration 1: Basic Synopsis\n",
      "We began by implementing foundational statistics computed\n",
      "in a single pass over the graph. This included edge counts\n",
      "per label and a matrix to store pairwise correlations between\n",
      "labels. By adding up the product of degrees, we were\n",
      "able to determine the precise number of pathways for any\n",
      "length-2 label combination. A join statistics matrix was also\n",
      "introduced to capture the number of two-hop paths between\n",
      "\n",
      "\n",
      "introduced to capture the number of two-hop paths between\n",
      "all label pairs, providing exact estimates for length-2 queries.\n",
      "Result:This baseline achieved reasonable accuracy\n",
      "on simple queries but struggled with complex patterns and\n",
      "showed high variance on longer paths.\n",
      "4.2.2 Iteration 2: Wander Joins and Ran-\n",
      "dom Walks\n",
      "We implemented a Wander Join estimator to increase accuracy\n",
      "for Kleene closures and long pathways. In order to naturally\n",
      "approximate the path distribution for each query, we con-\n",
      "\n",
      "\n",
      "approximate the path distribution for each query, we con-\n",
      "ducted random walks beginning from a collection of sampled\n",
      "nodes. To further refine this approach, we explored several ad-\n",
      "vanced adaptive sampling techniques: the Filtering-Sampling\n",
      "Approach (FaSTest), that dynamically adjusted sample sizes\n",
      "based on query characteristics, along with an adaptive stop-\n",
      "ping criteria that terminated walks once the 95% confidence\n",
      "interval around the Horvitz-Thompson estimate fell below a\n",
      "\n",
      "\n",
      "interval around the Horvitz-Thompson estimate fell below a\n",
      "5% error threshold, thereby saving time on dense queries.\n",
      "Result:Since this method physically traversed the graph\n",
      "structure to capture correlations, this method produced re-\n",
      "markable accuracy, however estimation time (Test) grew sig-\n",
      "nificantly. Even with our optimizations, a large number of\n",
      "walks were required in order to get stable estimates, which\n",
      "went against the time constraints set up for this project. In-\n",
      "\n",
      "\n",
      "went against the time constraints set up for this project. In-\n",
      "stead, we opted for a pre calculated sampling strategy.\n",
      "4.2.3 Iteration 3: Hybrid Approach\n",
      "Our estimating model was developed through an ordered\n",
      "breakdown of the problem rather than being chosen as a sin-\n",
      "gle technique. In order to address particular data features, we\n",
      "integrated various approaches in three logical steps.\n",
      "Step 1: Decomposition by DegreeThe existence of high-\n",
      "degree nodes is the main cause of errors in graph estima-\n",
      "\n",
      "\n",
      "degree nodes is the main cause of errors in graph estima-\n",
      "tion. We developed a split-execution model based on the End-\n",
      "Biased Sampling concepts. We proposed treating the graph\n",
      "G as Ghead and Gtail , into two separate subgraphs. Since any\n",
      "probabilistic variance here would propagate quadratically dur-\n",
      "ing joins, we concluded that Ghead (the top-K nodes) must\n",
      "be effectively retained in memory and queried accurately. In\n",
      "Gtail , where the variance is inherently lower, the probabilistic\n",
      "\n",
      "\n",
      "Gtail , where the variance is inherently lower, the probabilistic\n",
      "estimation is strongly restricted.\n",
      "Step 2: Handling SparsityStandard sampling is ineffec-\n",
      "tive for rare predicates in path queries, despite Estan et al.’s\n",
      "suggestion to weight by frequency. A global sample will be\n",
      "unable to capture a label L if it is present in only 0.1% of the\n",
      "graph. The sampling logic was expanded to be stratified. Each\n",
      "edge label is treated as a separate population strata. Regard-\n",
      "\n",
      "\n",
      "edge label is treated as a separate population strata. Regard-\n",
      "less of how uncommon the edge label is, we mathematically\n",
      "guarantee that the probability of providing a ”zero-estimate”\n",
      "for a valid query path is reduced by guaranteeing a constant\n",
      "reservoir size for each label.\n",
      "Step 3: Correlation PreservationIn essence, a path query\n",
      "X→Y→Z is an intersection problem. The fundamental cor-\n",
      "relations needed to compute these crossings are eliminated by\n",
      "\n",
      "\n",
      "relations needed to compute these crossings are eliminated by\n",
      "independent random sampling. We decided to use Correlated\n",
      "Hashing. In case, a node exists in the reservoir for label A (as\n",
      "a target), it will also be present in the reservoir for label B (as\n",
      "a source), as long as it satisfies the probability threshold, by\n",
      "making the inclusion of a node into a reservoir a deterministic\n",
      "function of its ID (h(v)≤p v). This maintains the linkability.\n",
      "\n",
      "\n",
      "function of its ID (h(v)≤p v). This maintains the linkability.\n",
      "Step 4: Simulation-Based EstimationFinally, we used the\n",
      "Seed-Based approach from the XSEED paper to solve for\n",
      "complex query types that defy static formulas, such as Kleene\n",
      "closures (+). In addition to being statistical representatives,\n",
      "we thought of our stratified samples as ”active seeds.” Rather\n",
      "than merging histograms, the estimator measures reachability\n",
      "by physically running the query from these seeds and scal-\n",
      "\n",
      "\n",
      "by physically running the query from these seeds and scal-\n",
      "ing the outcome by the inverse of the inclusion probability\n",
      "(1/p v).\n",
      "Result:This approach achieved the best trade-off between\n",
      "accuracy and estimation time on simple as well as complex\n",
      "queries. queries. The only drawback of this approach was that\n",
      "it occupied significantly more memory, which was further\n",
      "\n",
      "\n",
      "refined in the next iteration.\n",
      "4.2.4 Iteration 4: Optimization\n",
      "The final iteration focused on performance optimization with-\n",
      "out sacrificing on the accuracy. This iteration was done in\n",
      "order to reduce Estimation Time (Test) and Preparation Time\n",
      "(Tprep).\n",
      "• Binary Search for Adjacency:It was inefficient to\n",
      "search adjacency lists linearly for edges that matched a\n",
      "given start label for the high-degree top-k nodes. Using\n",
      "binary search on the sorted edge lists, we optimized the\n",
      "\n",
      "\n",
      "binary search on the sorted edge lists, we optimized the\n",
      "sampling algorithm. This resulted in a targeted speedup\n",
      "for the most frequently used nodes by lowering the edge\n",
      "lookup difficulty from linear to logarithmic.\n",
      "• Memory Reuse:Our code initially generated fresh tem-\n",
      "porary lists for each node it handled during the prepa-\n",
      "ration stage. This required more memory and was slow\n",
      "due to large number of nodes. These list creations were\n",
      "relocated outside of the main loop. The same memory\n",
      "\n",
      "\n",
      "relocated outside of the main loop. The same memory\n",
      "block is reused repeatedly since we only need to gener-\n",
      "ate the list once and then clear it after processing each\n",
      "node. As a result, the overhead of continuous memory\n",
      "allocation was significantly reduced.\n",
      "4.2.5 Results\n",
      "Our final hybrid estimator shows a good trade-off between\n",
      "runtime and estimation accuracy. The leaderboard’s perfor-\n",
      "mance metrics are as follows\n",
      "T able 1: Final Leaderboard Performance\n",
      "Metric Value\n",
      "Discord Username viper 101\n",
      "\n",
      "\n",
      "T able 1: Final Leaderboard Performance\n",
      "Metric Value\n",
      "Discord Username viper 101\n",
      "Synthetic Accuracy (Eqsyn) 1.18\n",
      "Real-Workload Accuracy (Eqreal) 8.86\n",
      "Preparation Time (Tprep) 153.88 ms\n",
      "Estimation Time (Test) 32.58 ms\n",
      "Peak Memory Usage (M) 24.07 MB\n",
      "Final Score 49.63\n",
      "5 PART2: EFFICIENTQUERYEVALUATION\n",
      "In Part 2 of the project, we shift focus from cardinality es-\n",
      "timation to efficient query evaluation. The goal is to make\n",
      "QuicksilverSQUIFfy: Smart, Quick, and Frugal. We focused\n",
      "\n",
      "\n",
      "QuicksilverSQUIFfy: Smart, Quick, and Frugal. We focused\n",
      "heavily on balancing the trade-offs between execution speed,\n",
      "preparation time, and strict memory limits. We didn’t just\n",
      "want it to be fast; we wanted it to be robust and memory\n",
      "efficient as per Frugal requirement.\n",
      "5.1 Strategy\n",
      "Our optimization strategy followed a systematic approach\n",
      "inspired by research into high-performance graph processing\n",
      "systems. We categorized our efforts into three pillars: Smart\n",
      "\n",
      "\n",
      "systems. We categorized our efforts into three pillars: Smart\n",
      "(choosing optimal algorithms), Quick (reducing computation\n",
      "time), and Frugal (minimizing memory overhead).\n",
      "5.1.1 Smart\n",
      "We made informed decisions based on query patterns and\n",
      "empirical benchmarking:\n",
      "• Separate Bound Source/Target Handlers:We used\n",
      "forward BFS traversal for queries with bound sources\n",
      "and backward BFS from the target for queries with\n",
      "bound targets. This method helped us to avoid full-graph\n",
      "\n",
      "\n",
      "bound targets. This method helped us to avoid full-graph\n",
      "traversals and significantly reduced the search space.\n",
      "• Node ID Mapping:We maintained a translation table\n",
      "to map the source and target node IDs from original to\n",
      "reordered IDs at evaluation time so that when the graph\n",
      "is reordered (discussed in Quick) and nodes get new\n",
      "IDs, the evaluator continues work with consistent node\n",
      "identifiers.\n",
      "5.1.2 Quick: Reducing Computation Time\n",
      "Here our primary focus was eliminating redundant work and\n",
      "\n",
      "\n",
      "5.1.2 Quick: Reducing Computation Time\n",
      "Here our primary focus was eliminating redundant work and\n",
      "improving cache efficiency:\n",
      "• Label-indexed CSR:We pre-partition edges by label\n",
      "during graph loading instead of filtering them during\n",
      "BFS, which eliminated the label check from the hottest\n",
      "code path. This reduced branch mispredictions and im-\n",
      "proved instruction level parallelism.\n",
      "• Graph Reordering:We implemented BFS based node\n",
      "ID reassignment, starting from the highest-degree node.\n",
      "\n",
      "\n",
      "ID reassignment, starting from the highest-degree node.\n",
      "This approach assigns consecutive IDs to the nodes vis-\n",
      "ited together during traversal, eventually improving spa-\n",
      "tial locality when accessing adjacency data in the CSR\n",
      "arrays.\n",
      "• Tracking visited nodes using timestamp:Rather than\n",
      "clearing the visited array between queries (O(V) oper-\n",
      "ation), we used a monotonically increasing timestamp\n",
      "counter to indicate the status of a node. We marked a\n",
      "\n",
      "\n",
      "counter to indicate the status of a node. We marked a\n",
      "node with the current timestamp when visiting it, and\n",
      "the node is considered unvisited if its timestamp doesn’t\n",
      "match the current query’s timestamp (O(1) operation).\n",
      "5.1.3 Frugal: Memory Efficiency\n",
      "Here we focused on minimizing dynamic memory operations\n",
      "because memory allocation and deallocation are expensive\n",
      "operations.\n",
      "• Memory Pooling:All vectors used during BFS traversal\n",
      "were pre-allocated once during the prepare phase and\n",
      "\n",
      "\n",
      "were pre-allocated once during the prepare phase and\n",
      "reused across all queries via swap() operations rather\n",
      "than repeated allocation and deallocation.\n",
      "• CSR Format:We converted adjacency lists to Com-\n",
      "pressed Sparse Row format, storing edges in contiguous\n",
      "arrays indexed by node ID, to reduce memory fragmen-\n",
      "tation and improve cache utilization.\n",
      "\n",
      "\n",
      "5.2 Literature Survey\n",
      "Work on fast graph query evaluation repeatedly shows that\n",
      "performance depends less on high-level query language rules\n",
      "and more on how traversal is executed: reducing wasted\n",
      "exploration, using cache-friendly graph layouts, and avoiding\n",
      "overhead inside the traversal loop. Shared-memory graph\n",
      "systems and RDF engines especially emphasize contiguous\n",
      "adjacency storage, locality-aware preprocessing, and access-\n",
      "path choices that prevent repeated filtering during evaluation.\n",
      "\n",
      "\n",
      "path choices that prevent repeated filtering during evaluation.\n",
      "In direction-optimizing BFS, Beamer et al. show that\n",
      "switching between traversal styles can dramatically reduce\n",
      "the number of edges examined on real “small-world” graphs,\n",
      "which is exactly the problem when a naive traversal expands\n",
      "too broadly. Their key message is that how you traverse (direc-\n",
      "tion/strategy decisions) can matter as much as raw low-level\n",
      "optimization, because it directly shrinks the explored search\n",
      "\n",
      "\n",
      "optimization, because it directly shrinks the explored search\n",
      "space [2]. Moreover, when graph preprocessing changes ver-\n",
      "tex IDs (for example after reordering), systems typically need\n",
      "a stable way to preserve semantics across representations.\n",
      "In the graph ordering work by Wei et al., the goal is to re-\n",
      "arrange vertex order to improve cache behavior. However,\n",
      "this implicitly creates a “logical graph” (original IDs) and a\n",
      "“physical graph” (reordered IDs). The paper motivates the\n",
      "\n",
      "\n",
      "“physical graph” (reordered IDs). The paper motivates the\n",
      "practical need to keep query evaluation correct even when\n",
      "the physical layout changes — conceptually the same reason\n",
      "many systems maintain an ID mapping layer around reordered\n",
      "graphs [9].\n",
      "For traversal-heavy workloads, research shows that perfor-\n",
      "mance is often memory-bound, so the fastest solutions focus\n",
      "on contiguous layouts and minimizing work in the tight loop.\n",
      "Ligra is a well-known shared-memory framework demon-\n",
      "\n",
      "\n",
      "Ligra is a well-known shared-memory framework demon-\n",
      "strating that graph traversal becomes significantly faster when\n",
      "adjacency is stored and processed in cache-friendly ways,\n",
      "enabling efficient neighbor scans and reducing overhead from\n",
      "pointer-heavy structures. Although Ligra is a framework, its\n",
      "results strongly support the idea that CSR-like representations\n",
      "and streamlined traversal primitives are a practical baseline\n",
      "for high throughput BFS-style processing [8]. A second line\n",
      "\n",
      "\n",
      "for high throughput BFS-style processing [8]. A second line\n",
      "of work targets locality directly via graph reordering. Wei et\n",
      "al. show that reordering vertices can reduce CPU cache miss\n",
      "ratios across several graph algorithms, producing speedups\n",
      "without changing the underlying algorithm. Their contribu-\n",
      "tion is important for query evaluation because it treats graph\n",
      "layout as an optimization target: by reassigning IDs so that\n",
      "vertices accessed close in time are also close in memory, the\n",
      "\n",
      "\n",
      "vertices accessed close in time are also close in memory, the\n",
      "cost of adjacency and metadata access drops [9]. Visited-\n",
      "state handling is another recurring bottleneck in fast traversal.\n",
      "Beamer et al. discuss optimizations such as using compact\n",
      "visited representations (for example bitmaps) to reduce costly\n",
      "random accesses and keep visited checks efficient on shared-\n",
      "memory machines. The point is that even “small” bookkeep-\n",
      "ing operations can dominate runtime when done at scale, so\n",
      "\n",
      "\n",
      "ing operations can dominate runtime when done at scale, so\n",
      "visited tracking must be engineered carefully [2]. A large\n",
      "part of the memory efficiency in graph query engines comes\n",
      "from keeping data structures compact and avoiding repeated\n",
      "allocations during query execution. Many systems therefore\n",
      "rely on two practical ideas such as storing adjacency in a con-\n",
      "tiguous sparse format (typically CSR), and pre-allocating and\n",
      "reusing working buffers instead of allocating/freeing mem-\n",
      "\n",
      "\n",
      "reusing working buffers instead of allocating/freeing mem-\n",
      "ory for every query. Several systems make this point even\n",
      "more explicitly by standardizing around CSR as the baseline\n",
      "representation due to its compactness and predictable access\n",
      "pattern. Dhulipala et al. (GBBS) state that graphs in the suite\n",
      "are stored in Compressed Sparse Row (CSR), where neigh-\n",
      "bor lists live in contiguous arrays with an offset array per\n",
      "node. This design reduces fragmentation and supports cache-\n",
      "\n",
      "\n",
      "node. This design reduces fragmentation and supports cache-\n",
      "friendly scans. The key motivation is that pointer-heavy adja-\n",
      "cency lists create overhead and irregular access, while CSR\n",
      "improves both memory footprint and traversal throughput [4].\n",
      "Many high-performance systems reduce memory overhead\n",
      "by minimizing dynamic allocations during execution. Winter\n",
      "et al. (faimGraph) present a memory-management approach\n",
      "designed for fully-dynamic graphs on the GPU. While faim-\n",
      "\n",
      "\n",
      "designed for fully-dynamic graphs on the GPU. While faim-\n",
      "Graph is GPU-focused, its core principle—using one large\n",
      "pre-allocation to avoid the overhead and fragmentation of\n",
      "repeated memory allocation or vector resizing—maps cleanly\n",
      "to CPU implementations. We adopted this by pre-allocating\n",
      "our working buffers once to keep runtime memory behavior\n",
      "stable. [10].\n",
      "5.3 Implementation Progress\n",
      "We followed an iterative approach for optimizing our per-\n",
      "\n",
      "\n",
      "stable. [10].\n",
      "5.3 Implementation Progress\n",
      "We followed an iterative approach for optimizing our per-\n",
      "formance, implementing and benchmarking each technique\n",
      "independently to measure its overall impact. In the sections\n",
      "below, we explain each optimization iteration experiments.\n",
      "We also report the performance metrics achieved in each itera-\n",
      "tion, along with a improvement percentage from the previous\n",
      "iteration (represented by ∆). A positive value indicates im-\n",
      "\n",
      "\n",
      "iteration (represented by ∆). A positive value indicates im-\n",
      "provement in the performance parameter whereas a negative\n",
      "value represents decrease in the performance.\n",
      "5.3.1 Iteration 1: Baseline Analysis\n",
      "The baseline implementation used standard adjacency lists\n",
      "with vectors of (label, target) pairs. Each query allocated\n",
      "fresh vectors for BFS traversal and used a boolean visited\n",
      "array that was cleared between queries. The simple evaluator\n",
      "\n",
      "\n",
      "array that was cleared between queries. The simple evaluator\n",
      "performed linear scans through the adjacency lists, checking\n",
      "each edge’s label against the query predicate.\n",
      "Results:Profiling revealed several bottlenecks: re-\n",
      "peated memory allocation for each query, O(V) cost to clear\n",
      "the visited array, poor cache locality due to scattered memory\n",
      "access, and redundant label checks in the inner traversal loop.\n",
      "5.3.2 Iteration 2: Bound-Aware Evaluation\n",
      "and Semi-Naive Transitive Closure\n",
      "\n",
      "\n",
      "5.3.2 Iteration 2: Bound-Aware Evaluation\n",
      "and Semi-Naive Transitive Closure\n",
      "After the experiments on our baseline, w e found that queries\n",
      "with bound sources or targets continued to unnecessarily tra-\n",
      "verse the whole graph. Furthermore, Kleene star searches\n",
      "\n",
      "\n",
      "T able 2: Iteration 1: Baseline Performance\n",
      "Metric Value\n",
      "Preparation Time (Tprep) 145.87 ms\n",
      "Evaluation TimeT syn\n",
      "eval 1213.70 ms\n",
      "Evaluation TimeT real\n",
      "eval 17993.72 ms\n",
      "Load Time (Tload ) 3985.82 ms\n",
      "Peak Memory Usage (Meval) 29.48 MB\n",
      "Score ∼19,800\n",
      "were creating duplicate work by recalculating every reachable\n",
      "node at each cycle.\n",
      "• Bound-Aware Evaluation:We implemented separate\n",
      "independent evaluation handlers for queries with bound\n",
      "sources and bound targets. Instead of traversing the\n",
      "\n",
      "\n",
      "sources and bound targets. Instead of traversing the\n",
      "entire graph, we start a breadth first search from the\n",
      "single known node, significantly reducing the search\n",
      "space for constrained queries.\n",
      "• Semi-Naive Transitive Closure:We replaced the naive\n",
      "transitive closure algorithm with a semi-naive approach\n",
      "for Kleene star queries. Instead of recalculating all\n",
      "reachable nodes at each iteration, we process only newly\n",
      "discovered edges, avoiding redundant computation and\n",
      "\n",
      "\n",
      "discovered edges, avoiding redundant computation and\n",
      "significantly improving performance for recursive path\n",
      "patterns.\n",
      "Results:The score improved from ˜19,800 to 7,248 (63%\n",
      "improvement). This confirmed that algorithmic issues were\n",
      "the major bottleneck. However, this also revealed that signifi-\n",
      "cant time was still being spent on constructing and managing\n",
      "intermediate result graphs, hinting us towards data structure\n",
      "optimizations.\n",
      "T able 3: Iteration 2: Bound-Aware Evaluation\n",
      "Metric Value ∆\n",
      "\n",
      "\n",
      "optimizations.\n",
      "T able 3: Iteration 2: Bound-Aware Evaluation\n",
      "Metric Value ∆\n",
      "Preparation Time (Tprep) 1505.31 ms −932%\n",
      "Eval Time Syn (Tsyn\n",
      "eval) 341.26 ms +72%\n",
      "Eval Time Real (Treal\n",
      "eval ) 6403.16 ms +64%\n",
      "Load Time (Tload ) 4092.68 ms −3%\n",
      "Peak Memory (Meval) 24.87 MB +16%\n",
      "Score 7,248 +63%\n",
      "5.3.3 Iteration 3: Compact Intermediate\n",
      "Results\n",
      "After the algorithmic changes were implemented, profiling\n",
      "showed that creating and maintaining intermediate result\n",
      "\n",
      "\n",
      "showed that creating and maintaining intermediate result\n",
      "graphs took a considerable amount of time. We observed\n",
      "that these intermediary structures kept duplicate data: reverse\n",
      "adjacency lists were kept even if joins only traveled forward,\n",
      "and labels were kept even though edges had already been\n",
      "filtered. To subside these issues, we used the following ap-\n",
      "proaches:\n",
      "• CompactIR Data Structure:We introduced a\n",
      "lightweight intermediate result structure that stores only\n",
      "\n",
      "\n",
      "lightweight intermediate result structure that stores only\n",
      "source-to-target mappings. We removed labels from in-\n",
      "termediate results since once edges are filtered by label\n",
      "during selection, storing the label becomes redundant.\n",
      "We also eliminated the reverse adjacency list because\n",
      "join operations only traverse forward, never requiring\n",
      "backward lookups on intermediate data.\n",
      "• Deferred Deduplication:We also removed per-insert\n",
      "duplicate checking, which required expensive lookups\n",
      "\n",
      "\n",
      "duplicate checking, which required expensive lookups\n",
      "on every edge addition. Instead, we allowed duplicates\n",
      "during construction and performed a batch sort-and-\n",
      "deduplicate operation at the end by using finalize().\n",
      "This approach is more cache-friendly and significantly\n",
      "reduces computation time.\n",
      "• Hash-based Existence Checking:For transitive closure\n",
      "computation, we replaced linear search with hash sets\n",
      "for tracking visited edges, reducing existence checks\n",
      "\n",
      "\n",
      "for tracking visited edges, reducing existence checks\n",
      "from O(N) to O(1) and speeding up Kleene star queries.\n",
      "Result:The score improved from 7,248 to 4,255 (41% im-\n",
      "provement). Memory usage and computation time were de-\n",
      "creased by eliminating redundant storage and delaying dedu-\n",
      "plication.\n",
      "T able 4: Iteration 3: Compact Intermediate Results\n",
      "Metric Value ∆\n",
      "Preparation Time (Tprep) 1476.81 ms +2%\n",
      "Eval Time Syn (Tsyn\n",
      "eval) 212.29 ms +38%\n",
      "Eval Time Real (Treal\n",
      "eval ) 3828.77 ms +40%\n",
      "\n",
      "\n",
      "Eval Time Syn (Tsyn\n",
      "eval) 212.29 ms +38%\n",
      "Eval Time Real (Treal\n",
      "eval ) 3828.77 ms +40%\n",
      "Load Time (Tload ) 3911.26 ms +4%\n",
      "Peak Memory (Meval) 10.40 MB +58%\n",
      "Score 4,255 +41%\n",
      "5.3.4 Iteration 4: Direct BFS Traversal\n",
      "In the previous iteration, our approach continued to materi-\n",
      "alize intermediate result graphs and carry out explicit join\n",
      "operations in spite of the CompactIR enhancements. Since\n",
      "we only want the final reachable nodes for path queries, not\n",
      "\n",
      "\n",
      "we only want the final reachable nodes for path queries, not\n",
      "the intermediary edges, we thought if this abstraction was\n",
      "even essential. As a result, we completely stopped using\n",
      "intermediate graph building.\n",
      "• Removed Intermediate Graph Construction:We\n",
      "eliminated the CompactIR data structure and join op-\n",
      "erations entirely. Instead of building intermediate result\n",
      "graphs and joining them, we now traverse the query path\n",
      "directly while maintaining only the current frontier. This\n",
      "\n",
      "\n",
      "directly while maintaining only the current frontier. This\n",
      "avoids materializing all intermediate edges, significantly\n",
      "reducing memory usage and computation time.\n",
      "• Specialized Evaluation Paths:We implemented three\n",
      "separate evaluation functions: forward BFS for bound-\n",
      "source queries, backward BFS for bound-target queries,\n",
      "\n",
      "\n",
      "and multi-source traversal for unbound queries. Each\n",
      "path is optimized for its specific access pattern.\n",
      "• Bitvector-based Tracking:We replaced hash sets with\n",
      "bitvectors for tracking frontier membership and visited\n",
      "nodes. For dense node ID spaces, bitvectors provide\n",
      "O(1) lookups with better cache locality than hash-based\n",
      "structures.\n",
      "• Label-indexed Edge Lists:Precomputed lists were\n",
      "added that provide direct access to all edges with a given\n",
      "\n",
      "\n",
      "added that provide direct access to all edges with a given\n",
      "label, eliminating the need to scan entire adjacency lists\n",
      "and filter by label during unbound evaluation.\n",
      "• Memory Optimizations:vector pre-reservation were\n",
      "added to reduce reallocations during BFS and used\n",
      "move semantics to avoid unnecessary vector copies when\n",
      "swapping frontiers.\n",
      "Result:The score improved from 4,255 to 2,063 (52%\n",
      "improvement). Eliminating intermediate graph construction\n",
      "\n",
      "\n",
      "improvement). Eliminating intermediate graph construction\n",
      "was a major win, confirming that the join-based abstraction\n",
      "was unnecessary overhead. However, experiments showed\n",
      "a new bottleneck: the unbound evaluation was allocating\n",
      "thousands of bitvectors per query, one for each active source.\n",
      "T able 5: Iteration 4: Direct BFS T raversal\n",
      "Metric Value ∆\n",
      "Preparation Time (Tprep) 1451.41 ms +2%\n",
      "Eval Time Syn (Tsyn\n",
      "eval) 55.70 ms +74%\n",
      "Eval Time Real (Treal\n",
      "eval ) 1896.07 ms +50%\n",
      "\n",
      "\n",
      "Eval Time Syn (Tsyn\n",
      "eval) 55.70 ms +74%\n",
      "Eval Time Real (Treal\n",
      "eval ) 1896.07 ms +50%\n",
      "Load Time (Tload ) 4456.80 ms −14%\n",
      "Peak Memory (Meval) 5.20 MB +50%\n",
      "Score 2,063 +52%\n",
      "5.3.5 Iteration 5: Timestamp-based Vis-\n",
      "ited Tracking\n",
      "In the previous iteration, direct BFS implementation showed a\n",
      "new bottleneck: in evaluateUnbound(), we were allocating\n",
      "a fresh vector<bool>(V) bitvector for each active source\n",
      "to track visited nodes. With thousands of active sources per\n",
      "\n",
      "\n",
      "to track visited nodes. With thousands of active sources per\n",
      "query, this meant thousands of O(V) allocations which caused\n",
      "a significant overhead on the query time.\n",
      "• Removed Per-Source Bitvector Allocation:In the pre-\n",
      "vious iteration, the unbound evaluation allocated a sepa-\n",
      "rate vector<bool>(V) for each active source to track\n",
      "visited nodes. With thousands of active sources, this\n",
      "caused thousands of memory allocations per query, cre-\n",
      "ating significant overhead.\n",
      "\n",
      "\n",
      "caused thousands of memory allocations per query, cre-\n",
      "ating significant overhead.\n",
      "• Shared Timestamp Array:We replaced per-source\n",
      "bitvectors with a single shared visitedTime array.\n",
      "Each source receives a unique timestamp via an incre-\n",
      "menting counter, and a node is considered visited if its\n",
      "timestamp matches the current value. This eliminates all\n",
      "per-source allocations and avoids the need to clear the\n",
      "array between sources, with the timestamp automatically\n",
      "invalidating previous visits.\n",
      "\n",
      "\n",
      "array between sources, with the timestamp automatically\n",
      "invalidating previous visits.\n",
      "• Graph Finalization:We added a finalizeGraph()\n",
      "function called once after loading that sorts all adja-\n",
      "cency lists by label and sorts label edge lists by source.\n",
      "This ensures edges with the same label are contiguous\n",
      "in memory, improving cache locality during traversal,\n",
      "and improves sequential access patterns during unbound\n",
      "evaluation.\n",
      "Result:The score improved from 2,063 to 1,395 (32% im-\n",
      "\n",
      "\n",
      "evaluation.\n",
      "Result:The score improved from 2,063 to 1,395 (32% im-\n",
      "provement). The shared timestamp technique eliminated thou-\n",
      "sands of allocations per query with minimal overhead. With\n",
      "allocation bottlenecks resolved, experiments now showed that\n",
      "cache misses during graph traversal had become the major\n",
      "cost, leading us toward memory layout optimizations.\n",
      "T able 6: Iteration 5: Timestamp-based Visited T racking\n",
      "Metric Value ∆\n",
      "Preparation Time (Tprep) 1469.60 ms −1%\n",
      "Eval Time Syn (Tsyn\n",
      "\n",
      "\n",
      "Metric Value ∆\n",
      "Preparation Time (Tprep) 1469.60 ms −1%\n",
      "Eval Time Syn (Tsyn\n",
      "eval) 50.18 ms +10%\n",
      "Eval Time Real (Treal\n",
      "eval ) 1238.07 ms +35%\n",
      "Load Time (Tload ) 4428.61 ms +1%\n",
      "Peak Memory (Meval) 5.00 MB +4%\n",
      "Score 1,395 +32%\n",
      "5.3.6 Iteration 6: CSR Format, Graph Re-\n",
      "ordering, and Label-Indexed Struc-\n",
      "tures\n",
      "Benchmarking revealed that cache misses during graph traver-\n",
      "sal had emerged as the main bottleneck following the re-\n",
      "moval of allocation overhead. The adjacency list structure\n",
      "\n",
      "\n",
      "moval of allocation overhead. The adjacency list structure\n",
      "vector<vector<pair>> dispersed edges throughout mem-\n",
      "ory, resulting in frequent cache misses while attempting to\n",
      "access neighbors. Furthermore, nodes visited together during\n",
      "BFS frequently had distant IDs due to the arbitrary assign-\n",
      "ment of node IDs during graph loading, which resulted in\n",
      "poor spatial proximity.\n",
      "• Memory Pooling:We moved all BFS-related vectors\n",
      "(frontier, nextFrontier, visitedTime, etc.) to\n",
      "\n",
      "\n",
      "• Memory Pooling:We moved all BFS-related vectors\n",
      "(frontier, nextFrontier, visitedTime, etc.) to\n",
      "class members, pre-allocated once during prepare().\n",
      "Instead of allocating new vectors per query, we reuse\n",
      "them via swap() and clear(), eliminating allocation\n",
      "overhead from the query hot path.\n",
      "• Compressed Sparse Row (CSR) Format:We\n",
      "converted the adjacency list representation\n",
      "(vector<vector<pair>>) to CSR format with\n",
      "two contiguous arrays: offsets[V+1] storing the\n",
      "\n",
      "\n",
      "(vector<vector<pair>>) to CSR format with\n",
      "two contiguous arrays: offsets[V+1] storing the\n",
      "starting index of each node’s edges, and edges[E]\n",
      "\n",
      "\n",
      "Figure 1: Final architecture of the QuickSilver Query Evaluator. Iteration numbers indicate when each optimization was introduced.\n",
      "storing the edge data contiguously. This dramatically\n",
      "improves cache utilization during traversal.\n",
      "• BFS-based Graph Reordering:We implemented BFS\n",
      "based node ID reassignment, starting from the highest-\n",
      "degree node. This approach assigns consecutive IDs to\n",
      "the nodes visited together during traversal, eventually\n",
      "improving spatial locality when accessing adjacency\n",
      "\n",
      "\n",
      "improving spatial locality when accessing adjacency\n",
      "data in the CSR arrays.\n",
      "• Node ID Translation:We maintained oldToNew[] and\n",
      "newToOld[] mapping arrays so that incoming queries\n",
      "using original node IDs are correctly translated to re-\n",
      "ordered IDs before evaluation.\n",
      "• Label-Indexed CSR:We created sep-\n",
      "arate CSR structures for each label\n",
      "(csr label offsets[label][node] and\n",
      "csr label targets[label][]). This eliminated the\n",
      "label comparison from the inner BFS loop entirely,\n",
      "\n",
      "\n",
      "csr label targets[label][]). This eliminated the\n",
      "label comparison from the inner BFS loop entirely,\n",
      "facilitating direct iteration over the edges that are\n",
      "guaranteed to have the correct label instead of checking\n",
      "if (edge.label == targetLabel)for every edge.\n",
      "• Redundant Storage Removal:After implementing\n",
      "label-indexed CSR, the original generic CSR arrays were\n",
      "no longer used. We removed them, reducing memory\n",
      "footprint and improving cache efficiency.\n",
      "\n",
      "\n",
      "no longer used. We removed them, reducing memory\n",
      "footprint and improving cache efficiency.\n",
      "Result:Our final result was achieved when the score in-\n",
      "creased from 1,395 to 716 (49% improvement). The cache\n",
      "locality issue was fully solved by using a combination of\n",
      "label-indexed structures, graph reordering, and CSR format.\n",
      "The final results achieved after this iteration can be found\n",
      "using Table 7.\n",
      "5.4 Results Summary\n",
      "Table 7 summarizes our complete optimization journey from\n",
      "\n",
      "\n",
      "using Table 7.\n",
      "5.4 Results Summary\n",
      "Table 7 summarizes our complete optimization journey from\n",
      "baseline to final implementation.\n",
      "T able 7: Final Leaderboard Performance: After Iteration 6\n",
      "Metric Value\n",
      "Discord Username wah shampy\n",
      "Preparation Time (Tprep) 1834.47 ms\n",
      "Evaluation TimeT syn\n",
      "eval 29.55 ms\n",
      "Evaluation TimeT real\n",
      "eval 564.33 ms\n",
      "Load Time (Tload ) 5053.52 ms\n",
      "Peak Memory Usage (Meval) 5.67 MB\n",
      "Score 716\n",
      "Table 8 summarizes our optimization journey from baseline\n",
      "to final implementation.\n",
      "\n",
      "\n",
      "Score 716\n",
      "Table 8 summarizes our optimization journey from baseline\n",
      "to final implementation.\n",
      "5.5 Conclusion: Part 2\n",
      "Through systematic optimization, we achieved a 94.8% reduc-\n",
      "tion in score (approximately 19,800 to 716), corresponding to\n",
      "a significant reduction in evaluation time. The most impactful\n",
      "optimizations were:\n",
      "\n",
      "\n",
      "T able 8: Overall Optimization Progress\n",
      "Iteration Type Score\n",
      "1. Baseline — 19,800\n",
      "2. Bound-Aware Eval S 7,248\n",
      "3. Compact IR F 4,255\n",
      "4. Direct BFS S, Q 2,063\n",
      "5. Timestamp Tracking Q, F 1,395\n",
      "6. CSR + Reordering Q 716\n",
      "S = Smart, Q = Quick, F = Frugal\n",
      "1. Label-indexed CSR:Eliminating the label check from\n",
      "the inner loop provided substantial gains by removing\n",
      "branch mispredictions and reducing the iteration count\n",
      "to only relevant edges.\n",
      "2. Generic CSR Format:Converting from vector-of-\n",
      "\n",
      "\n",
      "to only relevant edges.\n",
      "2. Generic CSR Format:Converting from vector-of-\n",
      "vectors to contiguous arrays dramatically improved\n",
      "cache locality and eliminated pointer chasing overhead.\n",
      "3. Memory Pooling:Pre-allocating and reusing vectors\n",
      "eliminated allocation overhead that previously domi-\n",
      "nated query processing time.\n",
      "4. Graph Reordering:BFS-based node ID reassignment\n",
      "improved spatial locality by ensuring frequently co-\n",
      "accessed nodes have nearby memory addresses.\n",
      "\n",
      "\n",
      "improved spatial locality by ensuring frequently co-\n",
      "accessed nodes have nearby memory addresses.\n",
      "Failed Optimization Attempts:Our optimization jour-\n",
      "ney also included attempts that did not yield positive results,\n",
      "providing valuable lessons about the gap between theoretical\n",
      "improvements and practical performance.\n",
      "• We implemented asingle-label fast paththat added\n",
      "a conditional branch to bypass the label iteration loop\n",
      "when a query involved only one label. In theory, this\n",
      "\n",
      "\n",
      "when a query involved only one label. In theory, this\n",
      "should have reduced loop overhead for the majority of\n",
      "our queries which involved single labels. However, this\n",
      "branch was evaluated at every node during BFS traversal,\n",
      "and the CPU’s branch predictor struggled with the mixed\n",
      "true/false pattern, causing pipeline stalls that exceeded\n",
      "the time saved by skipping the loop.\n",
      "• We attemptedfrontier sorting, where we sorted the BFS\n",
      "frontier by node ID before processing each level, expect-\n",
      "\n",
      "\n",
      "frontier by node ID before processing each level, expect-\n",
      "ing that sequential memory access to CSR arrays would\n",
      "improve cache hit rates. While theoretically sound, the\n",
      "O(n log n) sorting cost at every BFS level outweighed\n",
      "the cache benefits, particularly since our graph reorder-\n",
      "ing optimization had already improved spatial locality at\n",
      "preprocessing time, making runtime sorting redundant.\n",
      "These failures reinforced that micro-optimizations intro-\n",
      "\n",
      "\n",
      "These failures reinforced that micro-optimizations intro-\n",
      "ducing additional branches or per-level computations must be\n",
      "empirically validated, as their overhead can compound across\n",
      "millions of node visits in large-scale graph traversals.\n",
      "The final system achieves strong performance through a\n",
      "combination of memory efficiency, cache-aware data struc-\n",
      "tures, and algorithmic simplicity. These techniques are\n",
      "broadly applicable to graph processing systems where traver-\n",
      "sal performance is critical.\n",
      "\n",
      "\n",
      "broadly applicable to graph processing systems where traver-\n",
      "sal performance is critical.\n",
      "6 CONCLUSION\n",
      "This project addressed two fundamental challenges in graph\n",
      "database systems: cardinality estimation and efficient query\n",
      "evaluation.\n",
      "For cardinality estimation (Part 1), we developed a hybrid\n",
      "estimator combining end-biased sampling, stratified reser-\n",
      "voir sampling, correlated hashing, and XSEED-based simula-\n",
      "tion. This approach achieved competitive accuracy on both\n",
      "\n",
      "\n",
      "tion. This approach achieved competitive accuracy on both\n",
      "synthetic and real workloads while maintaining reasonable\n",
      "preparation and estimation times.\n",
      "For query evaluation (Part 2), we systematically optimized\n",
      "the Quicksilver system through memory pooling, CSR rep-\n",
      "resentations, graph reordering, and label-indexed data struc-\n",
      "tures. We learned that careful attention to memory access\n",
      "patterns and data structure design can yield dramatic perfor-\n",
      "mance gains.\n",
      "REFERENCES\n",
      "\n",
      "\n",
      "patterns and data structure design can yield dramatic perfor-\n",
      "mance gains.\n",
      "REFERENCES\n",
      "[1] B. Babcock and S. Chaudhuri. Towards a robust query optimizer: A principled\n",
      "and practical approach. InProceedings of the 2005 ACM SIGMOD International\n",
      "Conference on Management of Data, pages 119–130, Baltimore, MD, USA, 2005.\n",
      "ACM.\n",
      "[2] S. Beamer, K. Asanovic, and D. Patterson. Direction-optimizing breadth-first\n",
      "search. InProceedings of the International Conference for High Performance\n",
      "\n",
      "\n",
      "search. InProceedings of the International Conference for High Performance\n",
      "Computing, Networking, Storage and Analysis (SC), Salt Lake City, Utah, USA,\n",
      "Nov. 2012. IEEE.\n",
      "[3] G. Cormode. Synopses for massive data: Samples, histograms, wavelets, sketches.\n",
      "F oundations and Trends in Databases, 4(1–3):1–294, 2011.\n",
      "[4] L. Dhulipala, J. Shi, T. Tseng, G. E. Blelloch, and J. Shun. The graph based\n",
      "benchmark suite (gbbs). In3rd Joint International Workshop on Graph Data\n",
      "\n",
      "\n",
      "benchmark suite (gbbs). In3rd Joint International Workshop on Graph Data\n",
      "Management Experiences & Systems (GRADES) and Network Data Analytics\n",
      "(NDA) (GRADES-NDA’20), Portland, OR, USA, June 2020. ACM.\n",
      "[5] C. Estan and J. F. Naughton. End-biased samples for join cardinality estima-\n",
      "tion. InProceedings of the 22nd International Conference on Data Engineering\n",
      "(ICDE’06), page 20, Atlanta, GA, USA, 2006. IEEE.\n",
      "[6] F. Li, B. Wu, K. Yi, and Z. Zhao. Wander join: Online aggregation via random\n",
      "\n",
      "\n",
      "[6] F. Li, B. Wu, K. Yi, and Z. Zhao. Wander join: Online aggregation via random\n",
      "walks. InProceedings of the 2016 ACM SIGMOD International Conference on\n",
      "Management of Data, pages 615–629, San Francisco, CA, USA, 2016. ACM.\n",
      "[7] T. Neumann and G. Moerkotte. Characteristic sets: Accurate cardinality esti-\n",
      "mation for RDF queries with multiple joins. In2011 IEEE 27th International\n",
      "Conference on Data Engineering (ICDE 2011), pages 984–994, Hannover, Ger-\n",
      "many, Apr. 2011. IEEE.\n",
      "\n",
      "\n",
      "Conference on Data Engineering (ICDE 2011), pages 984–994, Hannover, Ger-\n",
      "many, Apr. 2011. IEEE.\n",
      "[8] J. Shun and G. E. Blelloch. Ligra: A lightweight graph processing framework\n",
      "for shared memory. InProceedings of the 18th ACM SIGPLAN Symposium on\n",
      "Principles and Practice of Parallel Programming (PPoPP). ACM, 2013.\n",
      "[9] H. Wei, J. X. Yu, C. Lu, and X. Lin. Speedup graph processing by graph ordering.\n",
      "InProceedings of the 2016 International Conference on Management of Data\n",
      "\n",
      "\n",
      "InProceedings of the 2016 International Conference on Management of Data\n",
      "(SIGMOD), San Francisco, CA, USA, 2016. ACM.\n",
      "[10] M. Winter, D. Mlakar, R. Zayer, H.-P. Seidel, and M. Steinberger. faimgraph:\n",
      "High performance management of fully-dynamic graphs under tight memory\n",
      "constraints on the gpu. InProceedings of SC18: International Conference for\n",
      "High Performance Computing, Networking, Storage and Analysis, Dallas, Texas,\n",
      "USA, Nov. 2018. IEEE.\n",
      "\n",
      "\n",
      "High Performance Computing, Networking, Storage and Analysis, Dallas, Texas,\n",
      "USA, Nov. 2018. IEEE.\n",
      "[11] N. Zhang, M. Ozsu, A. Aboulnaga, and I. Ilyas. Xseed: Accurate and fast\n",
      "cardinality estimation for xpath queries. In22nd International Conference on\n",
      "Data Engineering (ICDE’06), pages 61–61, 2006.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for document in pdf_documents:\n",
    "    print(document.page_content, end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "966e32ca-f4bf-4f12-b957-9564926a03c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ladies and gentlemen, friends, and fellow builders,\\n\\nWe live in an era where change no longer knocks on the door. It breaks it down. Every year, technology moves faster, systems grow more complex, and decisions that once took decades now take months, sometimes weeks. And in the middle of all this acceleration, there is one uncomfortable truth we must face: progress without direction is not innovation—it is chaos.\\n\\nFor generations, we measured success by what we could build. Bigger machines. Faster networks. Smarter systems. But today, the real challenge is not whether we can build something. It is whether we should, and if we do, whether we understand the consequences of what we are putting into the world.\\n\\nTechnology is not neutral. Every line of code carries assumptions. Every model reflects priorities. Every system amplifies certain voices while muting others. When we say we are “just engineers” or “just researchers,” we ignore the fact that our work shapes how people live, learn, communicate, and even think.\\n\\nLook around us. Algorithms decide what news we see. Automated systems influence who gets a loan, who gets a job interview, and who is flagged as a risk. These systems are efficient, scalable, and impressive. But efficiency without accountability can quietly erode trust. And trust, once lost, is far harder to rebuild than any system we deploy.\\n\\nThe future we are building will not be defined by our tools alone. It will be defined by the values we embed into them. If we optimize only for speed, we will get speed. If we optimize only for profit, we will get profit. But if we fail to optimize for fairness, transparency, and human dignity, we should not be surprised when the systems we create feel cold, distant, and unfair.'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create_documents\n",
    "speech = \"\"\n",
    "with open(\"./Data/Text/speech.txt\") as f:\n",
    "    speech = f.read()\n",
    "speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "fb51d067-2a7d-49b3-ad02-34b83812ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter_for_text = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "text_documents = text_splitter_for_text.create_documents([speech])\n",
    "# speech is not a list of documents by default, therefore we use create_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "bd0b86b7-ce6f-4807-9880-be3c1d7c147d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ladies and gentlemen, friends, and fellow builders,\n",
      "\n",
      "We live in an era where change no longer knocks on the door. It breaks it down. Every year, technology moves faster, systems grow more complex, and decisions that once took decades now take months, sometimes weeks. And in the middle of all this acceleration, there is one uncomfortable truth we must face: progress without direction is not innovation—it is chaos.\n",
      "\n",
      "For generations, we measured success by what we could build. Bigger machines. Faster networks. Smarter systems. But today, the real challenge is not whether we can build something. It is whether we should, and if we do, whether we understand the consequences of what we are putting into the world.\n",
      "\n",
      "Technology is not neutral. Every line of code carries assumptions. Every model reflects priorities. Every system amplifies certain voices while muting others. When we say we are “just engineers” or “just researchers,” we ignore the fact that our work shapes how people live, learn, communicate, and even think.\n",
      "\n",
      "Look around us. Algorithms decide what news we see. Automated systems influence who gets a loan, who gets a job interview, and who is flagged as a risk. These systems are efficient, scalable, and impressive. But efficiency without accountability can quietly erode trust. And trust, once lost, is far harder to rebuild than any system we deploy.\n",
      "\n",
      "The future we are building will not be defined by our tools alone. It will be defined by the values we embed into them. If we optimize only for speed, we will get speed. If we optimize only for profit, we will get profit. But if we fail to optimize for fairness, transparency, and human dignity, we should not be surprised when the systems we create feel cold, distant, and unfair.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for document in text_documents:\n",
    "    print(document.page_content, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee332b-0859-44f0-ab56-fdb1dafeed60",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Difference between RecursiveCharacterTextSplitter and CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2881496b-7647-4f40-b4b7-7a00a6c07e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CharacterTextSplitter: The dumb-but-predictable one.\n",
    "# How it works\n",
    "# 1.Splits text strictly by character count\n",
    "# 2.Uses one separator (default: \\n\\n)\n",
    "# 3. If a chunk is too big → it just hard-cuts it\n",
    "\n",
    "# RecursiveCharacterTextSplitter: The smart one. Default for a reason.\n",
    "# How it works\n",
    "# 1. Tries multiple separators in order\n",
    "# 2. Falls back recursively if chunk is still too big\n",
    "# 3. Default separator priority: [\"\\n\\n\", \"\\n\", \" \", \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "187348d0-8ee9-438b-95fd-f7681230ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter\n",
    ")\n",
    "\n",
    "basic = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "smart = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "b5f36527-c007-4579-81e7-012a3f71c30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2671, which is longer than the specified 1000\n",
      "Created a chunk of size 1373, which is longer than the specified 1000\n",
      "Created a chunk of size 1281, which is longer than the specified 1000\n",
      "Created a chunk of size 2352, which is longer than the specified 1000\n",
      "Created a chunk of size 1731, which is longer than the specified 1000\n",
      "Created a chunk of size 1067, which is longer than the specified 1000\n",
      "Created a chunk of size 1475, which is longer than the specified 1000\n",
      "Created a chunk of size 2881, which is longer than the specified 1000\n",
      "Created a chunk of size 1980, which is longer than the specified 1000\n",
      "Created a chunk of size 4145, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "character_docs = basic.split_documents(loaded_web_documents)\n",
    "recursive_character_docs = smart.split_documents(loaded_web_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "15190f73-fc71-4d30-a833-09823c5b2258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "PAGE_END\n",
      "Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "\n",
      "Tool use\n",
      "\n",
      "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\n",
      "\n",
      "Overview of a LLM-powered autonomous agent system.\n",
      "PAGE_END\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\n",
      "ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\n",
      "The ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\n",
      "Thought: ...\n",
      "Action: ...\n",
      "Observation: ...\n",
      "... (Repeated many times)\n",
      "PAGE_END\n",
      "Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\n",
      "\n",
      "In both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\n",
      "Reflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\n",
      "\n",
      "\n",
      "Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "PAGE_END\n",
      "Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "\n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\n",
      "\n",
      "\n",
      "Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\n",
      "PAGE_END\n",
      "Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\n",
      "To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\n",
      "The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\n",
      "PAGE_END\n",
      "After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\n",
      "\n",
      "The idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\n",
      "\n",
      "\n",
      "Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\n",
      "PAGE_END\n",
      "The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\n",
      "In reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\n",
      "In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\n",
      "PAGE_END\n",
      "Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\n",
      "\n",
      "Component Two: Memory#\n",
      "(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\n",
      "Types of Memory#\n",
      "Memory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\n",
      "\n",
      "\n",
      "Sensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\n",
      "PAGE_END\n",
      "Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\n",
      "\n",
      "\n",
      "Long-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\n",
      "\n",
      "Explicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\n",
      "Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\n",
      "\n",
      "Categorization of human memory.\n",
      "\n",
      "We can roughly consider the following mappings:\n",
      "PAGE_END\n",
      "Categorization of human memory.\n",
      "\n",
      "We can roughly consider the following mappings:\n",
      "\n",
      "Sensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\n",
      "Short-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\n",
      "Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\n",
      "\n",
      "Maximum Inner Product Search (MIPS)#\n",
      "The external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)​ algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\n",
      "A couple common choices of ANN algorithms for fast MIPS:\n",
      "PAGE_END\n",
      "LSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\n",
      "ANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\n",
      "HNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\n",
      "FAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\n",
      "ScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\tilde{x}_i$ such that the inner product $\\langle q, x_i \\rangle$ is as similar to the original distance of $\\angle q, \\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\n",
      "PAGE_END\n",
      "Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\n",
      "\n",
      "Check more MIPS algorithms and performance comparison in ann-benchmarks.com.\n",
      "Component Three: Tool Use#\n",
      "Tool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\n",
      "\n",
      "\n",
      "A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\n",
      "PAGE_END\n",
      "MRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\n",
      "They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\n",
      "Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\n",
      "ChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\n",
      "HuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\n",
      "PAGE_END\n",
      "Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\n",
      "\n",
      "The system comprises of 4 stages:\n",
      "(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\n",
      "Instruction:\n",
      "PAGE_END\n",
      "The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\n",
      "PAGE_END\n",
      "(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\n",
      "Instruction:\n",
      "\n",
      "Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\n",
      "\n",
      "(3) Task execution: Expert models execute on the specific tasks and log results.\n",
      "Instruction:\n",
      "PAGE_END\n",
      "(3) Task execution: Expert models execute on the specific tasks and log results.\n",
      "Instruction:\n",
      "\n",
      "With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\n",
      "PAGE_END\n",
      "(4) Response generation: LLM receives the execution results and provides summarized results to users.\n",
      "To put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\n",
      "API-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\n",
      "PAGE_END\n",
      "Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\n",
      "\n",
      "In the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\n",
      "\n",
      "Whether an API call is needed.\n",
      "Identify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\n",
      "Response based on the API results: the model can choose to refine and call again if results are not satisfied.\n",
      "\n",
      "This benchmark evaluates the agent’s tool use capabilities at three levels:\n",
      "PAGE_END\n",
      "This benchmark evaluates the agent’s tool use capabilities at three levels:\n",
      "\n",
      "Level-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\n",
      "Level-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\n",
      "Level-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\n",
      "PAGE_END\n",
      "Case Studies#\n",
      "Scientific Discovery Agent#\n",
      "ChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\n",
      "\n",
      "The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\n",
      "It is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\n",
      "PAGE_END\n",
      "One interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\n",
      "Boiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\n",
      "For example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\n",
      "PAGE_END\n",
      "inquired about current trends in anticancer drug discovery;\n",
      "selected a target;\n",
      "requested a scaffold targeting these compounds;\n",
      "Once the compound was identified, the model attempted its synthesis.\n",
      "PAGE_END\n",
      "They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\n",
      "Generative Agents Simulation#\n",
      "Generative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\n",
      "The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\n",
      "PAGE_END\n",
      "Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\n",
      "\n",
      "Each element is an observation, an event directly provided by the agent.\n",
      "- Inter-agent communication can trigger new natural language statements.\n",
      "\n",
      "\n",
      "Retrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\n",
      "\n",
      "Recency: recent events have higher scores\n",
      "Importance: distinguish mundane from core memories. Ask LM directly.\n",
      "Relevance: based on how related it is to the current situation / query.\n",
      "\n",
      "\n",
      "Reflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\n",
      "PAGE_END\n",
      "Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\n",
      "\n",
      "\n",
      "Planning & Reacting: translate the reflections and the environment information into actions\n",
      "\n",
      "Planning is essentially in order to optimize believability at the moment vs in time.\n",
      "Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\n",
      "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
      "Environment information is present in a tree structure.\n",
      "\n",
      "The generative agent architecture. (Image source: Park et al. 2023)\n",
      "PAGE_END\n",
      "The generative agent architecture. (Image source: Park et al. 2023)\n",
      "\n",
      "This fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\n",
      "Proof-of-Concept Examples#\n",
      "AutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\n",
      "Here is the system message used by AutoGPT, where {{...}} are user inputs:\n",
      "You are {{ai-name}}, {{user-provided AI bot description}}.\n",
      "Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
      "\n",
      "GOALS:\n",
      "PAGE_END\n",
      "GOALS:\n",
      "\n",
      "1. {{user-provided goal 1}}\n",
      "2. {{user-provided goal 2}}\n",
      "3. ...\n",
      "4. ...\n",
      "5. ...\n",
      "\n",
      "Constraints:\n",
      "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
      "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
      "3. No user assistance\n",
      "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
      "5. Use subprocesses for commands that will not terminate within a few minutes\n",
      "PAGE_END\n",
      "Commands:\n",
      "1. Google Search: \"google\", args: \"input\": \"<search>\"\n",
      "2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\n",
      "3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\n",
      "4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\n",
      "5. List GPT Agents: \"list_agents\", args:\n",
      "6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\n",
      "7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\n",
      "8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n",
      "9. Read file: \"read_file\", args: \"file\": \"<file>\"\n",
      "10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n",
      "11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\n",
      "12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\n",
      "13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\n",
      "14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\n",
      "15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\n",
      "16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\n",
      "17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\n",
      "18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\n",
      "19. Do Nothing: \"do_nothing\", args:\n",
      "20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\n",
      "PAGE_END\n",
      "Resources:\n",
      "1. Internet access for searches and information gathering.\n",
      "2. Long Term memory management.\n",
      "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
      "4. File output.\n",
      "\n",
      "Performance Evaluation:\n",
      "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
      "2. Constructively self-criticize your big-picture behavior constantly.\n",
      "3. Reflect on past decisions and strategies to refine your approach.\n",
      "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
      "PAGE_END\n",
      "You should only respond in JSON format as described below\n",
      "Response Format:\n",
      "{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"thought\",\n",
      "        \"reasoning\": \"reasoning\",\n",
      "        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n",
      "        \"criticism\": \"constructive self-criticism\",\n",
      "        \"speak\": \"thoughts summary to say to user\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"command name\",\n",
      "        \"args\": {\n",
      "            \"arg name\": \"value\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Ensure the response can be parsed by Python json.loads\n",
      "GPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\n",
      "Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\nThen you will pick one clarifying question, and wait for an answer from the user.\\n\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\n\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Summary of areas that need clarification:\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\n2. Details about the MVC components (e.g. which components are in each file)\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\n\\nClarifying question:\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\n\\nIs anything else unclear? If yes, only answer in the form:\\n{remaining unclear areas} remaining questions.\\n{Next question}\\nIf everything is sufficiently clear, only answer \\\"Nothing more to clarify.\\\".\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Remaining unclear areas: 2 remaining questions.\\nCan you provide more information about how the MVC components are split into separate files?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\n",
      "  }\n",
      "]\n",
      "Then after these clarification, the agent moved into the code writing mode with a different system message.\n",
      "System message:\n",
      "PAGE_END\n",
      "You will get instructions for code to write.\n",
      "You will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\n",
      "Make sure that every detail of the architecture is, in the end, implemented as code.\n",
      "Think step by step and reason yourself to the right decisions to make sure we get it right.\n",
      "You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\n",
      "Then you will output the content of each file including ALL code.\n",
      "Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that\n",
      "FILENAME is the lowercase file name including the file extension,\n",
      "LANG is the markup code block language for the code’s language, and CODE is the code:\n",
      "FILENAME\n",
      "CODE\n",
      "You will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\n",
      "Please note that the code should be fully functional. No placeholders.\n",
      "Follow a language and framework appropriate best practice file naming convention.\n",
      "Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\n",
      "Ensure to implement all code, if you are unsure, write a plausible implementation.\n",
      "Include module dependency or package manager dependency definition file.\n",
      "Before you finish, double check that all parts of the architecture is present in the files.\n",
      "Useful to know:\n",
      "You almost always put different classes in different files.\n",
      "For Python, you always create an appropriate requirements.txt file.\n",
      "For NodeJS, you always create an appropriate package.json file.\n",
      "You always add a comment briefly describing the purpose of the function definition.\n",
      "You try to add comments explaining very complex bits of logic.\n",
      "You always follow the best practices for the requested languages in terms of describing the code written as a defined\n",
      "package/project.\n",
      "Python toolbelt preferences:\n",
      "PAGE_END\n",
      "pytest\n",
      "dataclasses\n",
      "PAGE_END\n",
      "Conversatin samples:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\n\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\n\\n\\nPython toolbelt preferences:\\n- pytest\\n- dataclasses\\n\"\n",
      "  },\n",
      " #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Assumptions:\\n1. Model: The model will contain the game's data, such as level information, character states, and enemy positions.\\n2. View: The view will handle the game's visuals, including rendering the game objects, backgrounds, and updating the display.\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\n\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Please now remember the steps:\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nPlease note that the code should be fully functional. No placeholders.\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\"\n",
      "  }\n",
      "]\n",
      "Challenges#\n",
      "After going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\n",
      "PAGE_END\n",
      "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n",
      "\n",
      "\n",
      "Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\n",
      "PAGE_END\n",
      "Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\n",
      "\n",
      "\n",
      "Citation#\n",
      "Cited as:\n",
      "\n",
      "Weng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\n",
      "PAGE_END\n",
      "Or\n",
      "@article{weng2023agent,\n",
      "  title   = \"LLM-powered Autonomous Agents\",\n",
      "  author  = \"Weng, Lilian\",\n",
      "  journal = \"lilianweng.github.io\",\n",
      "  year    = \"2023\",\n",
      "  month   = \"Jun\",\n",
      "  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
      "}\n",
      "References#\n",
      "[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\n",
      "[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\n",
      "[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\n",
      "“ arXiv preprint arXiv:2302.02676 (2023).\n",
      "[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\n",
      "[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\n",
      "[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\n",
      "[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n",
      "[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\n",
      "[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\n",
      "[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\n",
      "[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\n",
      "[12] Parisi et al. “TALM: Tool Augmented Language Models”\n",
      "[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\n",
      "[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\n",
      "[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\n",
      "[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\n",
      "[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\n",
      "[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\n",
      "[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\n",
      "[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\n",
      "[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\n",
      "PAGE_END\n"
     ]
    }
   ],
   "source": [
    "for item in character_docs:\n",
    "    print(item.page_content, end=\"\\nPAGE_END\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "3682f4d5-3045-4028-9582-15dad09daca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "PAGE_END\n",
      "Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "\n",
      "Tool use\n",
      "\n",
      "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overview of a LLM-powered autonomous agent system.\n",
      "PAGE_END\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "PAGE_END\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "PAGE_END\n",
      "Self-Reflection#\n",
      "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\n",
      "ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\n",
      "The ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\n",
      "Thought: ...\n",
      "Action: ...\n",
      "Observation: ...\n",
      "... (Repeated many times)\n",
      "PAGE_END\n",
      "Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\n",
      "\n",
      "In both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\n",
      "Reflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\n",
      "\n",
      "\n",
      "Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "PAGE_END\n",
      "Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "\n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\n",
      "\n",
      "\n",
      "Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\n",
      "PAGE_END\n",
      "Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\n",
      "PAGE_END\n",
      "To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\n",
      "The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\n",
      "PAGE_END\n",
      "After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\n",
      "\n",
      "The idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\n",
      "\n",
      "\n",
      "Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\n",
      "PAGE_END\n",
      "The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\n",
      "In reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\n",
      "PAGE_END\n",
      "In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\n",
      "PAGE_END\n",
      "Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\n",
      "\n",
      "Component Two: Memory#\n",
      "(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\n",
      "Types of Memory#\n",
      "Memory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\n",
      "\n",
      "\n",
      "Sensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\n",
      "PAGE_END\n",
      "Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\n",
      "\n",
      "\n",
      "Long-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\n",
      "\n",
      "Explicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\n",
      "Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Categorization of human memory.\n",
      "\n",
      "We can roughly consider the following mappings:\n",
      "PAGE_END\n",
      "Categorization of human memory.\n",
      "\n",
      "We can roughly consider the following mappings:\n",
      "\n",
      "Sensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\n",
      "Short-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\n",
      "Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\n",
      "PAGE_END\n",
      "Maximum Inner Product Search (MIPS)#\n",
      "The external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)​ algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\n",
      "A couple common choices of ANN algorithms for fast MIPS:\n",
      "PAGE_END\n",
      "LSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\n",
      "ANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\n",
      "PAGE_END\n",
      "HNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\n",
      "PAGE_END\n",
      "FAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\n",
      "ScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\tilde{x}_i$ such that the inner product $\\langle q, x_i \\rangle$ is as similar to the original distance of $\\angle q, \\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\n",
      "PAGE_END\n",
      "Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\n",
      "\n",
      "Check more MIPS algorithms and performance comparison in ann-benchmarks.com.\n",
      "Component Three: Tool Use#\n",
      "Tool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\n",
      "\n",
      "\n",
      "A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\n",
      "PAGE_END\n",
      "MRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\n",
      "They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\n",
      "PAGE_END\n",
      "Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\n",
      "ChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\n",
      "HuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\n",
      "PAGE_END\n",
      "Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\n",
      "\n",
      "The system comprises of 4 stages:\n",
      "(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\n",
      "Instruction:\n",
      "PAGE_END\n",
      "The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\n",
      "PAGE_END\n",
      "(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\n",
      "Instruction:\n",
      "\n",
      "Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\n",
      "\n",
      "(3) Task execution: Expert models execute on the specific tasks and log results.\n",
      "Instruction:\n",
      "PAGE_END\n",
      "(3) Task execution: Expert models execute on the specific tasks and log results.\n",
      "Instruction:\n",
      "\n",
      "With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\n",
      "PAGE_END\n",
      "(4) Response generation: LLM receives the execution results and provides summarized results to users.\n",
      "To put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\n",
      "PAGE_END\n",
      "API-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\n",
      "PAGE_END\n",
      "Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\n",
      "\n",
      "In the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\n",
      "\n",
      "Whether an API call is needed.\n",
      "Identify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\n",
      "Response based on the API results: the model can choose to refine and call again if results are not satisfied.\n",
      "\n",
      "This benchmark evaluates the agent’s tool use capabilities at three levels:\n",
      "PAGE_END\n",
      "This benchmark evaluates the agent’s tool use capabilities at three levels:\n",
      "\n",
      "Level-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\n",
      "Level-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\n",
      "Level-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\n",
      "PAGE_END\n",
      "Case Studies#\n",
      "Scientific Discovery Agent#\n",
      "ChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\n",
      "\n",
      "The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\n",
      "It is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\n",
      "PAGE_END\n",
      "One interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\n",
      "Boiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\n",
      "For example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\n",
      "PAGE_END\n",
      "inquired about current trends in anticancer drug discovery;\n",
      "selected a target;\n",
      "requested a scaffold targeting these compounds;\n",
      "Once the compound was identified, the model attempted its synthesis.\n",
      "PAGE_END\n",
      "They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\n",
      "Generative Agents Simulation#\n",
      "Generative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\n",
      "The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\n",
      "PAGE_END\n",
      "Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\n",
      "\n",
      "Each element is an observation, an event directly provided by the agent.\n",
      "- Inter-agent communication can trigger new natural language statements.\n",
      "\n",
      "\n",
      "Retrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\n",
      "\n",
      "Recency: recent events have higher scores\n",
      "Importance: distinguish mundane from core memories. Ask LM directly.\n",
      "Relevance: based on how related it is to the current situation / query.\n",
      "\n",
      "\n",
      "Reflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\n",
      "PAGE_END\n",
      "Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\n",
      "\n",
      "\n",
      "Planning & Reacting: translate the reflections and the environment information into actions\n",
      "\n",
      "Planning is essentially in order to optimize believability at the moment vs in time.\n",
      "Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\n",
      "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
      "Environment information is present in a tree structure.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The generative agent architecture. (Image source: Park et al. 2023)\n",
      "PAGE_END\n",
      "The generative agent architecture. (Image source: Park et al. 2023)\n",
      "\n",
      "This fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\n",
      "Proof-of-Concept Examples#\n",
      "AutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\n",
      "Here is the system message used by AutoGPT, where {{...}} are user inputs:\n",
      "You are {{ai-name}}, {{user-provided AI bot description}}.\n",
      "Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
      "\n",
      "GOALS:\n",
      "PAGE_END\n",
      "GOALS:\n",
      "\n",
      "1. {{user-provided goal 1}}\n",
      "2. {{user-provided goal 2}}\n",
      "3. ...\n",
      "4. ...\n",
      "5. ...\n",
      "\n",
      "Constraints:\n",
      "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
      "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
      "3. No user assistance\n",
      "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
      "5. Use subprocesses for commands that will not terminate within a few minutes\n",
      "PAGE_END\n",
      "Commands:\n",
      "1. Google Search: \"google\", args: \"input\": \"<search>\"\n",
      "2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\n",
      "3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\n",
      "4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\n",
      "5. List GPT Agents: \"list_agents\", args:\n",
      "6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\n",
      "7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\n",
      "8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n",
      "9. Read file: \"read_file\", args: \"file\": \"<file>\"\n",
      "10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n",
      "11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\n",
      "12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\n",
      "13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\n",
      "PAGE_END\n",
      "11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\n",
      "12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\n",
      "13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\n",
      "14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\n",
      "15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\n",
      "16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\n",
      "17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\n",
      "18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\n",
      "19. Do Nothing: \"do_nothing\", args:\n",
      "20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\n",
      "PAGE_END\n",
      "Resources:\n",
      "1. Internet access for searches and information gathering.\n",
      "2. Long Term memory management.\n",
      "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
      "4. File output.\n",
      "\n",
      "Performance Evaluation:\n",
      "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
      "2. Constructively self-criticize your big-picture behavior constantly.\n",
      "3. Reflect on past decisions and strategies to refine your approach.\n",
      "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
      "PAGE_END\n",
      "You should only respond in JSON format as described below\n",
      "Response Format:\n",
      "{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"thought\",\n",
      "        \"reasoning\": \"reasoning\",\n",
      "        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n",
      "        \"criticism\": \"constructive self-criticism\",\n",
      "        \"speak\": \"thoughts summary to say to user\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"command name\",\n",
      "        \"args\": {\n",
      "            \"arg name\": \"value\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Ensure the response can be parsed by Python json.loads\n",
      "GPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\n",
      "Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "PAGE_END\n",
      "Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\nThen you will pick one clarifying question, and wait for an answer from the user.\\n\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\n\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "PAGE_END\n",
      "},\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\n\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Summary of areas that need clarification:\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\n2. Details about the MVC components (e.g. which components are in each file)\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\n\\nClarifying question:\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "PAGE_END\n",
      "},\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\n\\nIs anything else unclear? If yes, only answer in the form:\\n{remaining unclear areas} remaining questions.\\n{Next question}\\nIf everything is sufficiently clear, only answer \\\"Nothing more to clarify.\\\".\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Remaining unclear areas: 2 remaining questions.\\nCan you provide more information about how the MVC components are split into separate files?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\n",
      "  }\n",
      "]\n",
      "Then after these clarification, the agent moved into the code writing mode with a different system message.\n",
      "PAGE_END\n",
      "}\n",
      "]\n",
      "Then after these clarification, the agent moved into the code writing mode with a different system message.\n",
      "System message:\n",
      "PAGE_END\n",
      "You will get instructions for code to write.\n",
      "You will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\n",
      "Make sure that every detail of the architecture is, in the end, implemented as code.\n",
      "Think step by step and reason yourself to the right decisions to make sure we get it right.\n",
      "You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\n",
      "Then you will output the content of each file including ALL code.\n",
      "Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that\n",
      "FILENAME is the lowercase file name including the file extension,\n",
      "LANG is the markup code block language for the code’s language, and CODE is the code:\n",
      "FILENAME\n",
      "CODE\n",
      "You will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\n",
      "PAGE_END\n",
      "FILENAME\n",
      "CODE\n",
      "You will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\n",
      "Please note that the code should be fully functional. No placeholders.\n",
      "Follow a language and framework appropriate best practice file naming convention.\n",
      "Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\n",
      "Ensure to implement all code, if you are unsure, write a plausible implementation.\n",
      "Include module dependency or package manager dependency definition file.\n",
      "Before you finish, double check that all parts of the architecture is present in the files.\n",
      "Useful to know:\n",
      "You almost always put different classes in different files.\n",
      "For Python, you always create an appropriate requirements.txt file.\n",
      "For NodeJS, you always create an appropriate package.json file.\n",
      "You always add a comment briefly describing the purpose of the function definition.\n",
      "You try to add comments explaining very complex bits of logic.\n",
      "PAGE_END\n",
      "You always add a comment briefly describing the purpose of the function definition.\n",
      "You try to add comments explaining very complex bits of logic.\n",
      "You always follow the best practices for the requested languages in terms of describing the code written as a defined\n",
      "package/project.\n",
      "Python toolbelt preferences:\n",
      "PAGE_END\n",
      "pytest\n",
      "dataclasses\n",
      "PAGE_END\n",
      "Conversatin samples:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "PAGE_END\n",
      "\"content\": \"You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nPlease\n",
      "PAGE_END\n",
      "for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\n\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of\n",
      "PAGE_END\n",
      "Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\n\\n\\nPython toolbelt preferences:\\n- pytest\\n- dataclasses\\n\"\n",
      "PAGE_END\n",
      "},\n",
      " #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Assumptions:\\n1. Model: The model will contain the game's data, such as level information, character states, and enemy positions.\\n2. View: The view will handle the game's visuals, including rendering the game objects, backgrounds, and updating the display.\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\n\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "PAGE_END\n",
      "\"content\": \"Please now remember the steps:\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nPlease note that the code should be fully functional. No placeholders.\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. The code should be fully\n",
      "PAGE_END\n",
      "are imported by that file, and so on.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\"\n",
      "PAGE_END\n",
      "}\n",
      "]\n",
      "Challenges#\n",
      "After going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\n",
      "PAGE_END\n",
      "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n",
      "\n",
      "\n",
      "Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\n",
      "PAGE_END\n",
      "Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\n",
      "\n",
      "\n",
      "Citation#\n",
      "Cited as:\n",
      "\n",
      "Weng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\n",
      "PAGE_END\n",
      "Or\n",
      "@article{weng2023agent,\n",
      "  title   = \"LLM-powered Autonomous Agents\",\n",
      "  author  = \"Weng, Lilian\",\n",
      "  journal = \"lilianweng.github.io\",\n",
      "  year    = \"2023\",\n",
      "  month   = \"Jun\",\n",
      "  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
      "}\n",
      "References#\n",
      "[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\n",
      "[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\n",
      "[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\n",
      "“ arXiv preprint arXiv:2302.02676 (2023).\n",
      "[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\n",
      "[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\n",
      "[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\n",
      "[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n",
      "PAGE_END\n",
      "[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\n",
      "[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n",
      "[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\n",
      "[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\n",
      "[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\n",
      "[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\n",
      "[12] Parisi et al. “TALM: Tool Augmented Language Models”\n",
      "[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\n",
      "[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\n",
      "PAGE_END\n",
      "[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\n",
      "[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\n",
      "[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\n",
      "[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\n",
      "[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\n",
      "[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\n",
      "[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\n",
      "[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\n",
      "[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\n",
      "PAGE_END\n"
     ]
    }
   ],
   "source": [
    "for item in recursive_character_docs:\n",
    "    print(item.page_content, end=\"\\nPAGE_END\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f7519e-77de-4c04-9c79-92b083ef6178",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Splitting HTML Header Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9cce8b6a-cb11-4b2f-bf2c-7b25f364180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "54cc3093-55c7-43d7-a1b6-ae0151fd3734",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <meta charset=\"UTF-8\">\n",
    "  <title>Product Overview – NovaX Pro</title>\n",
    "  <meta name=\"description\" content=\"NovaX Pro is a high-performance smart device designed for professionals.\">\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "  <header>\n",
    "    <h1>NovaX Pro</h1>\n",
    "    <p class=\"tagline\">Power. Precision. Performance.</p>\n",
    "  </header>\n",
    "\n",
    "  <section id=\"overview\">\n",
    "    <h2>Overview</h2>\n",
    "    <p>\n",
    "      NovaX Pro is a next-generation smart device built for engineers, designers,\n",
    "      and data professionals who demand speed, reliability, and flexibility.\n",
    "      Designed with a minimalist aesthetic and engineered for maximum performance,\n",
    "      NovaX Pro adapts seamlessly to modern workflows.\n",
    "    </p>\n",
    "  </section>\n",
    "\n",
    "  <section id=\"features\">\n",
    "    <h2>Key Features</h2>\n",
    "  </section>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b65fafc4-fb68-4f63-b439-20ec6a67f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\")\n",
    "]\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_documents = html_splitter.split_text(html_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "a35416da-d2c3-451e-8495-29cb232b1317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NovaX Pro\n",
      "\n",
      "Power. Precision. Performance.\n",
      "\n",
      "Overview\n",
      "\n",
      "NovaX Pro is a next-generation smart device built for engineers, designers,\n",
      "      and data professionals who demand speed, reliability, and flexibility.\n",
      "      Designed with a minimalist aesthetic and engineered for maximum performance,\n",
      "      NovaX Pro adapts seamlessly to modern workflows.\n",
      "\n",
      "Key Features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in html_header_documents:\n",
    "    print(item.page_content, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d6b668ae-f9ff-4c7d-b076-b28fdabe5510",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_header_url_documents = html_splitter.split_text_from_url(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6ea5ab96-ff1f-4c8f-85e4-033477f91a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if (localStorage.getItem(\"pref-theme\") === \"dark\") {\n",
      "        document.body.classList.add('dark');\n",
      "    } else if (localStorage.getItem(\"pref-theme\") === \"light\") {\n",
      "        document.body.classList.remove('dark')\n",
      "    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {\n",
      "        document.body.classList.add('dark');\n",
      "    }  \n",
      "MathJax = {\n",
      "    tex: {\n",
      "      inlineMath: [['$', '$'], ['\\\\(', '\\\\)']],\n",
      "      displayMath: [['$$','$$'], ['\\\\[', '\\\\]']],\n",
      "      processEscapes: true,\n",
      "      processEnvironments: true\n",
      "    },\n",
      "    options: {\n",
      "      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']\n",
      "    }\n",
      "  };\n",
      "\n",
      "  window.addEventListener('load', (event) => {\n",
      "      document.querySelectorAll(\"mjx-container\").forEach(function(x){\n",
      "        x.parentElement.classList += 'has-jax'})\n",
      "    });  \n",
      "Lil'Log  \n",
      "|  \n",
      "Posts  \n",
      "Archive  \n",
      "Search  \n",
      "Tags  \n",
      "FAQ\n",
      "\n",
      "LLM Powered Autonomous Agents\n",
      "\n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng  \n",
      "Table of Contents  \n",
      "Agent System Overview  \n",
      "Component One: Planning  \n",
      "Task Decomposition  \n",
      "Self-Reflection  \n",
      "Component Two: Memory  \n",
      "Types of Memory  \n",
      "Maximum Inner Product Search (MIPS)  \n",
      "Component Three: Tool Use  \n",
      "Case Studies  \n",
      "Scientific Discovery Agent  \n",
      "Generative Agents Simulation  \n",
      "Proof-of-Concept Examples  \n",
      "Challenges  \n",
      "Citation  \n",
      "References  \n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as , and , serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.  \n",
      "AutoGPT  \n",
      "GPT-Engineer  \n",
      "BabyAGI\n",
      "\n",
      "Agent System Overview\n",
      "\n",
      "#  \n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:  \n",
      "Planning  \n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.  \n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.  \n",
      "Memory  \n",
      "Short-term memory: I would consider all the in-context learning (See ) as utilizing short-term memory of the model to learn.  \n",
      "Prompt Engineering  \n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.  \n",
      "Tool use  \n",
      "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.  \n",
      "Overview of a LLM-powered autonomous agent system.\n",
      "\n",
      "Component One: Planning\n",
      "\n",
      "#  \n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "\n",
      "Task Decomposition\n",
      "\n",
      "#  \n",
      "(CoT; ) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.  \n",
      "Chain of thought  \n",
      "Wei et al. 2022  \n",
      "( ) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.  \n",
      "Tree of Thoughts  \n",
      "Yao et al. 2023  \n",
      "Task decomposition can be done (1) by LLM with simple prompting like , , (2) by using task-specific instructions; e.g. for writing a novel, or (3) with human inputs.  \n",
      "\"Steps for XYZ.\\n1.\"  \n",
      "\"What are the subgoals for achieving XYZ?\"  \n",
      "\"Write a story outline.\"  \n",
      "Another quite distinct approach, ( ), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.  \n",
      "LLM+P  \n",
      "Liu et al. 2023\n",
      "\n",
      "Self-Reflection\n",
      "\n",
      "#  \n",
      "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.  \n",
      "( ) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.  \n",
      "ReAct  \n",
      "Yao et al. 2023  \n",
      "The ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:  \n",
      "Thought: ...\n",
      "Action: ...\n",
      "Observation: ...\n",
      "... (Repeated many times)  \n",
      "Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: ).  \n",
      "Yao et al. 2023  \n",
      "In both experiments on knowledge-intensive tasks and decision-making tasks, works better than the -only baseline where step is removed.  \n",
      "ReAct  \n",
      "Act  \n",
      "Thought: …  \n",
      "( ) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may the environment to start a new trial depending on the self-reflection results.  \n",
      "Reflexion  \n",
      "Shinn & Labash 2023  \n",
      "decide to reset  \n",
      "Illustration of the Reflexion framework. (Image source: )  \n",
      "Shinn & Labash, 2023  \n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.  \n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.  \n",
      "Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: )  \n",
      "Shinn & Labash, 2023  \n",
      "(CoH; ) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.  \n",
      "Chain of Hindsight  \n",
      "Liu et al. 2023  \n",
      "To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.  \n",
      "The training dataset in their experiments is a combination of , and .  \n",
      "WebGPT comparisons  \n",
      "summarization from human feedback  \n",
      "human preference dataset  \n",
      "After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: )  \n",
      "Liu et al. 2023  \n",
      "The idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. (AD; ) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.  \n",
      "Algorithm Distillation  \n",
      "Laskin et al. 2023  \n",
      "algorithm  \n",
      "Illustration of how Algorithm Distillation (AD) works. (Image source: ).  \n",
      "Laskin et al. 2023  \n",
      "The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.  \n",
      "In reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.  \n",
      "In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by ), RL^2 ( ; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.  \n",
      "UCB  \n",
      "Duan et al. 2017  \n",
      "Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with for \"dark\" environments and for watermaze. (Image source: )  \n",
      "A3C  \n",
      "DQN  \n",
      "Laskin et al. 2023\n",
      "\n",
      "Component Two: Memory\n",
      "\n",
      "#  \n",
      "(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my with ChatGPT.)  \n",
      "conversations\n",
      "\n",
      "Types of Memory\n",
      "\n",
      "#  \n",
      "Memory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.  \n",
      ": This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).  \n",
      "Sensory Memory  \n",
      "(STM) or : It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items ( ) and lasts for 20-30 seconds.  \n",
      "Short-Term Memory  \n",
      "Working Memory  \n",
      "Miller 1956  \n",
      "(LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:  \n",
      "Long-Term Memory  \n",
      "Explicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).  \n",
      "Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.  \n",
      "Categorization of human memory.  \n",
      "We can roughly consider the following mappings:  \n",
      "Sensory memory as learning embedding representations for raw inputs, including text, image or other modalities;  \n",
      "Short-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.  \n",
      "Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\n",
      "\n",
      "Maximum Inner Product Search (MIPS)\n",
      "\n",
      "#  \n",
      "The external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search ( ). To optimize the retrieval speed, the common choice is the algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.  \n",
      "MIPS  \n",
      "approximate nearest neighbors (ANN)​  \n",
      "A couple common choices of ANN algorithms for fast MIPS:  \n",
      "(Locality-Sensitive Hashing): It introduces a function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.  \n",
      "LSH  \n",
      "hashing  \n",
      "(Approximate Nearest Neighbors Oh Yeah): The core data structure are , a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.  \n",
      "ANNOY  \n",
      "random projection trees  \n",
      "(Hierarchical Navigable Small World): It is inspired by the idea of where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.  \n",
      "HNSW  \n",
      "small world networks  \n",
      "(Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.  \n",
      "FAISS  \n",
      "clustering  \n",
      "(Scalable Nearest Neighbors): The main innovation in ScaNN is . It quantizes a data point $x_i$ to $\\tilde{x}_i$ such that the inner product $\\langle q, x_i \\rangle$ is as similar to the original distance of $\\angle q, \\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.  \n",
      "ScaNN  \n",
      "anisotropic vector quantization  \n",
      "Comparison of MIPS algorithms, measured in recall@10. (Image source: )  \n",
      "Google Blog, 2020  \n",
      "Check more MIPS algorithms and performance comparison in .  \n",
      "ann-benchmarks.com\n",
      "\n",
      "Component Three: Tool Use\n",
      "\n",
      "#  \n",
      "Tool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.  \n",
      "A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: )  \n",
      "Animals using tools  \n",
      "( ), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).  \n",
      "MRKL  \n",
      "Karpas et al. 2022  \n",
      "They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, , determined by the LLM capability.  \n",
      "knowing when to and how to use the tools are crucial  \n",
      "Both (Tool Augmented Language Models; ) and ( ) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the of Prompt Engineering.  \n",
      "TALM  \n",
      "Parisi et al. 2022  \n",
      "Toolformer  \n",
      "Schick et al. 2023  \n",
      "“External APIs” section  \n",
      "ChatGPT and OpenAI API are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).  \n",
      "Plugins  \n",
      "function calling  \n",
      "( ) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.  \n",
      "HuggingGPT  \n",
      "Shen et al. 2023  \n",
      "Illustration of how HuggingGPT works. (Image source: )  \n",
      "Shen et al. 2023  \n",
      "The system comprises of 4 stages:  \n",
      ": LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.  \n",
      "(1) Task planning  \n",
      "Instruction:  \n",
      "The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"  \n",
      "-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.  \n",
      ": LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.  \n",
      "(2) Model selection  \n",
      "Instruction:  \n",
      "Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.  \n",
      ": Expert models execute on the specific tasks and log results.  \n",
      "(3) Task execution  \n",
      "Instruction:  \n",
      "With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.  \n",
      ": LLM receives the execution results and provides summarized results to users.  \n",
      "(4) Response generation  \n",
      "To put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.  \n",
      "( ) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.  \n",
      "API-Bank  \n",
      "Li et al. 2023  \n",
      "Pseudo code of how LLM makes an API call in API-Bank. (Image source: )  \n",
      "Li et al. 2023  \n",
      "In the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:  \n",
      "Whether an API call is needed.  \n",
      "Identify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).  \n",
      "Response based on the API results: the model can choose to refine and call again if results are not satisfied.  \n",
      "This benchmark evaluates the agent’s tool use capabilities at three levels:  \n",
      "Level-1 evaluates the ability to . Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.  \n",
      "call the API  \n",
      "Level-2 examines the ability to . The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.  \n",
      "retrieve the API  \n",
      "Level-3 assesses the ability to . Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.  \n",
      "plan API beyond retrieve and call\n",
      "\n",
      "Case Studies\n",
      "\n",
      "#\n",
      "\n",
      "Scientific Discovery Agent\n",
      "\n",
      "#  \n",
      "( ) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in , reflects what was previously described in the and and combines CoT reasoning with tools relevant to the tasks:  \n",
      "ChemCrow  \n",
      "Bran et al. 2023  \n",
      "LangChain  \n",
      "ReAct  \n",
      "MRKLs  \n",
      "The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.  \n",
      "It is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - .  \n",
      "Thought, Action, Action Input, Observation  \n",
      "One interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.  \n",
      "also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.  \n",
      "Boiko et al. (2023)  \n",
      "For example, when requested to , the model came up with the following reasoning steps:  \n",
      "\"develop a novel anticancer drug\"  \n",
      "inquired about current trends in anticancer drug discovery;  \n",
      "selected a target;  \n",
      "requested a scaffold targeting these compounds;  \n",
      "Once the compound was identified, the model attempted its synthesis.  \n",
      "They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\n",
      "\n",
      "Generative Agents Simulation\n",
      "\n",
      "#  \n",
      "( ) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.  \n",
      "Generative Agents  \n",
      "Park, et al. 2023  \n",
      "The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.  \n",
      "stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.  \n",
      "Memory  \n",
      "Each element is an , an event directly provided by the agent.\n",
      "- Inter-agent communication can trigger new natural language statements.  \n",
      "observation  \n",
      "model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.  \n",
      "Retrieval  \n",
      "Recency: recent events have higher scores  \n",
      "Importance: distinguish mundane from core memories. Ask LM directly.  \n",
      "Relevance: based on how related it is to the current situation / query.  \n",
      "mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are (<- note that this is a bit different from above)  \n",
      "Reflection  \n",
      "higher-level summaries of past events  \n",
      "self-reflection  \n",
      "Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.  \n",
      ": translate the reflections and the environment information into actions  \n",
      "Planning & Reacting  \n",
      "Planning is essentially in order to optimize believability at the moment vs in time.  \n",
      "Prompt template:  \n",
      "{Intro of an agent X}. Here is X's plan today in broad strokes: 1)  \n",
      "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.  \n",
      "Environment information is present in a tree structure.  \n",
      "The generative agent architecture. (Image source: )  \n",
      "Park et al. 2023  \n",
      "This fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\n",
      "\n",
      "Proof-of-Concept Examples\n",
      "\n",
      "#  \n",
      "has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.  \n",
      "AutoGPT  \n",
      "Here is the system message used by AutoGPT, where are user inputs:  \n",
      "{{...}}  \n",
      "You are {{ai-name}}, {{user-provided AI bot description}}.\n",
      "Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
      "\n",
      "GOALS:\n",
      "\n",
      "1. {{user-provided goal 1}}\n",
      "2. {{user-provided goal 2}}\n",
      "3. ...\n",
      "4. ...\n",
      "5. ...\n",
      "\n",
      "Constraints:\n",
      "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
      "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
      "3. No user assistance\n",
      "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
      "5. Use subprocesses for commands that will not terminate within a few minutes\n",
      "\n",
      "Commands:\n",
      "1. Google Search: \"google\", args: \"input\": \"<search>\"\n",
      "2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\n",
      "3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\n",
      "4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\n",
      "5. List GPT Agents: \"list_agents\", args:\n",
      "6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\n",
      "7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\n",
      "8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n",
      "9. Read file: \"read_file\", args: \"file\": \"<file>\"\n",
      "10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n",
      "11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\n",
      "12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\n",
      "13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\n",
      "14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\n",
      "15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\n",
      "16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\n",
      "17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\n",
      "18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\n",
      "19. Do Nothing: \"do_nothing\", args:\n",
      "20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\n",
      "\n",
      "Resources:\n",
      "1. Internet access for searches and information gathering.\n",
      "2. Long Term memory management.\n",
      "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
      "4. File output.\n",
      "\n",
      "Performance Evaluation:\n",
      "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
      "2. Constructively self-criticize your big-picture behavior constantly.\n",
      "3. Reflect on past decisions and strategies to refine your approach.\n",
      "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
      "\n",
      "You should only respond in JSON format as described below\n",
      "Response Format:\n",
      "{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"thought\",\n",
      "        \"reasoning\": \"reasoning\",\n",
      "        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n",
      "        \"criticism\": \"constructive self-criticism\",\n",
      "        \"speak\": \"thoughts summary to say to user\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"command name\",\n",
      "        \"args\": {\n",
      "            \"arg name\": \"value\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Ensure the response can be parsed by Python json.loads  \n",
      "is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.  \n",
      "GPT-Engineer  \n",
      "Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in .  \n",
      "{{user input text}}  \n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\nThen you will pick one clarifying question, and wait for an answer from the user.\\n\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\n\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Summary of areas that need clarification:\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\n2. Details about the MVC components (e.g. which components are in each file)\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\n\\nClarifying question:\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\n\\nIs anything else unclear? If yes, only answer in the form:\\n{remaining unclear areas} remaining questions.\\n{Next question}\\nIf everything is sufficiently clear, only answer \\\"Nothing more to clarify.\\\".\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Remaining unclear areas: 2 remaining questions.\\nCan you provide more information about how the MVC components are split into separate files?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\n",
      "  }\n",
      "]  \n",
      "Then after these clarification, the agent moved into the code writing mode with a different system message.\n",
      "System message:  \n",
      "You will get instructions for code to write.\n",
      "You will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\n",
      "Make sure that every detail of the architecture is, in the end, implemented as code.  \n",
      "Think step by step and reason yourself to the right decisions to make sure we get it right.\n",
      "You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.  \n",
      "Then you will output the content of each file including ALL code.\n",
      "Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that\n",
      "FILENAME is the lowercase file name including the file extension,\n",
      "LANG is the markup code block language for the code’s language, and CODE is the code:  \n",
      "FILENAME  \n",
      "CODE  \n",
      "You will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\n",
      "Please note that the code should be fully functional. No placeholders.  \n",
      "Follow a language and framework appropriate best practice file naming convention.\n",
      "Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\n",
      "Ensure to implement all code, if you are unsure, write a plausible implementation.\n",
      "Include module dependency or package manager dependency definition file.\n",
      "Before you finish, double check that all parts of the architecture is present in the files.  \n",
      "Useful to know:\n",
      "You almost always put different classes in different files.\n",
      "For Python, you always create an appropriate requirements.txt file.\n",
      "For NodeJS, you always create an appropriate package.json file.\n",
      "You always add a comment briefly describing the purpose of the function definition.\n",
      "You try to add comments explaining very complex bits of logic.\n",
      "You always follow the best practices for the requested languages in terms of describing the code written as a defined\n",
      "package/project.  \n",
      "Python toolbelt preferences:  \n",
      "pytest  \n",
      "dataclasses  \n",
      "Conversatin samples:  \n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\n\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\n\\n\\nPython toolbelt preferences:\\n- pytest\\n- dataclasses\\n\"\n",
      "  },\n",
      " #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Assumptions:\\n1. Model: The model will contain the game's data, such as level information, character states, and enemy positions.\\n2. View: The view will handle the game's visuals, including rendering the game objects, backgrounds, and updating the display.\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\n\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Please now remember the steps:\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nPlease note that the code should be fully functional. No placeholders.\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Challenges\n",
      "\n",
      "#  \n",
      "After going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:  \n",
      ": The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.  \n",
      "Finite context length  \n",
      ": Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.  \n",
      "Challenges in long-term planning and task decomposition  \n",
      ": Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.  \n",
      "Reliability of natural language interface\n",
      "\n",
      "Citation\n",
      "\n",
      "#  \n",
      "Cited as:  \n",
      "Weng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.  \n",
      "Or  \n",
      "@article{weng2023agent,\n",
      "  title   = \"LLM-powered Autonomous Agents\",\n",
      "  author  = \"Weng, Lilian\",\n",
      "  journal = \"lilianweng.github.io\",\n",
      "  year    = \"2023\",\n",
      "  month   = \"Jun\",\n",
      "  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
      "}\n",
      "\n",
      "References\n",
      "\n",
      "#  \n",
      "[1] Wei et al. NeurIPS 2022  \n",
      "“Chain of thought prompting elicits reasoning in large language models.”  \n",
      "[2] Yao et al. arXiv preprint arXiv:2305.10601 (2023).  \n",
      "“Tree of Thoughts: Dliberate Problem Solving with Large Language Models.”  \n",
      "[3] Liu et al. arXiv preprint arXiv:2302.02676 (2023).  \n",
      "“Chain of Hindsight Aligns Language Models with Feedback\n",
      "“  \n",
      "[4] Liu et al. arXiv preprint arXiv:2304.11477 (2023).  \n",
      "“LLM+P: Empowering Large Language Models with Optimal Planning Proficiency”  \n",
      "[5] Yao et al. ICLR 2023.  \n",
      "“ReAct: Synergizing reasoning and acting in language models.”  \n",
      "[6] Google Blog. July 28, 2020.  \n",
      "“Announcing ScaNN: Efficient Vector Similarity Search”  \n",
      "[7]  \n",
      "https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389  \n",
      "[8] Shinn & Labash. arXiv preprint arXiv:2303.11366 (2023).  \n",
      "“Reflexion: an autonomous agent with dynamic memory and self-reflection”  \n",
      "[9] Laskin et al. ICLR 2023.  \n",
      "“In-context Reinforcement Learning with Algorithm Distillation”  \n",
      "[10] Karpas et al. arXiv preprint arXiv:2205.00445 (2022).  \n",
      "“MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.”  \n",
      "[11] Nakano et al. arXiv preprint arXiv:2112.09332 (2021).  \n",
      "“Webgpt: Browser-assisted question-answering with human feedback.”  \n",
      "[12] Parisi et al.  \n",
      "“TALM: Tool Augmented Language Models”  \n",
      "[13] Schick et al. arXiv preprint arXiv:2302.04761 (2023).  \n",
      "“Toolformer: Language Models Can Teach Themselves to Use Tools.”  \n",
      "[14] Weaviate Blog. Sep 13, 2022.  \n",
      "Why is Vector Search so fast?  \n",
      "[15] Li et al. arXiv preprint arXiv:2304.08244 (2023).  \n",
      "“API-Bank: A Benchmark for Tool-Augmented LLMs”  \n",
      "[16] Shen et al. arXiv preprint arXiv:2303.17580 (2023).  \n",
      "“HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace”  \n",
      "[17] Bran et al. arXiv preprint arXiv:2304.05376 (2023).  \n",
      "“ChemCrow: Augmenting large-language models with chemistry tools.”  \n",
      "[18] Boiko et al. arXiv preprint arXiv:2304.05332 (2023).  \n",
      "“Emergent autonomous scientific research capabilities of large language models.”  \n",
      "[19] Joon Sung Park, et al. arXiv preprint arXiv:2304.03442 (2023).  \n",
      "“Generative Agents: Interactive Simulacra of Human Behavior.”  \n",
      "[20] AutoGPT.  \n",
      "https://github.com/Significant-Gravitas/Auto-GPT  \n",
      "[21] GPT-Engineer.  \n",
      "https://github.com/AntonOsika/gpt-engineer  \n",
      "Nlp  \n",
      "Language-Model  \n",
      "Agent  \n",
      "Steerability  \n",
      "Prompting  \n",
      "«  \n",
      "Adversarial Attacks on LLMs  \n",
      "»  \n",
      "Prompt Engineering  \n",
      "© 2025  \n",
      "Lil'Log  \n",
      "Powered by &  \n",
      "Hugo  \n",
      "PaperMod  \n",
      "let menu = document.getElementById('menu')\n",
      "    if (menu) {\n",
      "        menu.scrollLeft = localStorage.getItem(\"menu-scroll-position\");\n",
      "        menu.onscroll = function () {\n",
      "            localStorage.setItem(\"menu-scroll-position\", menu.scrollLeft);\n",
      "        }\n",
      "    }\n",
      "\n",
      "    document.querySelectorAll('a[href^=\"#\"]').forEach(anchor => {\n",
      "        anchor.addEventListener(\"click\", function (e) {\n",
      "            e.preventDefault();\n",
      "            var id = this.getAttribute(\"href\").substr(1);\n",
      "            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {\n",
      "                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({\n",
      "                    behavior: \"smooth\"\n",
      "                });\n",
      "            } else {\n",
      "                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();\n",
      "            }\n",
      "            if (id === \"top\") {\n",
      "                history.replaceState(null, null, \" \");\n",
      "            } else {\n",
      "                history.pushState(null, null, `#${id}`);\n",
      "            }\n",
      "        });\n",
      "    });  \n",
      "var mybutton = document.getElementById(\"top-link\");\n",
      "    window.onscroll = function () {\n",
      "        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {\n",
      "            mybutton.style.visibility = \"visible\";\n",
      "            mybutton.style.opacity = \"1\";\n",
      "        } else {\n",
      "            mybutton.style.visibility = \"hidden\";\n",
      "            mybutton.style.opacity = \"0\";\n",
      "        }\n",
      "    };  \n",
      "document.getElementById(\"theme-toggle\").addEventListener(\"click\", () => {\n",
      "        if (document.body.className.includes(\"dark\")) {\n",
      "            document.body.classList.remove('dark');\n",
      "            localStorage.setItem(\"pref-theme\", 'light');\n",
      "        } else {\n",
      "            document.body.classList.add('dark');\n",
      "            localStorage.setItem(\"pref-theme\", 'dark');\n",
      "        }\n",
      "    })  \n",
      "document.querySelectorAll('pre > code').forEach((codeblock) => {\n",
      "        const container = codeblock.parentNode.parentNode;\n",
      "\n",
      "        const copybutton = document.createElement('button');\n",
      "        copybutton.classList.add('copy-code');\n",
      "        copybutton.innerText = 'copy';\n",
      "\n",
      "        function copyingDone() {\n",
      "            copybutton.innerText = 'copied!';\n",
      "            setTimeout(() => {\n",
      "                copybutton.innerText = 'copy';\n",
      "            }, 2000);\n",
      "        }\n",
      "\n",
      "        copybutton.addEventListener('click', (cb) => {\n",
      "            if ('clipboard' in navigator) {\n",
      "                navigator.clipboard.writeText(codeblock.textContent);\n",
      "                copyingDone();\n",
      "                return;\n",
      "            }\n",
      "\n",
      "            const range = document.createRange();\n",
      "            range.selectNodeContents(codeblock);\n",
      "            const selection = window.getSelection();\n",
      "            selection.removeAllRanges();\n",
      "            selection.addRange(range);\n",
      "            try {\n",
      "                document.execCommand('copy');\n",
      "                copyingDone();\n",
      "            } catch (e) { };\n",
      "            selection.removeRange(range);\n",
      "        });\n",
      "\n",
      "        if (container.classList.contains(\"highlight\")) {\n",
      "            container.appendChild(copybutton);\n",
      "        } else if (container.parentNode.firstChild == container) {\n",
      "            \n",
      "        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == \"TABLE\") {\n",
      "            \n",
      "            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);\n",
      "        } else {\n",
      "            \n",
      "            codeblock.parentNode.appendChild(copybutton);\n",
      "        }\n",
      "    });\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in html_header_url_documents:\n",
    "    print(item.page_content, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74daa400-aa6a-4fa6-99bb-a864c40e1b66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Splitting JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "597bf440-67d2-4229-ba93-a4af01c610bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5b99eded-611b-4d4e-b664-09d46a37e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url=\"https://api.smith.langchain.com/openapi.json\").json()\n",
    "# print(response)\n",
    "# huge response -> splitting required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "634785c1-b1cc-4bdb-9fda-af198e90e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2f71fa89-c2df-4eb0-8175-718b0853578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_splitter = RecursiveJsonSplitter(max_chunk_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "e9f7d69f-638f-47fd-a3d6-cc317d0ebcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1865\n"
     ]
    }
   ],
   "source": [
    "json_documents = json_splitter.split_json(response)\n",
    "print(len(json_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "16fd6de4-02d5-4f39-b357-e3e99d154d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'openapi': '3.1.0', 'info': {'title': 'LangSmith', 'description': 'The LangSmith API is used to programmatically create and manage LangSmith resources.\\n\\n## Host\\nhttps://api.smith.langchain.com\\n\\n## Authentication\\nTo authenticate with the LangSmith API, set the `X-Api-Key` header\\nto a valid [LangSmith API key](https://docs.langchain.com/langsmith/create-account-api-key#create-an-api-key).\\n\\n', 'version': '0.1.0'}}\n",
      "\n",
      "{'paths': {'/api/v1/audit-logs': {'get': {'tags': ['audit-logs'], 'summary': 'Get Audit Logs', 'description': \"Retrieve audit log records for the authenticated user's organization in OCSF format.\\n\\nRequires both start_time and end_time parameters to filter logs within a date range.\\nSupports cursor-based pagination.\\n\\nReturns results in OCSF API Activity (Class UID: 6003) format,\\nwhich is compatible with security monitoring and SIEM tools.\\nReference: https://schema.ocsf.io/1.7.0/classes/api_activity\"}}}}\n",
      "\n",
      "{'paths': {'/api/v1/audit-logs': {'get': {'operationId': 'get_audit_logs_api_v1_audit_logs_get', 'security': [{'API Key': []}, {'Organization ID': []}, {'Bearer Auth': []}], 'parameters': [{'name': 'limit', 'in': 'query', 'required': False, 'schema': {'type': 'integer', 'maximum': 100, 'minimum': 1, 'description': 'Number of items to return', 'default': 10, 'title': 'Limit'}, 'description': 'Number of items to return'}, {'name': 'cursor', 'in': 'query', 'required': False, 'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'description': 'Cursor for pagination (base64-encoded JSON string)', 'title': 'Cursor'}, 'description': 'Cursor for pagination (base64-encoded JSON string)'}, {'name': 'workspace_id', 'in': 'query', 'required': False, 'schema': {'anyOf': [{'type': 'string', 'format': 'uuid'}, {'type': 'null'}], 'description': 'Filter by workspace ID', 'title': 'Workspace Id'}, 'description': 'Filter by workspace ID'}, {'name': 'start_time', 'in': 'query', 'required': True, 'schema': {'type': 'string', 'format': 'date-time', 'description': 'Start datetime (inclusive) in ISO 8601 format', 'title': 'Start Time'}, 'description': 'Start datetime (inclusive) in ISO 8601 format'}, {'name': 'end_time', 'in': 'query', 'required': True, 'schema': {'type': 'string', 'format': 'date-time', 'description': 'End datetime (inclusive) in ISO 8601 format', 'title': 'End Time'}, 'description': 'End datetime (inclusive) in ISO 8601 format'}]}}}}\n",
      "\n",
      "{'paths': {'/api/v1/audit-logs': {'get': {'responses': {'200': {'description': 'Successful Response', 'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ListAuditLogsOCSFResponse'}}}}, '422': {'description': 'Validation Error', 'content': {'application/json': {'schema': {'$ref': '#/components/schemas/HTTPValidationError'}}}}}}}}}\n",
      "\n",
      "{'paths': {'/api/v1/sessions/{session_id}/dashboard': {'post': {'tags': ['tracer-sessions'], 'summary': 'Get Tracing Project Prebuilt Dashboard', 'description': 'Get a prebuilt dashboard for a tracing project.', 'operationId': 'get_tracing_project_prebuilt_dashboard_api_v1_sessions__session_id__dashboard_post', 'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}}}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in json_documents[0:5]:\n",
    "    print(chunk, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "f79137ee-6c8c-4b29-a616-47090d0eb70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_documents = json_splitter.create_documents(texts=[response])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "3c3e0df4-2399-4293-b356-18f170739214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"description\": \"The LangSmith API is used to programmatically create and manage LangSmith resources.\\n\\n## Host\\nhttps://api.smith.langchain.com\\n\\n## Authentication\\nTo authenticate with the LangSmith API, set the `X-Api-Key` header\\nto a valid [LangSmith API key](https://docs.langchain.com/langsmith/create-account-api-key#create-an-api-key).\\n\\n\", \"version\": \"0.1.0\"}}\n",
      "\n",
      "{\"paths\": {\"/api/v1/audit-logs\": {\"get\": {\"tags\": [\"audit-logs\"], \"summary\": \"Get Audit Logs\", \"description\": \"Retrieve audit log records for the authenticated user's organization in OCSF format.\\n\\nRequires both start_time and end_time parameters to filter logs within a date range.\\nSupports cursor-based pagination.\\n\\nReturns results in OCSF API Activity (Class UID: 6003) format,\\nwhich is compatible with security monitoring and SIEM tools.\\nReference: https://schema.ocsf.io/1.7.0/classes/api_activity\"}}}}\n",
      "\n",
      "{\"paths\": {\"/api/v1/audit-logs\": {\"get\": {\"operationId\": \"get_audit_logs_api_v1_audit_logs_get\", \"security\": [{\"API Key\": []}, {\"Organization ID\": []}, {\"Bearer Auth\": []}], \"parameters\": [{\"name\": \"limit\", \"in\": \"query\", \"required\": false, \"schema\": {\"type\": \"integer\", \"maximum\": 100, \"minimum\": 1, \"description\": \"Number of items to return\", \"default\": 10, \"title\": \"Limit\"}, \"description\": \"Number of items to return\"}, {\"name\": \"cursor\", \"in\": \"query\", \"required\": false, \"schema\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"description\": \"Cursor for pagination (base64-encoded JSON string)\", \"title\": \"Cursor\"}, \"description\": \"Cursor for pagination (base64-encoded JSON string)\"}, {\"name\": \"workspace_id\", \"in\": \"query\", \"required\": false, \"schema\": {\"anyOf\": [{\"type\": \"string\", \"format\": \"uuid\"}, {\"type\": \"null\"}], \"description\": \"Filter by workspace ID\", \"title\": \"Workspace Id\"}, \"description\": \"Filter by workspace ID\"}, {\"name\": \"start_time\", \"in\": \"query\", \"required\": true, \"schema\": {\"type\": \"string\", \"format\": \"date-time\", \"description\": \"Start datetime (inclusive) in ISO 8601 format\", \"title\": \"Start Time\"}, \"description\": \"Start datetime (inclusive) in ISO 8601 format\"}, {\"name\": \"end_time\", \"in\": \"query\", \"required\": true, \"schema\": {\"type\": \"string\", \"format\": \"date-time\", \"description\": \"End datetime (inclusive) in ISO 8601 format\", \"title\": \"End Time\"}, \"description\": \"End datetime (inclusive) in ISO 8601 format\"}]}}}}\n",
      "\n",
      "{\"paths\": {\"/api/v1/audit-logs\": {\"get\": {\"responses\": {\"200\": {\"description\": \"Successful Response\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/ListAuditLogsOCSFResponse\"}}}}, \"422\": {\"description\": \"Validation Error\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/HTTPValidationError\"}}}}}}}}}\n",
      "\n",
      "{\"paths\": {\"/api/v1/sessions/{session_id}/dashboard\": {\"post\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Get Tracing Project Prebuilt Dashboard\", \"description\": \"Get a prebuilt dashboard for a tracing project.\", \"operationId\": \"get_tracing_project_prebuilt_dashboard_api_v1_sessions__session_id__dashboard_post\", \"security\": [{\"API Key\": []}, {\"Tenant ID\": []}, {\"Bearer Auth\": []}]}}}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in json_documents[:5]:\n",
    "    print(item.page_content, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56633a1-b916-45e1-8c4c-e6939421e3dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Documents to Vectors / Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a35446ff-9091-40a1-9806-862ca1140f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "# !pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "edada0a2-1911-454d-a04b-a444ba20594c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "991f59c8-97b0-4d31-85ef-b3baa85ac25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9bcc9506-d76d-4848-9d20-a71d92aa1a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x11ce5c310>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x122b55700>, model='text-embedding-3-large', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "# has 3072 output dimensions\n",
    "em_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fd4c5f7c-185e-4110-9d3e-3b87f2604362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"This is my first actual interaction with openAI embeddings\"\n",
    "# query_result = em_model.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "eaf3d3d1-0566-4b5c-ab1b-e8c6c4e8218a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01431331131607294,\n",
       " 0.022795815020799637,\n",
       " -0.012167046777904034,\n",
       " -0.05253585800528526,\n",
       " 0.009808354079723358,\n",
       " -0.023484379053115845,\n",
       " -1.6553085515624844e-05,\n",
       " 0.04368709772825241,\n",
       " 0.032904502004384995,\n",
       " 0.011009675450623035,\n",
       " 0.003094868967309594,\n",
       " -0.012489352375268936,\n",
       " -0.03070696070790291,\n",
       " 0.014547714963555336,\n",
       " -0.00912711676210165,\n",
       " 0.038999009877443314,\n",
       " 0.02506660670042038,\n",
       " 0.05801505967974663,\n",
       " -0.03214268758893013,\n",
       " -0.02697114273905754,\n",
       " 0.024290142580866814,\n",
       " -0.01194729283452034,\n",
       " -0.045620933175086975,\n",
       " -0.017682872712612152,\n",
       " 0.02824571542441845,\n",
       " -0.019426254555583,\n",
       " 0.012731081806123257,\n",
       " 0.030912064015865326,\n",
       " -0.021330788731575012,\n",
       " 0.01860583946108818,\n",
       " -0.00824077520519495,\n",
       " -0.019733909517526627,\n",
       " 0.015529283322393894,\n",
       " -0.006317927967756987,\n",
       " 0.022502809762954712,\n",
       " -0.009258968755602837,\n",
       " 0.03598105534911156,\n",
       " 0.010702020488679409,\n",
       " -0.0016856964211910963,\n",
       " 0.0194116048514843,\n",
       " 0.009757078252732754,\n",
       " 0.030179550871253014,\n",
       " -0.01977786049246788,\n",
       " 0.009559299796819687,\n",
       " -0.049840208142995834,\n",
       " -0.018268883228302002,\n",
       " 0.0009614237933419645,\n",
       " -0.03190828114748001,\n",
       " 0.016305746510624886,\n",
       " -0.004351129289716482,\n",
       " 0.015602534636855125,\n",
       " -0.003047255566343665,\n",
       " 0.021594494581222534,\n",
       " -0.009727776981890202,\n",
       " -0.019719259813427925,\n",
       " 0.013610098510980606,\n",
       " 0.01612994447350502,\n",
       " -0.07512657344341278,\n",
       " 0.06580900400876999,\n",
       " 0.012130421586334705,\n",
       " -0.0014961585402488708,\n",
       " 0.006808711681514978,\n",
       " -0.008357977494597435,\n",
       " -0.042075566947460175,\n",
       " 7.342301978496835e-05,\n",
       " -5.525325468624942e-05,\n",
       " -0.005501175299286842,\n",
       " 0.007903818972408772,\n",
       " -0.0097131272777915,\n",
       " 0.0235576294362545,\n",
       " -0.008965963497757912,\n",
       " -0.016437599435448647,\n",
       " -0.019470205530524254,\n",
       " 0.02073012851178646,\n",
       " 0.04348199442028999,\n",
       " -0.00889271218329668,\n",
       " 0.04811147600412369,\n",
       " 0.029168682172894478,\n",
       " -0.03773908689618111,\n",
       " 0.0013551496667787433,\n",
       " 0.023044871166348457,\n",
       " -0.015617185272276402,\n",
       " 0.021623793989419937,\n",
       " 0.0023312238045036793,\n",
       " 0.0506606251001358,\n",
       " 0.025286361575126648,\n",
       " -0.0003275708295404911,\n",
       " -0.06100371107459068,\n",
       " -0.017594970762729645,\n",
       " 0.029754692688584328,\n",
       " -0.04732036218047142,\n",
       " 0.02037852257490158,\n",
       " -0.031498074531555176,\n",
       " -0.007940445095300674,\n",
       " -0.027630403637886047,\n",
       " -0.007911144755780697,\n",
       " 0.02259071171283722,\n",
       " 0.02001226507127285,\n",
       " -0.0032322152983397245,\n",
       " 0.04263227805495262,\n",
       " -0.0012699950020760298,\n",
       " -0.021037783473730087,\n",
       " 0.0013276804238557816,\n",
       " -0.041958365589380264,\n",
       " -0.01976321078836918,\n",
       " 0.007237231824547052,\n",
       " 0.003732155542820692,\n",
       " 0.02673673816025257,\n",
       " -0.020861981436610222,\n",
       " 0.019484855234622955,\n",
       " -0.018210282549262047,\n",
       " -0.0077353413216769695,\n",
       " -0.0365084670484066,\n",
       " 0.01636434905230999,\n",
       " 0.0008721487247385085,\n",
       " 0.017741473391652107,\n",
       " -0.012914210557937622,\n",
       " 0.008218799717724323,\n",
       " 0.009398146532475948,\n",
       " -0.035014137625694275,\n",
       " 0.000516879721544683,\n",
       " -0.008401928469538689,\n",
       " 0.0039189462549984455,\n",
       " -0.028406867757439613,\n",
       " 0.03252359479665756,\n",
       " 0.02990119531750679,\n",
       " 0.0016682991990819573,\n",
       " 0.005955333821475506,\n",
       " 0.012247622944414616,\n",
       " 0.0014677736908197403,\n",
       " 0.016291096806526184,\n",
       " -0.00550850061699748,\n",
       " -0.049723006784915924,\n",
       " 0.022986268624663353,\n",
       " 0.0400538295507431,\n",
       " 0.004772324580699205,\n",
       " -0.009735102765262127,\n",
       " 0.03967292234301567,\n",
       " 0.03205478563904762,\n",
       " 0.0458846352994442,\n",
       " -0.02434874325990677,\n",
       " -0.0073800720274448395,\n",
       " -0.031351570039987564,\n",
       " 0.01118547935038805,\n",
       " -0.02484685368835926,\n",
       " -2.0201345250825398e-05,\n",
       " -0.04623624309897423,\n",
       " 0.02965214103460312,\n",
       " -0.007500936742872,\n",
       " -0.02011481672525406,\n",
       " -0.03718237951397896,\n",
       " 0.0008359808707609773,\n",
       " -0.004937140271067619,\n",
       " 0.0003921235620509833,\n",
       " 0.014042281545698643,\n",
       " -0.02437804453074932,\n",
       " 0.010409015230834484,\n",
       " 0.007471636403352022,\n",
       " 0.006131136789917946,\n",
       " -0.029710741713643074,\n",
       " 0.04570883512496948,\n",
       " -0.0009284606785513461,\n",
       " 0.015089775435626507,\n",
       " 0.0019832798279821873,\n",
       " 0.022927667945623398,\n",
       " 0.040083132684230804,\n",
       " -0.007328796200454235,\n",
       " -0.035248544067144394,\n",
       " -0.03958502039313316,\n",
       " 0.01799052767455578,\n",
       " 0.026077475398778915,\n",
       " 0.03507274016737938,\n",
       " 0.04916629567742348,\n",
       " 0.025242410600185394,\n",
       " -0.04623624309897423,\n",
       " 0.026678137481212616,\n",
       " 0.03565875068306923,\n",
       " -0.027366699650883675,\n",
       " -0.01815168187022209,\n",
       " 0.010159960016608238,\n",
       " 0.006076198536902666,\n",
       " 0.028040612116456032,\n",
       " 0.04063984006643295,\n",
       " 0.005420598667114973,\n",
       " -0.006046897731721401,\n",
       " 0.034076523035764694,\n",
       " 0.04075704514980316,\n",
       " -0.0029538602102547884,\n",
       " 0.01999761536717415,\n",
       " -0.02178494818508625,\n",
       " 0.044800516217947006,\n",
       " -0.01465026754885912,\n",
       " 0.00045850759488530457,\n",
       " 0.011822765693068504,\n",
       " 0.03237709030508995,\n",
       " 0.011991243809461594,\n",
       " -0.032435692846775055,\n",
       " 0.03269939497113228,\n",
       " 0.025608666241168976,\n",
       " -0.01718476414680481,\n",
       " 0.007321470882743597,\n",
       " -0.041255153715610504,\n",
       " 0.012577254325151443,\n",
       " -0.012818983756005764,\n",
       " -0.0068636504001915455,\n",
       " 0.020905930548906326,\n",
       " -0.0025912660639733076,\n",
       " -0.0006702497485093772,\n",
       " -0.008284726180136204,\n",
       " -0.0065962825901806355,\n",
       " 0.027249496430158615,\n",
       " -0.03911621496081352,\n",
       " -0.019382303580641747,\n",
       " -0.045855335891246796,\n",
       " 0.0038676706608384848,\n",
       " -0.001848680549301207,\n",
       " -0.03545364737510681,\n",
       " 0.021228237077593803,\n",
       " -0.012818983756005764,\n",
       " 0.00694788945838809,\n",
       " -0.004255902487784624,\n",
       " 0.06153112277388573,\n",
       " 0.03715307638049126,\n",
       " -0.03187898173928261,\n",
       " -0.04427310824394226,\n",
       " -0.02223910577595234,\n",
       " -0.020774079486727715,\n",
       " -0.03870600461959839,\n",
       " -0.02508125826716423,\n",
       " 0.0188255924731493,\n",
       " 0.037709787487983704,\n",
       " 0.008050321601331234,\n",
       " 0.057985756546258926,\n",
       " 0.01612994447350502,\n",
       " 0.00888538733124733,\n",
       " -0.03343190997838974,\n",
       " 0.035629451274871826,\n",
       " -0.009595924988389015,\n",
       " -0.009830329567193985,\n",
       " 0.020437123253941536,\n",
       " -0.02436339482665062,\n",
       " 0.037504684180021286,\n",
       " -0.043716397136449814,\n",
       " -0.0288317259401083,\n",
       " 0.0008277401211671531,\n",
       " 0.016085993498563766,\n",
       " 0.0036076284013688564,\n",
       " 0.011397907510399818,\n",
       " -0.025974923744797707,\n",
       " 0.0021682395599782467,\n",
       " 0.03492623567581177,\n",
       " 0.02625327929854393,\n",
       " 0.011903341859579086,\n",
       " 0.024773601442575455,\n",
       " -0.0176535714417696,\n",
       " 0.050103913992643356,\n",
       " 0.05156894028186798,\n",
       " 0.010570167563855648,\n",
       " 0.0006331662298180163,\n",
       " -0.0010685538873076439,\n",
       " -0.010892474092543125,\n",
       " 0.019103948026895523,\n",
       " 0.041020747274160385,\n",
       " 0.006336240563541651,\n",
       " -0.027483901008963585,\n",
       " -0.03908691182732582,\n",
       " 0.044800516217947006,\n",
       " -0.020905930548906326,\n",
       " -0.004695410840213299,\n",
       " 0.012335524894297123,\n",
       " 0.010767946019768715,\n",
       " 0.025930972769856453,\n",
       " 0.03451602905988693,\n",
       " 0.03281660005450249,\n",
       " 0.013434294611215591,\n",
       " -0.010841197334229946,\n",
       " -0.006845337338745594,\n",
       " -0.02849476970732212,\n",
       " 0.041137948632240295,\n",
       " -0.04907839372754097,\n",
       " 0.034193724393844604,\n",
       " -0.0045745461247861385,\n",
       " 0.032787296921014786,\n",
       " 0.030179550871253014,\n",
       " -0.047613367438316345,\n",
       " -0.01523627806454897,\n",
       " 0.00394824706017971,\n",
       " 0.0031150130089372396,\n",
       " -0.04169465973973274,\n",
       " -0.007801267318427563,\n",
       " -0.03475043550133705,\n",
       " 0.02143334038555622,\n",
       " -0.01136128231883049,\n",
       " 0.041255153715610504,\n",
       " -0.021360090002417564,\n",
       " 0.03814929723739624,\n",
       " 0.01813703030347824,\n",
       " -0.026399781927466393,\n",
       " -0.005112943239510059,\n",
       " -0.044800516217947006,\n",
       " -0.02118428610265255,\n",
       " 0.022663963958621025,\n",
       " -0.02566726878285408,\n",
       " -0.012460052035748959,\n",
       " -0.005882082507014275,\n",
       " 0.03486763685941696,\n",
       " 0.019353002309799194,\n",
       " 0.01448911428451538,\n",
       " -0.020510373637080193,\n",
       " 0.028436169028282166,\n",
       " 0.02037852257490158,\n",
       " -0.019821811467409134,\n",
       " -0.00800637062638998,\n",
       " -0.01342696975916624,\n",
       " -0.03050185553729534,\n",
       " -0.029989097267389297,\n",
       " 0.0194116048514843,\n",
       " -0.03612755984067917,\n",
       " 0.00668052164837718,\n",
       " -0.015602534636855125,\n",
       " 0.029461687430739403,\n",
       " -0.017521720379590988,\n",
       " -0.024788253009319305,\n",
       " 0.006032247561961412,\n",
       " -0.0058747571893036366,\n",
       " -0.0020217369310557842,\n",
       " 0.019426254555583,\n",
       " -0.029490988701581955,\n",
       " 0.007962420582771301,\n",
       " -0.03636196255683899,\n",
       " 0.019924363121390343,\n",
       " -0.032318487763404846,\n",
       " -0.029813293367624283,\n",
       " 0.017580321058630943,\n",
       " -0.0026663485914468765,\n",
       " -0.020583625882864,\n",
       " -0.013082688674330711,\n",
       " -0.007083404343575239,\n",
       " -0.003741312073543668,\n",
       " -0.010284487158060074,\n",
       " -0.007852543145418167,\n",
       " -0.025945622473955154,\n",
       " -0.0030454243533313274,\n",
       " -0.03199618309736252,\n",
       " -0.0016454081051051617,\n",
       " -0.016217846423387527,\n",
       " 0.04641204699873924,\n",
       " 0.04310108721256256,\n",
       " -0.017375217750668526,\n",
       " 0.015206977725028992,\n",
       " 0.0008886302821338177,\n",
       " 0.02048107422888279,\n",
       " -0.0010429158573970199,\n",
       " -0.038559503853321075,\n",
       " -0.0026571922935545444,\n",
       " -0.032201286405324936,\n",
       " -0.049136996269226074,\n",
       " -0.019836461171507835,\n",
       " -0.01318524032831192,\n",
       " 0.03521924093365669,\n",
       " -0.029359135776758194,\n",
       " 0.004750349093228579,\n",
       " 0.017155462875962257,\n",
       " -0.0014146664179861546,\n",
       " -0.013624748215079308,\n",
       " -0.050426218658685684,\n",
       " 0.0024832203052937984,\n",
       " -0.009669176302850246,\n",
       " 0.04412660375237465,\n",
       " 0.010409015230834484,\n",
       " -0.0007746329065412283,\n",
       " 0.015675785019993782,\n",
       " -0.03129297122359276,\n",
       " 0.0006853578379377723,\n",
       " -0.010086708702147007,\n",
       " -0.02449524588882923,\n",
       " -0.007050441112369299,\n",
       " -0.02741065062582493,\n",
       " 0.019748561084270477,\n",
       " 0.02011481672525406,\n",
       " -0.009141766466200352,\n",
       " -4.2090905481018126e-05,\n",
       " -0.030560458078980446,\n",
       " 0.0021865523885935545,\n",
       " 0.004743024241179228,\n",
       " -0.015983441844582558,\n",
       " 0.024553848430514336,\n",
       " 0.015119075775146484,\n",
       " -0.01743381842970848,\n",
       " 0.04321828857064247,\n",
       " 0.044419609010219574,\n",
       " -0.020818030461668968,\n",
       " 0.013954379595816135,\n",
       " -0.023396477103233337,\n",
       " 0.021272188052535057,\n",
       " 0.0047906371764838696,\n",
       " -0.01740451715886593,\n",
       " 0.01966065913438797,\n",
       " 0.003889645915478468,\n",
       " 0.043276891112327576,\n",
       " -0.038793906569480896,\n",
       " 0.009178392589092255,\n",
       " 0.0047796498984098434,\n",
       " 0.0011491302866488695,\n",
       " 0.005687966477125883,\n",
       " -0.011280706152319908,\n",
       " -0.0447712168097496,\n",
       " 0.01681850664317608,\n",
       " -0.00432549137622118,\n",
       " 0.02575516887009144,\n",
       " -0.01705291122198105,\n",
       " -0.0014503764687106013,\n",
       " 1.2911979183627409e-06,\n",
       " 8.235052519012243e-05,\n",
       " -0.0053436849266290665,\n",
       " -0.024934755638241768,\n",
       " -0.0288317259401083,\n",
       " 0.008152873255312443,\n",
       " -0.0017232376849278808,\n",
       " -0.011331981979310513,\n",
       " 0.01465759240090847,\n",
       " -0.019045347347855568,\n",
       " 0.0352778434753418,\n",
       " 0.035951755940914154,\n",
       " -0.05429388955235481,\n",
       " -0.0019027034286409616,\n",
       " 0.012789683416485786,\n",
       " 0.04210486635565758,\n",
       " -0.005259445868432522,\n",
       " -0.054440394043922424,\n",
       " -0.02707369439303875,\n",
       " 0.0006514790584333241,\n",
       " 0.020407821983098984,\n",
       " 0.03152737393975258,\n",
       " -0.02906613051891327,\n",
       " -0.013551496900618076,\n",
       " 0.006354553624987602,\n",
       " 0.015016524121165276,\n",
       " -0.03158597648143768,\n",
       " 0.030677659437060356,\n",
       " -0.02317672222852707,\n",
       " -0.0074679735116660595,\n",
       " 0.010870498605072498,\n",
       " -0.007845218293368816,\n",
       " -0.04342339187860489,\n",
       " 0.022151203826069832,\n",
       " -0.015280229039490223,\n",
       " -0.02908078022301197,\n",
       " 0.047847773879766464,\n",
       " -0.004761336836963892,\n",
       " -0.009434771724045277,\n",
       " 0.03545364737510681,\n",
       " -0.0883704125881195,\n",
       " -0.04028823599219322,\n",
       " -0.0009440265712328255,\n",
       " -0.04453681409358978,\n",
       " 0.02141869068145752,\n",
       " 0.01284095924347639,\n",
       " -0.036332663148641586,\n",
       " -0.03190828114748001,\n",
       " -0.015119075775146484,\n",
       " 0.005109280813485384,\n",
       " 0.05095363035798073,\n",
       " 0.015719735994935036,\n",
       " 0.007662089541554451,\n",
       " -0.04535722732543945,\n",
       " -0.026575583964586258,\n",
       " -0.033373307436704636,\n",
       " 0.015968792140483856,\n",
       " 0.041489556431770325,\n",
       " -0.011207454837858677,\n",
       " -0.01870839111506939,\n",
       " 0.025594016537070274,\n",
       " 0.03205478563904762,\n",
       " -0.004497631918638945,\n",
       " -0.0009760740795172751,\n",
       " -0.02461244910955429,\n",
       " 0.019396953284740448,\n",
       " -0.004222939722239971,\n",
       " 0.02317672222852707,\n",
       " -4.163308403803967e-06,\n",
       " 0.007343446370214224,\n",
       " 0.012203672900795937,\n",
       " -0.03507274016737938,\n",
       " -0.00977905374020338,\n",
       " 0.0020949882455170155,\n",
       " -0.0014329792466014624,\n",
       " -0.018693741410970688,\n",
       " -0.013478245586156845,\n",
       " 0.022341657429933548,\n",
       " -0.009427446871995926,\n",
       " 0.025989573448896408,\n",
       " -0.024392694234848022,\n",
       " -0.02389458566904068,\n",
       " -0.029696092009544373,\n",
       " -0.027029743418097496,\n",
       " -0.015631835907697678,\n",
       " -0.0024264506064355373,\n",
       " 0.04166536033153534,\n",
       " 0.034076523035764694,\n",
       " -0.006021259818226099,\n",
       " 0.019484855234622955,\n",
       " 0.009844979271292686,\n",
       " -0.011046301573514938,\n",
       " 0.007771966978907585,\n",
       " 0.03211338445544243,\n",
       " -0.0022506474051624537,\n",
       " -0.015631835907697678,\n",
       " -0.012489352375268936,\n",
       " 0.002538158791139722,\n",
       " 0.002752418862655759,\n",
       " -0.04368709772825241,\n",
       " -0.0011784308589994907,\n",
       " -0.0011006012791767716,\n",
       " 0.015705086290836334,\n",
       " 0.030941365286707878,\n",
       " 0.004754011519253254,\n",
       " -0.002891596406698227,\n",
       " 0.014364587143063545,\n",
       " 0.02387993596494198,\n",
       " -0.014701543375849724,\n",
       " -0.0015813132049515843,\n",
       " 0.020202718675136566,\n",
       " 0.0447126142680645,\n",
       " -0.0005081811686977744,\n",
       " 0.02084732986986637,\n",
       " -0.0033713928423821926,\n",
       " -0.01705291122198105,\n",
       " -0.024700351059436798,\n",
       " -0.03923341631889343,\n",
       " -0.01336104329675436,\n",
       " -0.015192327089607716,\n",
       " -0.01873769238591194,\n",
       " -0.015719735994935036,\n",
       " -0.0009046539780683815,\n",
       " 0.025095907971262932,\n",
       " -0.004475656431168318,\n",
       " 0.031615275889635086,\n",
       " -0.01211577095091343,\n",
       " 0.0008098850958049297,\n",
       " 0.0032120710238814354,\n",
       " 0.014042281545698643,\n",
       " 0.0037651187740266323,\n",
       " 0.0010126996785402298,\n",
       " -0.0050433543510735035,\n",
       " -0.0076401145197451115,\n",
       " 0.01670130528509617,\n",
       " -0.02144799195230007,\n",
       " -0.04058124125003815,\n",
       " 0.008094272576272488,\n",
       " 0.0144231878221035,\n",
       " 0.003439150284975767,\n",
       " 0.02601887471973896,\n",
       " -2.3019803848001175e-05,\n",
       " -0.00394824706017971,\n",
       " 0.004424380604177713,\n",
       " -0.017448468133807182,\n",
       " 0.012262273579835892,\n",
       " -0.02166774496436119,\n",
       " 0.004508619662374258,\n",
       " 0.009332220070064068,\n",
       " 0.0051788692362606525,\n",
       " -0.036654967814683914,\n",
       " -0.008563080802559853,\n",
       " -0.0005287830717861652,\n",
       " -0.047613367438316345,\n",
       " -0.0028696211520582438,\n",
       " 0.012643180787563324,\n",
       " 0.0016023729695007205,\n",
       " 0.002959354082122445,\n",
       " 0.008621682412922382,\n",
       " -0.0029227284248918295,\n",
       " -0.005123930983245373,\n",
       " 0.02789410948753357,\n",
       " -0.003741312073543668,\n",
       " 0.03308030217885971,\n",
       " 0.038911107927560806,\n",
       " -0.05622772499918938,\n",
       " -0.015324179083108902,\n",
       " 0.003316454356536269,\n",
       " -0.012452727183699608,\n",
       " -0.004369442351162434,\n",
       " -0.01601274125277996,\n",
       " -0.00014959296095184982,\n",
       " 0.032084085047245026,\n",
       " 0.0074679735116660595,\n",
       " 0.002803694922477007,\n",
       " -0.041723962873220444,\n",
       " -0.016203194856643677,\n",
       " -0.005497512873262167,\n",
       " 0.01955810748040676,\n",
       " 0.033607713878154755,\n",
       " 0.03630336374044418,\n",
       " -0.0037394806277006865,\n",
       " -0.03158597648143768,\n",
       " 0.008526455610990524,\n",
       " 0.03340261057019234,\n",
       " 0.013126639649271965,\n",
       " 0.013207215815782547,\n",
       " -0.0027707316912710667,\n",
       " 0.02695649117231369,\n",
       " -0.016335047781467438,\n",
       " 0.03800279274582863,\n",
       " 0.005860107019543648,\n",
       " -0.03492623567581177,\n",
       " -0.025799119845032692,\n",
       " 0.007669414859265089,\n",
       " 0.03023815155029297,\n",
       " -0.0005814324831590056,\n",
       " 0.005753892473876476,\n",
       " -0.030999965965747833,\n",
       " -0.016100643202662468,\n",
       " 0.03603965789079666,\n",
       " -0.012525978498160839,\n",
       " 0.006230026017874479,\n",
       " 0.007266532629728317,\n",
       " -0.019279751926660538,\n",
       " 0.015617185272276402,\n",
       " -0.022151203826069832,\n",
       " 0.003049086779356003,\n",
       " 0.0029538602102547884,\n",
       " 0.02270791493356228,\n",
       " 0.009200368076562881,\n",
       " -0.0005333612789399922,\n",
       " 0.024290142580866814,\n",
       " 0.012511327862739563,\n",
       " -0.010914448648691177,\n",
       " -0.04649994894862175,\n",
       " 0.00994753185659647,\n",
       " 0.0034025246277451515,\n",
       " 0.013141289353370667,\n",
       " 0.018459336832165718,\n",
       " -0.06164832413196564,\n",
       " -9.857455552264582e-06,\n",
       " -0.016891758888959885,\n",
       " 0.012848284095525742,\n",
       " 0.02448059618473053,\n",
       " 0.013910428620874882,\n",
       " 0.000985230435617268,\n",
       " 0.002397150034084916,\n",
       " -0.002475895220413804,\n",
       " -0.015456032007932663,\n",
       " -0.03170317783951759,\n",
       " 0.04377499967813492,\n",
       " -0.014884671196341515,\n",
       " 0.003056412097066641,\n",
       " 0.013683349825441837,\n",
       " 0.014379236847162247,\n",
       " 0.018913494423031807,\n",
       " -0.025872372090816498,\n",
       " 0.000656057265587151,\n",
       " -0.007625463884323835,\n",
       " 0.01684780791401863,\n",
       " 0.007771966978907585,\n",
       " 0.019455553963780403,\n",
       " 0.0036350975278764963,\n",
       " 0.04198766499757767,\n",
       " 0.010789921507239342,\n",
       " -0.012562603689730167,\n",
       " 0.0017360566416755319,\n",
       " 0.010965725407004356,\n",
       " 0.022063301876187325,\n",
       " 0.01029913779348135,\n",
       " -0.039379917085170746,\n",
       " 0.02213655412197113,\n",
       " -0.025257060304284096,\n",
       " -0.03129297122359276,\n",
       " -0.029051480814814568,\n",
       " 0.003365898970514536,\n",
       " 0.03428162634372711,\n",
       " -0.01418145839124918,\n",
       " -0.026780689135193825,\n",
       " -0.05751694738864899,\n",
       " -0.036801472306251526,\n",
       " 0.03586385399103165,\n",
       " 0.0004479777126107365,\n",
       " -0.014642941765487194,\n",
       " 0.0016820337623357773,\n",
       " 0.014796770177781582,\n",
       " -0.00022810924565419555,\n",
       " -0.007061428856104612,\n",
       " -0.04934209957718849,\n",
       " -0.0076401145197451115,\n",
       " -0.004164338577538729,\n",
       " 0.0494593009352684,\n",
       " -0.00641681719571352,\n",
       " 0.004801624920219183,\n",
       " 0.007574188057333231,\n",
       " -0.01637899875640869,\n",
       " 0.02635583095252514,\n",
       " 0.007522912230342627,\n",
       " -0.03551224619150162,\n",
       " 0.023118121549487114,\n",
       " 0.02046642266213894,\n",
       " 0.0064680930227041245,\n",
       " 0.003578327829018235,\n",
       " -0.015163026750087738,\n",
       " -0.005519488360732794,\n",
       " 0.008797485381364822,\n",
       " -0.037973493337631226,\n",
       " -0.03518994152545929,\n",
       " -0.012914210557937622,\n",
       " 0.01359544787555933,\n",
       " 0.026692787185311317,\n",
       " -0.0022066964302212,\n",
       " -0.012965486384928226,\n",
       " 0.01424738485366106,\n",
       " -0.0016600583912804723,\n",
       " -0.0006382022402249277,\n",
       " 0.02128683775663376,\n",
       " -0.009420122019946575,\n",
       " 0.004296191036701202,\n",
       " 0.025945622473955154,\n",
       " 0.03823719918727875,\n",
       " -0.04107934981584549,\n",
       " 0.0032340465113520622,\n",
       " -0.011024326086044312,\n",
       " 0.018503287807106972,\n",
       " 0.009625225327908993,\n",
       " -0.003248696681112051,\n",
       " -0.020583625882864,\n",
       " 0.0646955817937851,\n",
       " 0.03217198699712753,\n",
       " -0.021140335127711296,\n",
       " 0.010174610652029514,\n",
       " 0.004439030773937702,\n",
       " 0.005629365332424641,\n",
       " -0.003444644156843424,\n",
       " 0.01177881471812725,\n",
       " 0.008965963497757912,\n",
       " 0.029989097267389297,\n",
       " -0.02591632306575775,\n",
       " -0.002908078022301197,\n",
       " 0.01211577095091343,\n",
       " 0.004252240061759949,\n",
       " -0.03510203957557678,\n",
       " -0.03460393100976944,\n",
       " 0.0010447471868246794,\n",
       " 0.0056037274189293385,\n",
       " -0.030179550871253014,\n",
       " 0.038090694695711136,\n",
       " -0.020393172279000282,\n",
       " -0.019689958542585373,\n",
       " -0.03814929723739624,\n",
       " -0.00799904577434063,\n",
       " -0.015104425139725208,\n",
       " 0.00936884619295597,\n",
       " -0.010907123796641827,\n",
       " -0.014555040746927261,\n",
       " 0.011976593174040318,\n",
       " 0.05004531145095825,\n",
       " 0.0028183450922369957,\n",
       " 0.03275799751281738,\n",
       " -0.0046221595257520676,\n",
       " 0.0009275450720451772,\n",
       " -0.011163503862917423,\n",
       " 0.017375217750668526,\n",
       " -0.009808354079723358,\n",
       " -0.010723995976150036,\n",
       " 0.015602534636855125,\n",
       " 0.003580159042030573,\n",
       " 0.019733909517526627,\n",
       " -0.017697522416710854,\n",
       " 0.06575039774179459,\n",
       " 0.016525501385331154,\n",
       " 0.005962658673524857,\n",
       " 0.01967530883848667,\n",
       " -0.018195630982518196,\n",
       " -0.019689958542585373,\n",
       " -0.023821335285902023,\n",
       " 0.009149092249572277,\n",
       " 0.026311879977583885,\n",
       " 0.006149449851363897,\n",
       " 0.02046642266213894,\n",
       " -0.01218169741332531,\n",
       " -0.0030673996079713106,\n",
       " 0.010401689447462559,\n",
       " -0.010225886479020119,\n",
       " -0.02262001298367977,\n",
       " -0.00977905374020338,\n",
       " -0.026809988543391228,\n",
       " -0.012921535409986973,\n",
       " -0.020158767700195312,\n",
       " 0.019572757184505463,\n",
       " -0.0033420922700315714,\n",
       " -0.025051956996321678,\n",
       " 0.012606554664671421,\n",
       " -0.01976321078836918,\n",
       " 0.008152873255312443,\n",
       " 0.016291096806526184,\n",
       " -0.023821335285902023,\n",
       " -0.021858198568224907,\n",
       " 0.020202718675136566,\n",
       " 0.03249429166316986,\n",
       " 0.021111035719513893,\n",
       " -0.004951790440827608,\n",
       " 0.051627542823553085,\n",
       " 0.02719089575111866,\n",
       " 0.01636434905230999,\n",
       " -0.04034683480858803,\n",
       " 0.020583625882864,\n",
       " -0.006457105278968811,\n",
       " 0.02026131935417652,\n",
       " 0.04670505225658417,\n",
       " -0.04348199442028999,\n",
       " -0.00847517978399992,\n",
       " 0.0009614237933419645,\n",
       " 0.008489829488098621,\n",
       " 0.03258219361305237,\n",
       " 0.02108173444867134,\n",
       " 0.000579601211939007,\n",
       " 0.012071819975972176,\n",
       " -0.018093079328536987,\n",
       " 0.053619977086782455,\n",
       " -0.016628053039312363,\n",
       " 0.03035535290837288,\n",
       " -0.022751865908503532,\n",
       " -0.002010749187320471,\n",
       " -0.012613880448043346,\n",
       " -0.013324418105185032,\n",
       " -0.005629365332424641,\n",
       " 0.004310841206461191,\n",
       " -0.016554802656173706,\n",
       " -0.005742904730141163,\n",
       " -0.0042266021482646465,\n",
       " 0.011266055516898632,\n",
       " 0.019045347347855568,\n",
       " -0.012313549406826496,\n",
       " 0.029842594638466835,\n",
       " -0.01541208103299141,\n",
       " 0.019016046077013016,\n",
       " -0.003605796955525875,\n",
       " -0.030091648921370506,\n",
       " 0.030179550871253014,\n",
       " -0.009918230585753918,\n",
       " 0.039848726242780685,\n",
       " 0.00789649412035942,\n",
       " -0.014100882224738598,\n",
       " 0.04093284532427788,\n",
       " 0.014459813944995403,\n",
       " -0.011288031004369259,\n",
       " -0.00741303525865078,\n",
       " -0.013265817426145077,\n",
       " -0.016510851681232452,\n",
       " 0.017345916479825974,\n",
       " 0.009661851450800896,\n",
       " -0.010335763916373253,\n",
       " 0.005871094763278961,\n",
       " 0.014298660680651665,\n",
       " -0.025286361575126648,\n",
       " -0.006501056253910065,\n",
       " 0.014291335828602314,\n",
       " -0.006325252819806337,\n",
       " -0.007266532629728317,\n",
       " 0.00021162768825888634,\n",
       " 0.015089775435626507,\n",
       " -0.0016280109994113445,\n",
       " 0.00473569892346859,\n",
       " -0.0103064626455307,\n",
       " 0.010789921507239342,\n",
       " 0.03920411691069603,\n",
       " -0.030765561386942863,\n",
       " -0.023689482361078262,\n",
       " 0.0003213902236893773,\n",
       " 0.010438315570354462,\n",
       " -0.03993662819266319,\n",
       " -0.01276770792901516,\n",
       " -0.009691151790320873,\n",
       " 0.005263108294457197,\n",
       " -0.0010584817500784993,\n",
       " 0.005083642899990082,\n",
       " 0.011280706152319908,\n",
       " 0.0020253993570804596,\n",
       " 0.031498074531555176,\n",
       " 0.025491464883089066,\n",
       " 0.028436169028282166,\n",
       " 0.031732477247714996,\n",
       " 0.0016161076491698623,\n",
       " -0.03744608163833618,\n",
       " -0.035629451274871826,\n",
       " -0.01894279569387436,\n",
       " 0.03858880326151848,\n",
       " 0.023264624178409576,\n",
       " 0.03058975748717785,\n",
       " 0.02635583095252514,\n",
       " -0.023147422820329666,\n",
       " 0.008467854000627995,\n",
       " 0.020290620625019073,\n",
       " 0.01683315634727478,\n",
       " 0.0025491465348750353,\n",
       " -0.010548192076385021,\n",
       " -0.00709072919562459,\n",
       " 0.02591632306575775,\n",
       " -0.004277877975255251,\n",
       " 0.008907362818717957,\n",
       " 0.005537800956517458,\n",
       " 0.010313788428902626,\n",
       " 0.009515348821878433,\n",
       " -0.011610336601734161,\n",
       " -0.01719941385090351,\n",
       " 0.0025839409790933132,\n",
       " -0.031732477247714996,\n",
       " 0.008167523890733719,\n",
       " 0.0046551222912967205,\n",
       " 0.010526216588914394,\n",
       " -0.00840925332158804,\n",
       " -0.016759905964136124,\n",
       " -0.01118547935038805,\n",
       " -0.0037394806277006865,\n",
       " 0.012511327862739563,\n",
       " -0.005512163043022156,\n",
       " 0.00334026082418859,\n",
       " 0.02752785198390484,\n",
       " -0.0012974642449989915,\n",
       " 0.013068038038909435,\n",
       " -0.0288317259401083,\n",
       " 0.0022634663619101048,\n",
       " 0.0036900360137224197,\n",
       " 0.07952164858579636,\n",
       " 0.005219157785177231,\n",
       " 0.0034629569854587317,\n",
       " 0.028641272336244583,\n",
       " 0.02285441756248474,\n",
       " -0.008270075544714928,\n",
       " -0.019455553963780403,\n",
       " -0.016672004014253616,\n",
       " -0.027689004316926003,\n",
       " 0.00535467267036438,\n",
       " 0.023396477103233337,\n",
       " 0.007962420582771301,\n",
       " -0.022107252851128578,\n",
       " 0.029022179543972015,\n",
       " 0.021565193310379982,\n",
       " 0.002585772192105651,\n",
       " 0.005244795698672533,\n",
       " -0.003823719685897231,\n",
       " 0.0052960715256631374,\n",
       " -0.01857653819024563,\n",
       " -0.025403562933206558,\n",
       " -0.006695172283798456,\n",
       " 0.019162548705935478,\n",
       " 0.00929559487849474,\n",
       " 0.0038640080019831657,\n",
       " 0.004680760204792023,\n",
       " -0.0035215578973293304,\n",
       " -0.041958365589380264,\n",
       " 0.035043440759181976,\n",
       " -0.012460052035748959,\n",
       " -0.0030106299091130495,\n",
       " 0.0038530202582478523,\n",
       " -0.014752819202840328,\n",
       " 0.012203672900795937,\n",
       " -0.012042519636452198,\n",
       " 0.03472113236784935,\n",
       " 0.004175326321274042,\n",
       " -0.019396953284740448,\n",
       " -0.009757078252732754,\n",
       " -0.021477291360497475,\n",
       " -0.018884195014834404,\n",
       " -0.008321351371705532,\n",
       " 0.0045745461247861385,\n",
       " -0.010496916249394417,\n",
       " -0.01799052767455578,\n",
       " -0.00266085471957922,\n",
       " -0.0015373624628409743,\n",
       " 0.003609459614381194,\n",
       " 0.02647303231060505,\n",
       " -0.004285203292965889,\n",
       " 0.002673673676326871,\n",
       " -0.01848863810300827,\n",
       " -0.013947054743766785,\n",
       " -0.005329034756869078,\n",
       " -0.006662209052592516,\n",
       " -0.02130148932337761,\n",
       " 0.03603965789079666,\n",
       " 0.013229191303253174,\n",
       " -0.010584818199276924,\n",
       " 0.01873769238591194,\n",
       " -0.02143334038555622,\n",
       " 0.022869067266583443,\n",
       " 0.00800637062638998,\n",
       " -0.011493134312331676,\n",
       " 0.018312834203243256,\n",
       " 0.04389220103621483,\n",
       " -0.00783056765794754,\n",
       " 0.01094374991953373,\n",
       " 0.03378351777791977,\n",
       " -0.009573949500918388,\n",
       " -0.037358179688453674,\n",
       " -0.019499504938721657,\n",
       " -0.01717011258006096,\n",
       " -0.005112943239510059,\n",
       " -0.007771966978907585,\n",
       " -0.025022655725479126,\n",
       " -0.022634662687778473,\n",
       " 0.023235322907567024,\n",
       " -0.007691390346735716,\n",
       " 0.0024777264334261417,\n",
       " 0.005673315841704607,\n",
       " 0.0042705531232059,\n",
       " ...]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3421d179-5154-4e9c-883d-46c8c64038fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83871b59-c6d7-48a8-aa5c-4ef685a5be74",
   "metadata": {},
   "source": [
    "### Custom Output Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "561eeeed-725e-4ab3-819a-9c3b1c6e29d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x122c867c0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x122c86ac0>, model='text-embedding-3-large', dimensions=1024, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_custom_model = OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions=1024)\n",
    "em_custom_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f36fe05f-fbbd-4a62-bd7d-5f125c659392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"This is my first actual interaction with openAI embeddings\"\n",
    "# query_result_custom = em_custom_model.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6a6a6ddd-df7b-4218-8d0a-e0baaf249010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_result_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc4078f-0bda-4321-ac6f-2543062048f0",
   "metadata": {},
   "source": [
    "### Vectorizing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5e43d247-c9fa-4629-8e1a-93b1c2e68afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './Data/Text/speech.txt'}, page_content='Ladies and gentlemen, friends, and fellow builders,\\n\\nWe live in an era where change no longer knocks on the door. It breaks it down. Every year, technology moves faster, systems grow more complex, and decisions that once took decades now take months, sometimes weeks. And in the middle of all this acceleration, there is one uncomfortable truth we must face: progress without direction is not innovation—it is chaos.\\n\\nFor generations, we measured success by what we could build. Bigger machines. Faster networks. Smarter systems. But today, the real challenge is not whether we can build something. It is whether we should, and if we do, whether we understand the consequences of what we are putting into the world.\\n\\nTechnology is not neutral. Every line of code carries assumptions. Every model reflects priorities. Every system amplifies certain voices while muting others. When we say we are “just engineers” or “just researchers,” we ignore the fact that our work shapes how people live, learn, communicate, and even think.\\n\\nLook around us. Algorithms decide what news we see. Automated systems influence who gets a loan, who gets a job interview, and who is flagged as a risk. These systems are efficient, scalable, and impressive. But efficiency without accountability can quietly erode trust. And trust, once lost, is far harder to rebuild than any system we deploy.\\n\\nThe future we are building will not be defined by our tools alone. It will be defined by the values we embed into them. If we optimize only for speed, we will get speed. If we optimize only for profit, we will get profit. But if we fail to optimize for fairness, transparency, and human dignity, we should not be surprised when the systems we create feel cold, distant, and unfair.')]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ead03239-2950-4135-b8b5-b89c916bc27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './Data/Text/speech.txt'}, page_content='Ladies and gentlemen, friends, and fellow builders,\\n\\nWe live in an era where change no longer knocks on the door. It breaks it down. Every year, technology moves faster, systems grow more complex, and decisions that once took decades now take months, sometimes weeks. And in the middle of all this acceleration, there is one uncomfortable truth we must face: progress without direction is not innovation—it is chaos.'),\n",
       " Document(metadata={'source': './Data/Text/speech.txt'}, page_content='For generations, we measured success by what we could build. Bigger machines. Faster networks. Smarter systems. But today, the real challenge is not whether we can build something. It is whether we should, and if we do, whether we understand the consequences of what we are putting into the world.'),\n",
       " Document(metadata={'source': './Data/Text/speech.txt'}, page_content='Technology is not neutral. Every line of code carries assumptions. Every model reflects priorities. Every system amplifies certain voices while muting others. When we say we are “just engineers” or “just researchers,” we ignore the fact that our work shapes how people live, learn, communicate, and even think.'),\n",
       " Document(metadata={'source': './Data/Text/speech.txt'}, page_content='Look around us. Algorithms decide what news we see. Automated systems influence who gets a loan, who gets a job interview, and who is flagged as a risk. These systems are efficient, scalable, and impressive. But efficiency without accountability can quietly erode trust. And trust, once lost, is far harder to rebuild than any system we deploy.'),\n",
       " Document(metadata={'source': './Data/Text/speech.txt'}, page_content='The future we are building will not be defined by our tools alone. It will be defined by the values we embed into them. If we optimize only for speed, we will get speed. If we optimize only for profit, we will get profit. But if we fail to optimize for fairness, transparency, and human dignity, we should not be surprised when the systems we create feel cold, distant, and unfair.')]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "split_text_documents = text_splitter.split_documents(text_documents)\n",
    "split_text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbbccdf-a9af-4234-b8e9-32312386bb21",
   "metadata": {},
   "source": [
    "### Storing the vectors into Vector Store DB, example ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f5bfadde-c99d-4501-86af-e8b5aecc6c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a2181510-72c6-45d0-a1f3-93580b64926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0d029d19-9c74-43fe-9de4-6865e8c518f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x1205c8970>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = Chroma.from_documents(split_text_documents, em_custom_model)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eb945d1f-42cb-4d35-8318-36a7d68ea3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': './Data/Text/speech.txt'}, page_content='For generations, we measured success by what we could build. Bigger machines. Faster networks. Smarter systems. But today, the real challenge is not whether we can build something. It is whether we should, and if we do, whether we understand the consequences of what we are putting into the world.'), Document(metadata={'source': './Data/Text/speech.txt'}, page_content='Ladies and gentlemen, friends, and fellow builders,\\n\\nWe live in an era where change no longer knocks on the door. It breaks it down. Every year, technology moves faster, systems grow more complex, and decisions that once took decades now take months, sometimes weeks. And in the middle of all this acceleration, there is one uncomfortable truth we must face: progress without direction is not innovation—it is chaos.'), Document(metadata={'source': './Data/Text/speech.txt'}, page_content='The future we are building will not be defined by our tools alone. It will be defined by the values we embed into them. If we optimize only for speed, we will get speed. If we optimize only for profit, we will get profit. But if we fail to optimize for fairness, transparency, and human dignity, we should not be surprised when the systems we create feel cold, distant, and unfair.'), Document(metadata={'source': './Data/Text/speech.txt'}, page_content='Technology is not neutral. Every line of code carries assumptions. Every model reflects priorities. Every system amplifies certain voices while muting others. When we say we are “just engineers” or “just researchers,” we ignore the fact that our work shapes how people live, learn, communicate, and even think.')]\n"
     ]
    }
   ],
   "source": [
    "query=\"But today, the real challenge is not whether\"\n",
    "# retrieved_results = db.similarity_search(query)\n",
    "print(retrieved_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6566d14-4ae5-4992-bcdf-b1989f712e88",
   "metadata": {},
   "source": [
    "By default, similarity search ≠ “give me the best one”\n",
    "It means:\n",
    "\n",
    "“Give me the top-k most similar documents”\n",
    "\n",
    "And k defaults to > 1 (usually k=4, depending on the vector store).\n",
    "\n",
    "So you’re asking:\n",
    "\n",
    "“What are the top few chunks that kind of match this sentence?”\n",
    "\n",
    "Not:\n",
    "\n",
    "“What is the single best match?”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d80dd62-937b-4b96-a1f1-3387a22f29a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Using Open Source Models through OLLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "22265e76-c187-4d51-8ae2-5d73451ad73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dd28f85e-783a-4e58-b141-9061f11aac5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(base_url='http://localhost:11434', model='mxbai-embed-large:latest', embed_instruction='passage: ', query_instruction='query: ', mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None, show_progress=False, headers=None, model_kwargs=None)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = (\n",
    "    OllamaEmbeddings(model=\"mxbai-embed-large:latest\") # default -> llama2 Embeddings\n",
    ")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "30bc42bb-1395-4ba0-a67a-347e8e4497fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = embeddings.embed_documents([\n",
    "    \"Alpha is the first letter of greek alphabets\",\n",
    "    \"Beta is the second letter of greek aplhabets\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4f639f5b-13a1-4ba2-8799-9ae416d74f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7d454803-5b0a-4b09-91bb-8114d54020fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02232862263917923,\n",
       " -0.011172731406986713,\n",
       " 0.05565343424677849,\n",
       " 0.004261345602571964,\n",
       " -0.059006091207265854,\n",
       " -0.002101231599226594,\n",
       " 0.05839128419756889,\n",
       " 0.014102347195148468,\n",
       " -0.0006132923299446702,\n",
       " 0.04060807451605797,\n",
       " 0.003592876950278878,\n",
       " 0.0008210025262087584,\n",
       " -0.01609811745584011,\n",
       " 0.00807739794254303,\n",
       " -0.04263812676072121,\n",
       " -0.01741427183151245,\n",
       " -0.032701026648283005,\n",
       " -0.03561025857925415,\n",
       " -0.04910216107964516,\n",
       " -0.013773754239082336,\n",
       " 0.006644190289080143,\n",
       " 0.030934231355786324,\n",
       " -0.08119471371173859,\n",
       " 0.014379146508872509,\n",
       " -0.012773838825523853,\n",
       " 0.047786515206098557,\n",
       " 0.0017566366586834192,\n",
       " 0.0016951588913798332,\n",
       " 0.06454253941774368,\n",
       " 0.04427395015954971,\n",
       " 0.01942199468612671,\n",
       " 0.036683741956949234,\n",
       " 0.03538158908486366,\n",
       " -0.019382568076252937,\n",
       " -5.7232424296671525e-05,\n",
       " -0.01037557702511549,\n",
       " 0.05384887382388115,\n",
       " -0.02253211848437786,\n",
       " -0.00800285767763853,\n",
       " -0.04165192320942879,\n",
       " 0.03943972289562225,\n",
       " -0.01664954610168934,\n",
       " 0.030306879431009293,\n",
       " -0.015053304843604565,\n",
       " -0.06006656587123871,\n",
       " -0.0012055581901222467,\n",
       " -0.0008683637715876102,\n",
       " -0.05053345859050751,\n",
       " -0.04443616047501564,\n",
       " -0.03670257702469826,\n",
       " 0.012573079206049442,\n",
       " -0.021892504766583443,\n",
       " 0.04294092580676079,\n",
       " -0.0323970802128315,\n",
       " -0.023975495249032974,\n",
       " -0.013185343705117702,\n",
       " 0.007970056496560574,\n",
       " -0.021245302632451057,\n",
       " -0.05398532375693321,\n",
       " 0.041516199707984924,\n",
       " 0.016032928600907326,\n",
       " 0.024562720209360123,\n",
       " 0.011083363555371761,\n",
       " -0.03088601864874363,\n",
       " 0.0044228495098650455,\n",
       " -0.011468448676168919,\n",
       " -0.006334618665277958,\n",
       " 0.022942280396819115,\n",
       " 0.001792660914361477,\n",
       " 0.003503459971398115,\n",
       " 0.01328741293400526,\n",
       " 0.050982531160116196,\n",
       " 0.007270542439073324,\n",
       " -0.04986192286014557,\n",
       " -0.032306645065546036,\n",
       " -0.015858259052038193,\n",
       " -0.020233646035194397,\n",
       " -0.022981084883213043,\n",
       " -0.040619831532239914,\n",
       " 0.04775502532720566,\n",
       " 0.009448549710214138,\n",
       " 0.01933267153799534,\n",
       " 0.028733430430293083,\n",
       " 0.0030846456065773964,\n",
       " -0.020657405257225037,\n",
       " -0.01143870409578085,\n",
       " 0.01952275261282921,\n",
       " 0.044512975960969925,\n",
       " -0.03779982402920723,\n",
       " 0.007594638038426638,\n",
       " 0.06546337902545929,\n",
       " 0.0393085703253746,\n",
       " -0.008447641506791115,\n",
       " -0.0002000828244490549,\n",
       " 0.03218750283122063,\n",
       " 0.034685343503952026,\n",
       " -0.013360581360757351,\n",
       " -0.02101687714457512,\n",
       " 0.016243508085608482,\n",
       " 0.03244084492325783,\n",
       " 0.057659395039081573,\n",
       " 0.05593707412481308,\n",
       " -0.02303386479616165,\n",
       " 0.06288288533687592,\n",
       " -0.0642564594745636,\n",
       " 0.02364169992506504,\n",
       " 0.0021785576827824116,\n",
       " 0.005914539564400911,\n",
       " 0.007171947974711657,\n",
       " -0.0010020800400525331,\n",
       " 0.011727135628461838,\n",
       " 0.0066670626401901245,\n",
       " -0.0002448810264468193,\n",
       " -0.005982821807265282,\n",
       " -0.001491042785346508,\n",
       " 0.023953502997756004,\n",
       " -0.031094439327716827,\n",
       " 0.025863366201519966,\n",
       " -0.06735404580831528,\n",
       " -0.03087729960680008,\n",
       " -0.004328345879912376,\n",
       " 0.059391219168901443,\n",
       " 0.0028845053166151047,\n",
       " -0.007197125814855099,\n",
       " 0.02958434820175171,\n",
       " 0.007199442479759455,\n",
       " -0.02375318855047226,\n",
       " 0.010505849495530128,\n",
       " -0.07557469606399536,\n",
       " 0.02273261547088623,\n",
       " -0.013146726414561272,\n",
       " 0.00637606019154191,\n",
       " 0.00732587231323123,\n",
       " 0.015159725211560726,\n",
       " -0.004818227607756853,\n",
       " 0.0015296662459149957,\n",
       " 0.02850562147796154,\n",
       " 0.016410479322075844,\n",
       " 0.020714202895760536,\n",
       " -0.03837193548679352,\n",
       " 0.056324996054172516,\n",
       " 0.00956704281270504,\n",
       " -0.02880851738154888,\n",
       " 0.07486866414546967,\n",
       " 0.014100678265094757,\n",
       " -0.02951095439493656,\n",
       " 0.017141014337539673,\n",
       " 0.0213164109736681,\n",
       " -0.0610794797539711,\n",
       " 0.05653955042362213,\n",
       " -0.04317713528871536,\n",
       " -0.01706046052277088,\n",
       " -0.01744946278631687,\n",
       " 0.020111197605729103,\n",
       " -0.03578614443540573,\n",
       " 0.012081928551197052,\n",
       " 0.03026425652205944,\n",
       " -0.00011589725181693211,\n",
       " 0.007928639650344849,\n",
       " -0.003031749278306961,\n",
       " -0.028308574110269547,\n",
       " -0.025690753012895584,\n",
       " 0.008191507309675217,\n",
       " 0.055039480328559875,\n",
       " -0.02917037159204483,\n",
       " 0.0490177646279335,\n",
       " -0.020987913012504578,\n",
       " -0.030685557052493095,\n",
       " 0.013094644993543625,\n",
       " -0.022917235270142555,\n",
       " -0.01223769411444664,\n",
       " -4.3881056626560166e-05,\n",
       " -0.033462848514318466,\n",
       " 0.005699430126696825,\n",
       " 0.01991989277303219,\n",
       " 0.03092074953019619,\n",
       " -0.031222853809595108,\n",
       " -0.06054151803255081,\n",
       " 0.012983769178390503,\n",
       " -0.0020363887306302786,\n",
       " 0.006996891461312771,\n",
       " 0.0057385447435081005,\n",
       " 0.03475304692983627,\n",
       " -0.006137124262750149,\n",
       " 0.014267979189753532,\n",
       " 0.014483899809420109,\n",
       " 0.0012864891905337572,\n",
       " -0.010219386778771877,\n",
       " -0.03337656334042549,\n",
       " -0.04310823604464531,\n",
       " 0.02471153996884823,\n",
       " 0.04900754615664482,\n",
       " 0.004271131474524736,\n",
       " -0.02027592994272709,\n",
       " -0.008782602846622467,\n",
       " 0.02506246417760849,\n",
       " -0.07950631529092789,\n",
       " -0.006011409685015678,\n",
       " -0.026047946885228157,\n",
       " -0.05307864770293236,\n",
       " 0.01660393550992012,\n",
       " 0.0076425326988101006,\n",
       " -0.004259942099452019,\n",
       " 0.03627146780490875,\n",
       " -0.029359381645917892,\n",
       " -0.024227675050497055,\n",
       " 0.024935293942689896,\n",
       " 0.03495538979768753,\n",
       " -0.03232179582118988,\n",
       " 0.019355209544301033,\n",
       " 0.018522731959819794,\n",
       " 0.025572268292307854,\n",
       " -0.06334591656923294,\n",
       " 0.04087970033288002,\n",
       " 0.025868330150842667,\n",
       " -0.005671807564795017,\n",
       " -0.047163061797618866,\n",
       " 0.05589194595813751,\n",
       " -0.020851245149970055,\n",
       " 0.03149983659386635,\n",
       " -0.02974580228328705,\n",
       " 0.006241379305720329,\n",
       " -0.012248248793184757,\n",
       " 0.05412828177213669,\n",
       " 0.0033481610007584095,\n",
       " -0.02784995175898075,\n",
       " 0.02854503132402897,\n",
       " 0.06057217717170715,\n",
       " -0.0256026741117239,\n",
       " -0.011445633135735989,\n",
       " -0.011995002627372742,\n",
       " 0.030840985476970673,\n",
       " -0.009903688915073872,\n",
       " 0.05353672057390213,\n",
       " 0.028958145529031754,\n",
       " 0.012695414014160633,\n",
       " 0.04489578679203987,\n",
       " 0.057930782437324524,\n",
       " 0.021174704656004906,\n",
       " 0.022348465397953987,\n",
       " 0.0020389852579683065,\n",
       " 0.021322954446077347,\n",
       " 0.09770306199789047,\n",
       " 0.016542375087738037,\n",
       " -0.01742549054324627,\n",
       " 0.0336756631731987,\n",
       " -0.009769043885171413,\n",
       " -0.0342349037528038,\n",
       " -0.018559491261839867,\n",
       " 0.018948767334222794,\n",
       " 0.008276082575321198,\n",
       " 0.0413363054394722,\n",
       " 0.013074846006929874,\n",
       " -0.00635091308504343,\n",
       " -0.07434742152690887,\n",
       " -0.0038412902504205704,\n",
       " 0.01160022709518671,\n",
       " 0.04278187453746796,\n",
       " -0.051981717348098755,\n",
       " -0.0541759692132473,\n",
       " -0.012725085951387882,\n",
       " 0.0021242082584649324,\n",
       " 0.01309747714549303,\n",
       " 0.00752713019028306,\n",
       " 0.012696079909801483,\n",
       " 0.032677751034498215,\n",
       " 0.041599832475185394,\n",
       " 0.03078550286591053,\n",
       " -0.008637627586722374,\n",
       " -0.021359603852033615,\n",
       " -0.030095506459474564,\n",
       " -0.0474805012345314,\n",
       " -0.09255046397447586,\n",
       " 0.010526563972234726,\n",
       " -0.04479103162884712,\n",
       " -0.0035000196658074856,\n",
       " 0.0422641783952713,\n",
       " -0.056985028088092804,\n",
       " 0.02696271613240242,\n",
       " -0.02224195934832096,\n",
       " -0.033874742686748505,\n",
       " 0.02058039791882038,\n",
       " -0.055815353989601135,\n",
       " 0.02792152762413025,\n",
       " 0.017537331208586693,\n",
       " 0.040131889283657074,\n",
       " -0.004902286920696497,\n",
       " 0.013327490538358688,\n",
       " 0.014400177635252476,\n",
       " 0.04790281876921654,\n",
       " -0.05068559572100639,\n",
       " -0.002731847343966365,\n",
       " -0.010966087691485882,\n",
       " -0.047471921890974045,\n",
       " 0.025092896074056625,\n",
       " -0.007291148882359266,\n",
       " -0.005781370215117931,\n",
       " -0.021047847345471382,\n",
       " -0.04149411618709564,\n",
       " -0.055785421282052994,\n",
       " -0.015366082079708576,\n",
       " 0.01563503034412861,\n",
       " -0.01422838680446148,\n",
       " -0.007404271047562361,\n",
       " -0.029391078278422356,\n",
       " 0.045133836567401886,\n",
       " 0.04324008896946907,\n",
       " -0.024820959195494652,\n",
       " -0.0041477563790977,\n",
       " 0.0066239177249372005,\n",
       " -0.0788382738828659,\n",
       " 0.04899761825799942,\n",
       " -0.019472453743219376,\n",
       " -0.0028861358296126127,\n",
       " -0.05098046362400055,\n",
       " 0.029416432604193687,\n",
       " 0.010077928192913532,\n",
       " 0.015777094289660454,\n",
       " -0.009140443056821823,\n",
       " -0.03641372174024582,\n",
       " 0.01293633971363306,\n",
       " -0.016389094293117523,\n",
       " 0.021585363894701004,\n",
       " -0.0014776650350540876,\n",
       " 0.007865671068429947,\n",
       " 0.025633839890360832,\n",
       " 0.008498819544911385,\n",
       " -0.08315078169107437,\n",
       " 0.02853597328066826,\n",
       " -0.0076004271395504475,\n",
       " -0.025773122906684875,\n",
       " -0.01751488260924816,\n",
       " -0.018599247559905052,\n",
       " 0.026180820539593697,\n",
       " 0.009425999596714973,\n",
       " -0.014803154394030571,\n",
       " -0.0219892431050539,\n",
       " -0.03636296093463898,\n",
       " 0.012095674872398376,\n",
       " 0.026800595223903656,\n",
       " 0.005210536066442728,\n",
       " -0.029995853081345558,\n",
       " -0.019322922453284264,\n",
       " 0.040387094020843506,\n",
       " -0.00642275158315897,\n",
       " -0.004897145088762045,\n",
       " 0.00955183245241642,\n",
       " -0.022124655544757843,\n",
       " -0.04521467164158821,\n",
       " 0.018490293994545937,\n",
       " -0.01518160942941904,\n",
       " 0.04634876921772957,\n",
       " 0.000955567171331495,\n",
       " 0.05256311222910881,\n",
       " -0.0071581886149942875,\n",
       " 0.03334472328424454,\n",
       " -0.013292989693582058,\n",
       " 0.02008003741502762,\n",
       " 0.029982686042785645,\n",
       " 0.016058096662163734,\n",
       " -0.023598073050379753,\n",
       " 0.0578337125480175,\n",
       " 0.02343282289803028,\n",
       " 0.00217553973197937,\n",
       " 0.0010997828794643283,\n",
       " -0.05524547025561333,\n",
       " 0.010832516476511955,\n",
       " 0.002593743847683072,\n",
       " -0.0034717300441116095,\n",
       " -0.05328355357050896,\n",
       " 0.0378577746450901,\n",
       " -0.012468390166759491,\n",
       " -0.042367566376924515,\n",
       " 0.009440082125365734,\n",
       " -0.007422144524753094,\n",
       " -0.006951246410608292,\n",
       " 0.05556102469563484,\n",
       " 0.00959249772131443,\n",
       " 0.036651112139225006,\n",
       " -0.016465160995721817,\n",
       " -0.0014284042408689857,\n",
       " 0.0017529394244775176,\n",
       " 0.01893853396177292,\n",
       " 0.05384431034326553,\n",
       " 0.005124973598867655,\n",
       " 0.052895475178956985,\n",
       " 0.008729959838092327,\n",
       " -0.011923805810511112,\n",
       " -0.016154224053025246,\n",
       " -0.0487309955060482,\n",
       " -0.019016167148947716,\n",
       " -0.017711825668811798,\n",
       " 0.02587164379656315,\n",
       " -0.0002854345366358757,\n",
       " -0.02381163462996483,\n",
       " -0.032281294465065,\n",
       " 0.05074727162718773,\n",
       " 0.004828405100852251,\n",
       " 0.027949674054980278,\n",
       " -0.0011859432561323047,\n",
       " 0.01079865638166666,\n",
       " 0.020808910951018333,\n",
       " -0.00240119406953454,\n",
       " 0.054042261093854904,\n",
       " 0.011919238604605198,\n",
       " 0.02900620922446251,\n",
       " -0.026445679366588593,\n",
       " 0.06788835674524307,\n",
       " -0.011224296875298023,\n",
       " 0.03535185381770134,\n",
       " -0.041992414742708206,\n",
       " 0.034420594573020935,\n",
       " -0.0031168689019978046,\n",
       " 0.005614565685391426,\n",
       " 0.012874588370323181,\n",
       " -0.015841424465179443,\n",
       " -0.07587574422359467,\n",
       " 0.007527275010943413,\n",
       " 0.016620317474007607,\n",
       " 0.034384604543447495,\n",
       " -0.03684661164879799,\n",
       " -0.032782915979623795,\n",
       " 0.010150442831218243,\n",
       " 0.022855320945382118,\n",
       " -0.0004877771425526589,\n",
       " -0.013772694393992424,\n",
       " 0.040507152676582336,\n",
       " -0.051016516983509064,\n",
       " 0.037133265286684036,\n",
       " 0.042162589728832245,\n",
       " -0.020586516708135605,\n",
       " 0.0097458865493536,\n",
       " -0.0006615177844651043,\n",
       " -0.014652046374976635,\n",
       " -0.018079817295074463,\n",
       " 0.007162688300013542,\n",
       " 0.04604082554578781,\n",
       " -0.010263817384839058,\n",
       " 0.05084474757313728,\n",
       " -0.025291359052062035,\n",
       " 0.05172353982925415,\n",
       " 0.017576714977622032,\n",
       " -0.025360487401485443,\n",
       " -0.023922286927700043,\n",
       " -0.004490136168897152,\n",
       " 0.015992572531104088,\n",
       " 0.020134009420871735,\n",
       " 0.019208896905183792,\n",
       " -0.05279521644115448,\n",
       " -0.03285691514611244,\n",
       " 0.0516260601580143,\n",
       " -0.06380031257867813,\n",
       " 0.002680060686543584,\n",
       " -0.029948772862553596,\n",
       " 0.015439490787684917,\n",
       " -0.05263122543692589,\n",
       " -0.0014229783555492759,\n",
       " 0.007567595224827528,\n",
       " -0.003691799473017454,\n",
       " -0.01405949704349041,\n",
       " 0.023979831486940384,\n",
       " 0.020598353818058968,\n",
       " 0.029585454612970352,\n",
       " -0.016313137486577034,\n",
       " -0.05754174292087555,\n",
       " 0.039541274309158325,\n",
       " 0.019248606637120247,\n",
       " -0.04114115238189697,\n",
       " 0.008337191306054592,\n",
       " -0.005038410425186157,\n",
       " -0.010278946720063686,\n",
       " -0.012518079951405525,\n",
       " -0.03784697502851486,\n",
       " 0.011473010294139385,\n",
       " 0.01753181405365467,\n",
       " -0.020953763276338577,\n",
       " 0.021519752219319344,\n",
       " 0.036052998155355453,\n",
       " -0.025190789252519608,\n",
       " -0.008754433132708073,\n",
       " 0.012265518307685852,\n",
       " 0.00878180656582117,\n",
       " 0.004871409386396408,\n",
       " -0.017725303769111633,\n",
       " -0.047095414251089096,\n",
       " -0.05748520791530609,\n",
       " 0.023441487923264503,\n",
       " -0.03268088772892952,\n",
       " -0.007921498268842697,\n",
       " -0.0075652324594557285,\n",
       " 0.018298935145139694,\n",
       " 0.017188897356390953,\n",
       " 0.0019829007796943188,\n",
       " -0.010013234801590443,\n",
       " -0.008973116986453533,\n",
       " -0.023271741345524788,\n",
       " -0.035202741622924805,\n",
       " -0.05239628255367279,\n",
       " 0.005806881934404373,\n",
       " 0.03971399739384651,\n",
       " 0.07943980395793915,\n",
       " 0.009194159880280495,\n",
       " -0.007394499145448208,\n",
       " -0.012484555132687092,\n",
       " 0.0061843302100896835,\n",
       " -0.04534268006682396,\n",
       " -0.047133468091487885,\n",
       " -0.016284853219985962,\n",
       " 0.0364973209798336,\n",
       " 0.04252941906452179,\n",
       " 0.03580578789114952,\n",
       " 0.01622430793941021,\n",
       " -0.005990632809698582,\n",
       " -0.017667043954133987,\n",
       " 0.039385564625263214,\n",
       " 0.010875524021685123,\n",
       " 0.039497360587120056,\n",
       " -0.020201100036501884,\n",
       " 0.002559482818469405,\n",
       " 0.05734499543905258,\n",
       " 0.027428913861513138,\n",
       " -0.04514143615961075,\n",
       " -0.008307305164635181,\n",
       " 0.03892887756228447,\n",
       " -0.007389279548078775,\n",
       " 0.026192067191004753,\n",
       " -0.018307413905858994,\n",
       " -0.057773008942604065,\n",
       " -0.05836794152855873,\n",
       " -0.043476928025484085,\n",
       " -0.018198290839791298,\n",
       " -0.03975703939795494,\n",
       " -0.022822244092822075,\n",
       " -0.03981276601552963,\n",
       " -0.005379013251513243,\n",
       " -0.0178392194211483,\n",
       " -0.00985411461442709,\n",
       " 0.01848616637289524,\n",
       " -0.016665199771523476,\n",
       " -0.006667028646916151,\n",
       " -0.059158485382795334,\n",
       " 0.01846095733344555,\n",
       " 0.025747882202267647,\n",
       " 0.028691763058304787,\n",
       " -0.03619053214788437,\n",
       " -0.0035811348352581263,\n",
       " 0.010943676345050335,\n",
       " 0.041586875915527344,\n",
       " 0.0052163610234856606,\n",
       " -0.007166895549744368,\n",
       " 0.0025777744594961405,\n",
       " 0.02007647231221199,\n",
       " 0.009351284243166447,\n",
       " 0.031157808378338814,\n",
       " -0.017857788130640984,\n",
       " -0.045141372829675674,\n",
       " 0.003171013668179512,\n",
       " 0.0326244980096817,\n",
       " -0.015244961716234684,\n",
       " 0.02535134181380272,\n",
       " -0.013107351958751678,\n",
       " 0.012946921400725842,\n",
       " -0.005180930718779564,\n",
       " -0.029825618490576744,\n",
       " -0.001336855930276215,\n",
       " -0.016187017783522606,\n",
       " -0.0217861570417881,\n",
       " -0.00835687667131424,\n",
       " 0.06586048752069473,\n",
       " -0.023977572098374367,\n",
       " -0.0022764632012695074,\n",
       " -0.015973281115293503,\n",
       " 0.04368387535214424,\n",
       " 0.0516558438539505,\n",
       " 0.0050534592010080814,\n",
       " -0.06481030583381653,\n",
       " -0.02558765560388565,\n",
       " -0.027954986318945885,\n",
       " -0.07268310338258743,\n",
       " 0.022539133206009865,\n",
       " -0.010752127505838871,\n",
       " 0.006533456034958363,\n",
       " -0.005220737773925066,\n",
       " -0.05306435376405716,\n",
       " 0.05506932735443115,\n",
       " -0.01778186485171318,\n",
       " 0.015635591000318527,\n",
       " 0.0570237897336483,\n",
       " 0.0012208224507048726,\n",
       " -0.0006785342120565474,\n",
       " -0.0487518347799778,\n",
       " 0.006805185694247484,\n",
       " 0.00927868764847517,\n",
       " -0.02535734511911869,\n",
       " -0.0010152107570320368,\n",
       " -0.038711801171302795,\n",
       " -0.03565556928515434,\n",
       " 0.022559402510523796,\n",
       " -0.03110097162425518,\n",
       " -0.013233762234449387,\n",
       " -0.0349201001226902,\n",
       " 0.020766254514455795,\n",
       " 0.06078135967254639,\n",
       " 0.002848211443051696,\n",
       " 0.03051247075200081,\n",
       " -0.01159912720322609,\n",
       " -0.024279460310935974,\n",
       " -0.017249828204512596,\n",
       " 0.034321121871471405,\n",
       " 0.05516280233860016,\n",
       " 0.0056841191835701466,\n",
       " 0.021068478003144264,\n",
       " -0.017676183953881264,\n",
       " -0.006442761979997158,\n",
       " 0.006930760107934475,\n",
       " -0.011690836399793625,\n",
       " -0.021160900592803955,\n",
       " -0.02389945462346077,\n",
       " 0.025939490646123886,\n",
       " 0.013766712509095669,\n",
       " -0.014866843819618225,\n",
       " 0.053822629153728485,\n",
       " -0.005029882770031691,\n",
       " -0.06806861609220505,\n",
       " -0.020477809011936188,\n",
       " -0.006487586535513401,\n",
       " 0.001277790404856205,\n",
       " -0.043692562729120255,\n",
       " -0.035495076328516006,\n",
       " -0.019014470279216766,\n",
       " 0.07520005106925964,\n",
       " -0.012979162856936455,\n",
       " -0.01052278932183981,\n",
       " -0.015144867822527885,\n",
       " -0.0354083850979805,\n",
       " 0.026457950472831726,\n",
       " -0.020778683945536613,\n",
       " 0.004268374294042587,\n",
       " -0.0003660910588223487,\n",
       " 0.00820335652679205,\n",
       " 0.04413215443491936,\n",
       " -0.0034076713491231203,\n",
       " 0.025452062487602234,\n",
       " -0.05351822078227997,\n",
       " 0.018065212294459343,\n",
       " -0.0027350313030183315,\n",
       " 0.017398960888385773,\n",
       " 0.03848876431584358,\n",
       " 0.004573867190629244,\n",
       " -0.007470646873116493,\n",
       " 0.03235684707760811,\n",
       " 0.03298969939351082,\n",
       " 0.055333785712718964,\n",
       " -0.01631118543446064,\n",
       " 0.0006841457798145711,\n",
       " 0.04782183840870857,\n",
       " -0.04013960063457489,\n",
       " -0.02162097580730915,\n",
       " 0.0017167633632197976,\n",
       " 0.02081453427672386,\n",
       " 0.001724197412841022,\n",
       " -0.01958315819501877,\n",
       " -0.03466879576444626,\n",
       " -0.001783357234671712,\n",
       " 0.029863571748137474,\n",
       " 0.0009640646167099476,\n",
       " -0.0015918067656457424,\n",
       " -0.030798515304923058,\n",
       " 0.00031519297044724226,\n",
       " -0.05385768413543701,\n",
       " -0.04965865612030029,\n",
       " 0.0011666938662528992,\n",
       " -0.02159872278571129,\n",
       " 0.008063580840826035,\n",
       " 0.04463837295770645,\n",
       " -0.02822355180978775,\n",
       " 0.015534116886556149,\n",
       " -0.015356599353253841,\n",
       " 0.06234091892838478,\n",
       " -0.028350142762064934,\n",
       " 0.04740610718727112,\n",
       " -0.03546861559152603,\n",
       " 0.016997212544083595,\n",
       " -0.03475497290492058,\n",
       " 0.0020109638571739197,\n",
       " -0.04884074628353119,\n",
       " -0.01679995097219944,\n",
       " -0.03718625009059906,\n",
       " -0.016957197338342667,\n",
       " -0.041929155588150024,\n",
       " 0.017660409212112427,\n",
       " 0.016708126291632652,\n",
       " 0.03186483681201935,\n",
       " -0.004114862997084856,\n",
       " 0.023805566132068634,\n",
       " -0.028591524809598923,\n",
       " -0.00683576799929142,\n",
       " 0.05671696737408638,\n",
       " 0.02211897261440754,\n",
       " -0.016259899362921715,\n",
       " 0.040912970900535583,\n",
       " 0.024637119844555855,\n",
       " 0.00898826029151678,\n",
       " -0.016313303261995316,\n",
       " -0.010700576938688755,\n",
       " -0.017142783850431442,\n",
       " -0.007509918417781591,\n",
       " 0.026148203760385513,\n",
       " 0.04465605318546295,\n",
       " -0.02766398712992668,\n",
       " 0.026603462174534798,\n",
       " -0.02364078164100647,\n",
       " -0.004606903064996004,\n",
       " -0.04989989101886749,\n",
       " -0.029934730380773544,\n",
       " -0.040939297527074814,\n",
       " -0.0175652913749218,\n",
       " -0.03377696871757507,\n",
       " -0.006190442014485598,\n",
       " -0.016622429713606834,\n",
       " 0.07110536098480225,\n",
       " 0.022677576169371605,\n",
       " -0.02074427902698517,\n",
       " -0.002996931318193674,\n",
       " 0.00299253361299634,\n",
       " 0.01669463887810707,\n",
       " 0.013148997910320759,\n",
       " 0.0048896740190684795,\n",
       " 0.014070028439164162,\n",
       " 0.017207758501172066,\n",
       " 0.005885313730686903,\n",
       " -0.004529710393399,\n",
       " -0.0013766051270067692,\n",
       " 0.011590360663831234,\n",
       " -0.010968622751533985,\n",
       " -0.0004788747464772314,\n",
       " -0.035549014806747437,\n",
       " -0.04259254410862923,\n",
       " 0.027710750699043274,\n",
       " -0.01118163950741291,\n",
       " 0.026232659816741943,\n",
       " 0.04976966232061386,\n",
       " -0.06842382997274399,\n",
       " -0.0055152541026473045,\n",
       " -0.041822146624326706,\n",
       " -0.058309588581323624,\n",
       " 0.02450382709503174,\n",
       " -0.05850159004330635,\n",
       " -0.019219722598791122,\n",
       " 0.009528536349534988,\n",
       " -0.005473202094435692,\n",
       " 0.014069439843297005,\n",
       " 0.026744510978460312,\n",
       " -0.05651496350765228,\n",
       " 0.021145042032003403,\n",
       " 0.041987840086221695,\n",
       " -0.0210577379912138,\n",
       " -0.043014612048864365,\n",
       " 0.0020138316322118044,\n",
       " 0.024622183293104172,\n",
       " -0.019509147852659225,\n",
       " -0.01361014973372221,\n",
       " -0.00984262302517891,\n",
       " 0.00380213581956923,\n",
       " 0.01611911691725254,\n",
       " 0.007495421450585127,\n",
       " -0.013409879989922047,\n",
       " 0.011727114208042622,\n",
       " 0.0019255023216828704,\n",
       " -0.0036932658404111862,\n",
       " -0.02625482901930809,\n",
       " -0.018938716500997543,\n",
       " -0.015107905492186546,\n",
       " -0.02666463889181614,\n",
       " 0.007128983270376921,\n",
       " 0.00039372971514239907,\n",
       " 0.018163159489631653,\n",
       " -0.01067511085420847,\n",
       " 0.02324560284614563,\n",
       " -0.053826965391635895,\n",
       " 0.02460278384387493,\n",
       " 0.010687492787837982,\n",
       " -0.025354377925395966,\n",
       " 0.01868141070008278,\n",
       " -0.011067179962992668,\n",
       " -0.03818286210298538,\n",
       " 0.05789029970765114,\n",
       " 0.0682726725935936,\n",
       " 0.017337672412395477,\n",
       " 0.013051723130047321,\n",
       " -0.00408842833712697,\n",
       " 0.004420932848006487,\n",
       " 0.01902029477059841,\n",
       " -0.021697454154491425,\n",
       " -0.018788494169712067,\n",
       " 0.04899011179804802,\n",
       " 0.029448634013533592,\n",
       " -0.018158111721277237,\n",
       " -0.03892190009355545,\n",
       " 0.003915064036846161,\n",
       " 0.019145410507917404,\n",
       " -0.01581747457385063,\n",
       " -0.02822829596698284,\n",
       " 0.0016245837323367596,\n",
       " -0.015237758867442608,\n",
       " 0.0719580426812172,\n",
       " -0.030298184603452682,\n",
       " -0.03237989917397499,\n",
       " -0.034877363592386246,\n",
       " -0.013288481160998344,\n",
       " -0.03793249651789665,\n",
       " 0.025787802413105965,\n",
       " -0.047995954751968384,\n",
       " -0.01535097137093544,\n",
       " -0.021253813058137894,\n",
       " -0.015573469921946526,\n",
       " 0.02466747909784317,\n",
       " -0.03780451416969299,\n",
       " 0.0003008091589435935,\n",
       " -0.02346750721335411,\n",
       " -0.012059886008501053,\n",
       " 0.04300066828727722,\n",
       " 0.046102479100227356,\n",
       " 0.044140398502349854,\n",
       " -0.018512191250920296,\n",
       " 0.052898991852998734,\n",
       " 0.001926105353049934,\n",
       " -0.014761199243366718,\n",
       " 0.02919108048081398,\n",
       " 0.002624825108796358,\n",
       " 0.028203750029206276,\n",
       " -0.006870006676763296,\n",
       " -0.01825336553156376,\n",
       " 0.0023054657503962517,\n",
       " -0.005114029161632061,\n",
       " 0.002477708039805293,\n",
       " -0.036468133330345154,\n",
       " -0.030697213485836983,\n",
       " 0.03321026638150215,\n",
       " 0.004282830283045769,\n",
       " -0.01629112847149372,\n",
       " 0.04489155858755112,\n",
       " -0.02987494505941868,\n",
       " -0.0030427270103245974,\n",
       " -0.0002626189962029457,\n",
       " 0.03455258160829544,\n",
       " -0.00022437972074840218,\n",
       " -0.0015008944319561124,\n",
       " 0.04104707017540932,\n",
       " -0.03519173339009285,\n",
       " -0.025943905115127563,\n",
       " 0.025675460696220398,\n",
       " 0.005787405651062727,\n",
       " 0.01870087720453739,\n",
       " 0.008291518315672874,\n",
       " 0.06435272842645645,\n",
       " 0.014138609170913696,\n",
       " 0.020693447440862656,\n",
       " 0.03226417675614357,\n",
       " -0.00033161547617055476,\n",
       " -0.01486794464290142,\n",
       " 0.011518730781972408,\n",
       " -0.016484344378113747,\n",
       " -0.042385414242744446,\n",
       " 0.031919997185468674,\n",
       " -0.04420439153909683,\n",
       " 0.012081614695489407,\n",
       " 0.018175896257162094,\n",
       " -0.010349907912313938,\n",
       " -0.019316677004098892,\n",
       " 0.01249643974006176,\n",
       " 0.003727981122210622,\n",
       " -0.028844933956861496,\n",
       " -0.020032139495015144,\n",
       " 0.01648956909775734,\n",
       " -0.017408469691872597,\n",
       " 0.05396737530827522,\n",
       " 0.007734279613941908,\n",
       " 0.0033396140206605196,\n",
       " 0.026979632675647736,\n",
       " -0.03787607327103615,\n",
       " 0.02080891840159893,\n",
       " -0.00034430314553901553,\n",
       " 0.012009434401988983,\n",
       " -0.05281513184309006,\n",
       " -0.012484352104365826,\n",
       " -0.03894611820578575,\n",
       " -0.024006027728319168,\n",
       " -0.013276653364300728,\n",
       " -0.050503261387348175,\n",
       " -0.0168830044567585,\n",
       " 0.03056931495666504,\n",
       " -0.009480212815105915,\n",
       " -0.06724938750267029,\n",
       " -0.0048878369852900505,\n",
       " 0.01693408191204071,\n",
       " 0.002825353993102908,\n",
       " 0.006972188595682383,\n",
       " -0.03525952994823456,\n",
       " 0.03613517805933952,\n",
       " 0.044253844767808914,\n",
       " 0.03691267594695091,\n",
       " -0.04589751362800598,\n",
       " 0.053177058696746826,\n",
       " -0.002220102120190859,\n",
       " 0.0007004954968579113,\n",
       " 0.044171594083309174,\n",
       " -0.02830236218869686,\n",
       " 0.04724063724279404,\n",
       " 0.0378403402864933,\n",
       " -0.005969381891191006,\n",
       " -0.006151693873107433,\n",
       " 0.039422448724508286,\n",
       " -0.012058675289154053,\n",
       " -0.03178262710571289,\n",
       " 0.007469534873962402,\n",
       " 0.03621393069624901,\n",
       " 0.01683078519999981,\n",
       " -0.044120848178863525,\n",
       " -0.07045342773199081,\n",
       " -0.02542862296104431,\n",
       " -0.05054130777716637,\n",
       " 0.0007199704414233565,\n",
       " -0.02430308237671852,\n",
       " 0.04614081233739853,\n",
       " 0.008455217815935612,\n",
       " 0.018133575096726418,\n",
       " 0.011681018397212029,\n",
       " -0.06540088355541229,\n",
       " 0.2427520453929901,\n",
       " 0.07172387093305588,\n",
       " 0.054594844579696655,\n",
       " -0.001963271526619792,\n",
       " -0.022456465288996696,\n",
       " 0.04515949264168739,\n",
       " 0.028557248413562775,\n",
       " -0.02959482930600643,\n",
       " 0.020333077758550644,\n",
       " 0.003989857621490955,\n",
       " -0.0006793115753680468,\n",
       " -0.03597106412053108,\n",
       " -0.031026659533381462,\n",
       " 0.035530779510736465,\n",
       " 0.012292282655835152,\n",
       " 0.04729628190398216,\n",
       " -0.05736849084496498,\n",
       " -0.016882047057151794,\n",
       " 0.023737546056509018,\n",
       " -0.038775596767663956,\n",
       " -0.05881933867931366,\n",
       " 0.00520313810557127,\n",
       " -0.006130371242761612,\n",
       " 0.009890727698802948,\n",
       " 0.024692723527550697,\n",
       " -0.01042462233453989,\n",
       " 0.07181032001972198,\n",
       " -0.03503919765353203,\n",
       " -0.0002123251324519515,\n",
       " 0.027453571557998657,\n",
       " -0.018788130953907967,\n",
       " -0.0239203292876482,\n",
       " 0.03309144452214241,\n",
       " 0.0039789979346096516,\n",
       " 0.01311994343996048,\n",
       " -0.021513696759939194,\n",
       " -0.01465670857578516,\n",
       " -0.03063366562128067,\n",
       " -0.05009622871875763,\n",
       " -0.0028819115832448006,\n",
       " -0.0042076692916452885,\n",
       " 0.0024811045732349157,\n",
       " 0.010305013507604599,\n",
       " -0.022839972749352455,\n",
       " -0.016920363530516624,\n",
       " 0.00953025184571743,\n",
       " -0.015676112845540047,\n",
       " 0.03667374700307846,\n",
       " -0.031065339222550392,\n",
       " -0.03698211535811424,\n",
       " 0.04803525656461716,\n",
       " -0.007132818456739187,\n",
       " 0.0029480471275746822,\n",
       " -0.04689775034785271,\n",
       " -0.01928161270916462,\n",
       " 0.028630826622247696,\n",
       " -0.0009580656769685447,\n",
       " -0.027919935062527657,\n",
       " -0.022401828318834305,\n",
       " 0.009012718684971333,\n",
       " -0.03413267061114311,\n",
       " -0.03204113990068436,\n",
       " -0.007162302732467651,\n",
       " -0.016412200406193733,\n",
       " -0.014636922627687454,\n",
       " 0.051451798528432846,\n",
       " 0.007703028153628111,\n",
       " 0.035209983587265015,\n",
       " -0.03858897462487221,\n",
       " 0.004163132049143314,\n",
       " -0.011126125231385231,\n",
       " ...]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2cc06332-13f3-40d0-a437-6ebbd4753155",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_ans = embeddings.embed_query(\"What is the second letter of the greek alphabets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e829c72a-cd94-414c-8371-e9d1ba985b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.018760541453957558,\n",
       " 0.0027914312668144703,\n",
       " 0.01640276424586773,\n",
       " -0.008009024895727634,\n",
       " -0.08523359894752502,\n",
       " -0.016433728858828545,\n",
       " 0.05806936323642731,\n",
       " 0.023995280265808105,\n",
       " 0.0005751606659032404,\n",
       " 0.03280412033200264,\n",
       " 0.005269171670079231,\n",
       " 0.01563948579132557,\n",
       " -0.03185107558965683,\n",
       " 0.008660389110445976,\n",
       " -0.06639841198921204,\n",
       " -0.0028611638117581606,\n",
       " -0.018696248531341553,\n",
       " 0.0002917601086664945,\n",
       " -0.04092410206794739,\n",
       " -0.012840654700994492,\n",
       " 0.05591011047363281,\n",
       " 0.053638946264982224,\n",
       " -0.07699041813611984,\n",
       " 0.019978661090135574,\n",
       " -0.003759740386158228,\n",
       " 0.015828536823391914,\n",
       " -0.027374276891350746,\n",
       " -0.03970276564359665,\n",
       " 0.007332733366638422,\n",
       " 0.05431250482797623,\n",
       " -0.030517537146806717,\n",
       " 0.019720114767551422,\n",
       " 0.026561785489320755,\n",
       " -0.03649875149130821,\n",
       " -0.0012056123232468963,\n",
       " -0.03193274885416031,\n",
       " 0.038359418511390686,\n",
       " -0.0350051112473011,\n",
       " -0.004734362009912729,\n",
       " -0.05996692553162575,\n",
       " 0.02492077462375164,\n",
       " 6.737854710081592e-05,\n",
       " -0.014212377369403839,\n",
       " -0.03216804563999176,\n",
       " -0.0369146503508091,\n",
       " -0.004834766499698162,\n",
       " -0.02869066223502159,\n",
       " -0.041680093854665756,\n",
       " -0.07674014568328857,\n",
       " -0.02473577857017517,\n",
       " -0.0058761825785040855,\n",
       " -0.008351219817996025,\n",
       " 0.03435409814119339,\n",
       " -0.014043101109564304,\n",
       " -0.023487290367484093,\n",
       " 0.017553025856614113,\n",
       " 0.021465620025992393,\n",
       " -0.012413155287504196,\n",
       " -0.030611110851168633,\n",
       " 0.009500348009169102,\n",
       " 0.041304051876068115,\n",
       " 0.0002939550904557109,\n",
       " 0.005062801297754049,\n",
       " -0.04416252300143242,\n",
       " -0.032475974410772324,\n",
       " -0.023546870797872543,\n",
       " 0.04813879355788231,\n",
       " -0.018673229962587357,\n",
       " 0.049955934286117554,\n",
       " 0.02682206593453884,\n",
       " 0.008457450196146965,\n",
       " 0.05321073904633522,\n",
       " 0.0005353716551326215,\n",
       " -0.04308352246880531,\n",
       " -0.03398197144269943,\n",
       " 0.01751873455941677,\n",
       " -0.030167507007718086,\n",
       " -0.019638335332274437,\n",
       " -0.05216231569647789,\n",
       " 0.04331718385219574,\n",
       " -0.01377356518059969,\n",
       " 0.028390007093548775,\n",
       " -0.003907491452991962,\n",
       " -0.014232047833502293,\n",
       " 0.0017894355114549398,\n",
       " 0.0118399141356349,\n",
       " 0.02426893077790737,\n",
       " 0.016419868916273117,\n",
       " -0.002365397522225976,\n",
       " 0.01654679700732231,\n",
       " 0.0438804067671299,\n",
       " 0.02592131309211254,\n",
       " -0.003168080234900117,\n",
       " 0.03210792690515518,\n",
       " 0.067726269364357,\n",
       " 0.010449906811118126,\n",
       " 0.017617706209421158,\n",
       " 0.009519319981336594,\n",
       " 0.029532872140407562,\n",
       " 0.02447253279387951,\n",
       " 0.06580107659101486,\n",
       " 0.047491904348134995,\n",
       " -0.02940039150416851,\n",
       " 0.04975375533103943,\n",
       " -0.01789041981101036,\n",
       " 0.013345242477953434,\n",
       " -0.005452742800116539,\n",
       " 0.021959517151117325,\n",
       " 0.0035122670233249664,\n",
       " -0.016084598377346992,\n",
       " 0.03595558553934097,\n",
       " 0.00677313469350338,\n",
       " 0.006525026168674231,\n",
       " -0.010226513259112835,\n",
       " -0.0038993784692138433,\n",
       " 0.019009726122021675,\n",
       " 0.009352441877126694,\n",
       " 0.02714378386735916,\n",
       " -0.07531793415546417,\n",
       " -0.009275490418076515,\n",
       " 0.0125163234770298,\n",
       " 0.04073484241962433,\n",
       " 0.009888898581266403,\n",
       " 0.04734654352068901,\n",
       " 0.01434903685003519,\n",
       " -0.02709200233221054,\n",
       " -0.015789803117513657,\n",
       " -0.00259644677862525,\n",
       " -0.07169277966022491,\n",
       " 0.02548965997993946,\n",
       " 0.004383126739412546,\n",
       " -0.010481972247362137,\n",
       " -0.010540345683693886,\n",
       " 0.022355781868100166,\n",
       " -0.03178985416889191,\n",
       " 0.0009667545673437417,\n",
       " 0.006242016796022654,\n",
       " -0.004062582738697529,\n",
       " 0.03463802859187126,\n",
       " -0.025177083909511566,\n",
       " 0.04067125916481018,\n",
       " 0.02338123507797718,\n",
       " -0.023717958480119705,\n",
       " 0.06597401201725006,\n",
       " -0.02538422867655754,\n",
       " -0.004833657760173082,\n",
       " 0.020422084257006645,\n",
       " 0.06706766784191132,\n",
       " -0.06444726884365082,\n",
       " 0.023246821016073227,\n",
       " -0.016787473112344742,\n",
       " -0.0063984571024775505,\n",
       " -0.0654476061463356,\n",
       " 0.018181221559643745,\n",
       " -0.056795407086610794,\n",
       " 0.012000010348856449,\n",
       " 0.03792276233434677,\n",
       " -0.0015973912086337805,\n",
       " 0.024940431118011475,\n",
       " -0.0003460664884187281,\n",
       " -0.029476700350642204,\n",
       " -0.006436729803681374,\n",
       " 0.030400872230529785,\n",
       " 0.05647603049874306,\n",
       " -0.008901442401111126,\n",
       " 0.06587780267000198,\n",
       " -0.02158333733677864,\n",
       " 0.006405696738511324,\n",
       " 0.03400304168462753,\n",
       " -0.04382338374853134,\n",
       " 0.00044586058356799185,\n",
       " -0.021413713693618774,\n",
       " 0.0016898574540391564,\n",
       " 0.03048272617161274,\n",
       " 0.026120223104953766,\n",
       " 0.028515545651316643,\n",
       " -0.02111773192882538,\n",
       " -0.023694809526205063,\n",
       " 0.03300248086452484,\n",
       " 0.04630733281373978,\n",
       " 0.001340193091891706,\n",
       " -0.004461691714823246,\n",
       " 0.03666797652840614,\n",
       " -0.00884254276752472,\n",
       " 0.0059881615452468395,\n",
       " 0.034743621945381165,\n",
       " 0.0024491329677402973,\n",
       " 0.007582277525216341,\n",
       " -0.029567407444119453,\n",
       " -0.01554727740585804,\n",
       " -0.009964489378035069,\n",
       " 0.029464900493621826,\n",
       " 0.007748246658593416,\n",
       " 0.0026855829637497663,\n",
       " -0.04983438551425934,\n",
       " 0.02239111065864563,\n",
       " -0.05571858957409859,\n",
       " -0.014903455041348934,\n",
       " -0.0188430342823267,\n",
       " -0.05629340559244156,\n",
       " 0.004734153859317303,\n",
       " 0.0742725357413292,\n",
       " 0.035425715148448944,\n",
       " 0.036326244473457336,\n",
       " -0.022367481142282486,\n",
       " -0.02418239414691925,\n",
       " 0.04320201650261879,\n",
       " 0.01478755846619606,\n",
       " -0.01678176410496235,\n",
       " 0.004935600329190493,\n",
       " 0.05153360590338707,\n",
       " 0.004963611718267202,\n",
       " -0.038463979959487915,\n",
       " 0.07511581480503082,\n",
       " 0.009190821088850498,\n",
       " -0.0013862388441339135,\n",
       " -0.044132739305496216,\n",
       " 0.06361954659223557,\n",
       " -0.06578747183084488,\n",
       " 0.010558096691966057,\n",
       " -0.013855952769517899,\n",
       " 0.04114145413041115,\n",
       " -0.009641706943511963,\n",
       " 0.07661794871091843,\n",
       " -0.027243241667747498,\n",
       " -0.017889298498630524,\n",
       " 0.04987018182873726,\n",
       " 0.02635302022099495,\n",
       " -0.018034622073173523,\n",
       " 0.0375668965280056,\n",
       " -0.01169948372989893,\n",
       " 0.03521779179573059,\n",
       " 0.001043505733832717,\n",
       " 0.01588430628180504,\n",
       " 0.013261004351079464,\n",
       " -7.707696568104438e-06,\n",
       " 0.045856915414333344,\n",
       " 0.07141983509063721,\n",
       " 0.028207985684275627,\n",
       " 0.008004921488463879,\n",
       " 0.0036773723550140858,\n",
       " -0.027858614921569824,\n",
       " 0.09129902720451355,\n",
       " -0.0010046305833384395,\n",
       " 0.035889752209186554,\n",
       " 0.02772532030940056,\n",
       " 0.014752059243619442,\n",
       " -0.016364920884370804,\n",
       " -0.01786164566874504,\n",
       " -0.0012170186964794993,\n",
       " 0.02968537248671055,\n",
       " 0.03979683294892311,\n",
       " 0.024288805201649666,\n",
       " -0.016921211034059525,\n",
       " -0.06784571707248688,\n",
       " -0.021467158570885658,\n",
       " 0.03982093930244446,\n",
       " 0.033969584852457047,\n",
       " -0.02526674047112465,\n",
       " -0.036090027540922165,\n",
       " -0.012047603726387024,\n",
       " -0.02368435263633728,\n",
       " 0.04185403138399124,\n",
       " -0.011263897642493248,\n",
       " -0.009844752959907055,\n",
       " -0.0035328709054738283,\n",
       " 0.04463662579655647,\n",
       " 0.01728980988264084,\n",
       " -0.023198317736387253,\n",
       " 0.001482451451011002,\n",
       " -0.04915384575724602,\n",
       " -0.027099356055259705,\n",
       " -0.04616612195968628,\n",
       " -0.033819787204265594,\n",
       " -0.03852875903248787,\n",
       " -0.017715491354465485,\n",
       " 0.052433956414461136,\n",
       " -0.029455427080392838,\n",
       " 0.017370831221342087,\n",
       " -0.017930004745721817,\n",
       " -0.04694695398211479,\n",
       " -0.0035594564396888018,\n",
       " -0.08892104774713516,\n",
       " 0.0338875874876976,\n",
       " 0.008966158144176006,\n",
       " 0.07625929266214371,\n",
       " 0.015637407079339027,\n",
       " -0.01704360544681549,\n",
       " 0.016147637739777565,\n",
       " 0.040433369576931,\n",
       " -0.04158378392457962,\n",
       " 0.005059967283159494,\n",
       " -0.012410333380103111,\n",
       " -0.04734831675887108,\n",
       " -0.023873882368206978,\n",
       " -0.022964343428611755,\n",
       " 0.00037012973916716874,\n",
       " -0.029049968346953392,\n",
       " -0.055040113627910614,\n",
       " -0.09232981503009796,\n",
       " 0.00481788907200098,\n",
       " 0.01183764822781086,\n",
       " -0.0009809480980038643,\n",
       " 0.013988028280436993,\n",
       " -0.022168928757309914,\n",
       " 0.027402132749557495,\n",
       " 0.04059194400906563,\n",
       " -0.009649820625782013,\n",
       " 0.005384598858654499,\n",
       " 0.02170928195118904,\n",
       " -0.03608575463294983,\n",
       " 0.05852019414305687,\n",
       " -0.01266456302255392,\n",
       " -0.010175877250730991,\n",
       " -0.05202103033661842,\n",
       " 0.040169987827539444,\n",
       " 0.010380682535469532,\n",
       " 0.02941715158522129,\n",
       " -0.013871589675545692,\n",
       " -0.0184260793030262,\n",
       " 0.014927171170711517,\n",
       " 0.0029008646961301565,\n",
       " 0.011336084455251694,\n",
       " -0.04249575734138489,\n",
       " 0.010681813582777977,\n",
       " 0.03391848877072334,\n",
       " -0.025536704808473587,\n",
       " -0.08367163687944412,\n",
       " 0.025484634563326836,\n",
       " -0.019569704309105873,\n",
       " -0.024059541523456573,\n",
       " -0.0016704052686691284,\n",
       " -0.03210139647126198,\n",
       " 0.023433228954672813,\n",
       " -0.022305868566036224,\n",
       " 0.01940225064754486,\n",
       " 0.014733385294675827,\n",
       " -0.0203708503395319,\n",
       " 0.016873857006430626,\n",
       " 0.029871946200728416,\n",
       " 0.04287000745534897,\n",
       " -0.029939377680420876,\n",
       " -0.03296707198023796,\n",
       " 0.010139730758965015,\n",
       " -0.023670759052038193,\n",
       " -0.009580335579812527,\n",
       " 0.021941090002655983,\n",
       " -0.008183903060853481,\n",
       " -0.054802000522613525,\n",
       " 0.021402640268206596,\n",
       " -0.02017892152070999,\n",
       " 0.028421303257346153,\n",
       " 0.0019141918746754527,\n",
       " 0.029742248356342316,\n",
       " 0.01718081347644329,\n",
       " 0.03841723874211311,\n",
       " -0.020546967163681984,\n",
       " 0.03821779042482376,\n",
       " 0.016224417835474014,\n",
       " 0.04584639519453049,\n",
       " -0.010866462253034115,\n",
       " 0.04707469046115875,\n",
       " 0.007780452258884907,\n",
       " 0.007182570174336433,\n",
       " 0.03768488019704819,\n",
       " -0.040205687284469604,\n",
       " 0.008933376520872116,\n",
       " 0.029906298965215683,\n",
       " 0.012472033500671387,\n",
       " -0.020403381437063217,\n",
       " 0.019933531060814857,\n",
       " -0.005590686574578285,\n",
       " -0.040617380291223526,\n",
       " 0.005262225400656462,\n",
       " -0.029668666422367096,\n",
       " -0.005686918273568153,\n",
       " 0.046272289007902145,\n",
       " -0.014028800651431084,\n",
       " 0.016080616042017937,\n",
       " -0.02948671206831932,\n",
       " -0.01838083378970623,\n",
       " -0.005773343611508608,\n",
       " 0.004902890417724848,\n",
       " 0.006545843090862036,\n",
       " 0.04043826460838318,\n",
       " 0.03847767040133476,\n",
       " 0.0317787304520607,\n",
       " 0.002221829956397414,\n",
       " -0.05710265412926674,\n",
       " -0.047356508672237396,\n",
       " -0.005037588067352772,\n",
       " -0.018334073945879936,\n",
       " 0.035841312259435654,\n",
       " -0.015312292613089085,\n",
       " -0.029094794765114784,\n",
       " -0.0225205160677433,\n",
       " 0.046110574156045914,\n",
       " -0.0026847971603274345,\n",
       " 0.04095427691936493,\n",
       " 0.0075550321489572525,\n",
       " -0.013393951579928398,\n",
       " 0.03463022783398628,\n",
       " 0.0018693760503083467,\n",
       " 0.05813746526837349,\n",
       " -0.022039923816919327,\n",
       " 0.03211032226681709,\n",
       " 0.005649236496537924,\n",
       " 0.049291063100099564,\n",
       " 0.01007224153727293,\n",
       " -0.004023385234177113,\n",
       " -0.05232885479927063,\n",
       " 0.013939409516751766,\n",
       " -0.03305518627166748,\n",
       " 0.010513554327189922,\n",
       " 0.009039720520377159,\n",
       " -0.017286866903305054,\n",
       " -0.06866253912448883,\n",
       " -0.005013465881347656,\n",
       " 0.010658604092895985,\n",
       " 0.03434598818421364,\n",
       " -0.015007388778030872,\n",
       " -0.009922889061272144,\n",
       " -0.01854434609413147,\n",
       " -0.007902768440544605,\n",
       " 0.03541508689522743,\n",
       " -0.015474212355911732,\n",
       " -0.025320254266262054,\n",
       " -0.02322470396757126,\n",
       " 0.01588865928351879,\n",
       " 0.022610189393162727,\n",
       " 0.006004406604915857,\n",
       " 0.02766411192715168,\n",
       " -0.0063562714494764805,\n",
       " -0.016253968700766563,\n",
       " -0.057612039148807526,\n",
       " 0.0053742737509310246,\n",
       " 0.043208204209804535,\n",
       " 0.014932946301996708,\n",
       " 0.02746797911822796,\n",
       " -0.026092683896422386,\n",
       " 0.0768948644399643,\n",
       " 0.01828695461153984,\n",
       " -0.017462287098169327,\n",
       " -0.02515755221247673,\n",
       " -0.04240698739886284,\n",
       " 0.001422515488229692,\n",
       " -0.021723801270127296,\n",
       " 0.0262340996414423,\n",
       " -0.030720312148332596,\n",
       " 0.002913336269557476,\n",
       " 0.01554260402917862,\n",
       " -0.05310862138867378,\n",
       " -0.00505964457988739,\n",
       " 0.012735756114125252,\n",
       " 0.04937979206442833,\n",
       " -0.003920096904039383,\n",
       " -0.003868027124553919,\n",
       " 0.0019518053159117699,\n",
       " 0.0034398927818983793,\n",
       " -0.01655752770602703,\n",
       " 0.028009658679366112,\n",
       " -0.015792159363627434,\n",
       " 0.025067545473575592,\n",
       " -0.04598024860024452,\n",
       " -0.041154518723487854,\n",
       " 0.04397638887166977,\n",
       " 0.005742683541029692,\n",
       " -0.027882326394319534,\n",
       " 0.03250136226415634,\n",
       " 0.0025728947948664427,\n",
       " 6.203023076523095e-05,\n",
       " 0.020508114248514175,\n",
       " -0.024110989645123482,\n",
       " 0.0020135967060923576,\n",
       " 0.01630558632314205,\n",
       " -0.022126780822873116,\n",
       " 0.015517442487180233,\n",
       " 0.0016959141939878464,\n",
       " -0.03251372277736664,\n",
       " 0.019360756501555443,\n",
       " -0.021610045805573463,\n",
       " 0.0025259980466216803,\n",
       " 0.013173137791454792,\n",
       " 0.00204250356182456,\n",
       " -0.036302514374256134,\n",
       " -0.029628509655594826,\n",
       " 0.0544767789542675,\n",
       " -0.0005692703416571021,\n",
       " 0.004725325852632523,\n",
       " -0.005820548161864281,\n",
       " -0.011603382416069508,\n",
       " 0.012534805573523045,\n",
       " 0.018734650686383247,\n",
       " -0.04041730985045433,\n",
       " -0.02929108217358589,\n",
       " -0.030854979529976845,\n",
       " -0.05167097970843315,\n",
       " -0.02400306798517704,\n",
       " 0.03235558047890663,\n",
       " 0.028650479391217232,\n",
       " 0.02558670938014984,\n",
       " -0.005299976095557213,\n",
       " -0.027955634519457817,\n",
       " -0.011173246428370476,\n",
       " 0.003190909279510379,\n",
       " -0.029065264388918877,\n",
       " -0.05045141652226448,\n",
       " -0.006214857567101717,\n",
       " 0.01258081290870905,\n",
       " 0.015993569046258926,\n",
       " -0.03207223862409592,\n",
       " 0.027524545788764954,\n",
       " 0.030289385467767715,\n",
       " -0.02662203647196293,\n",
       " 0.009742279537022114,\n",
       " 0.046451617032289505,\n",
       " 0.04704085737466812,\n",
       " -0.016349323093891144,\n",
       " -0.0009550935938023031,\n",
       " 0.07025833427906036,\n",
       " 0.03978123888373375,\n",
       " -0.08393149077892303,\n",
       " -0.0024877404794096947,\n",
       " -0.0001390226825606078,\n",
       " 0.003764799330383539,\n",
       " 0.023205164819955826,\n",
       " 0.0433465875685215,\n",
       " -0.031405214220285416,\n",
       " -0.04690752178430557,\n",
       " -0.02283896505832672,\n",
       " 0.013579664751887321,\n",
       " -0.04364991560578346,\n",
       " -0.0005172556848265231,\n",
       " -0.04072710499167442,\n",
       " -0.03222772106528282,\n",
       " -0.05274849012494087,\n",
       " -0.01098913885653019,\n",
       " -0.021131791174411774,\n",
       " -0.002649089554324746,\n",
       " -0.0106417341157794,\n",
       " -0.03762723505496979,\n",
       " -0.0024284501560032368,\n",
       " 0.036154504865407944,\n",
       " 0.008308392949402332,\n",
       " -0.03164234757423401,\n",
       " -0.018504545092582703,\n",
       " 0.03543228656053543,\n",
       " 0.032989680767059326,\n",
       " 0.0036287116818130016,\n",
       " 0.039951276034116745,\n",
       " -0.04794449731707573,\n",
       " 0.004196526017040014,\n",
       " -0.011972223408520222,\n",
       " 0.009852181188762188,\n",
       " -0.00834778044372797,\n",
       " -0.05011795088648796,\n",
       " 0.018451299518346786,\n",
       " 0.029272358864545822,\n",
       " -0.03284623473882675,\n",
       " -0.006919220555573702,\n",
       " -0.04056964069604874,\n",
       " -0.022500410676002502,\n",
       " -0.023476483300328255,\n",
       " -0.021131770685315132,\n",
       " -0.00811956450343132,\n",
       " -0.005304884631186724,\n",
       " 0.0014195035910233855,\n",
       " -0.01536368578672409,\n",
       " 0.026184415444731712,\n",
       " -0.012862925417721272,\n",
       " -0.019215945154428482,\n",
       " 0.012916117906570435,\n",
       " 0.04060905799269676,\n",
       " 0.03425227105617523,\n",
       " 0.002470343140885234,\n",
       " -0.015223054215312004,\n",
       " -0.06487147510051727,\n",
       " -0.01130868960171938,\n",
       " -0.0788230150938034,\n",
       " 0.006995969917625189,\n",
       " 0.023055335506796837,\n",
       " 0.004069429822266102,\n",
       " -0.01949470117688179,\n",
       " -0.00832335278391838,\n",
       " 0.06453293561935425,\n",
       " -0.008891139179468155,\n",
       " 0.022489024326205254,\n",
       " 0.04194285720586777,\n",
       " -0.014006179757416248,\n",
       " 0.012838326394557953,\n",
       " -0.050935037434101105,\n",
       " 0.018558526411652565,\n",
       " 0.03071695752441883,\n",
       " -0.03251684084534645,\n",
       " -0.0012417661491781473,\n",
       " -0.0012952913530170918,\n",
       " -0.04150237888097763,\n",
       " 0.005848877131938934,\n",
       " -0.0544687956571579,\n",
       " -0.04936416447162628,\n",
       " -0.029021460562944412,\n",
       " 0.03595329821109772,\n",
       " 0.054502081125974655,\n",
       " -0.024096211418509483,\n",
       " 0.028689803555607796,\n",
       " -0.021101858466863632,\n",
       " -0.009357898496091366,\n",
       " -0.029871255159378052,\n",
       " 0.05290974676609039,\n",
       " 0.01877964474260807,\n",
       " -0.02009442448616028,\n",
       " 0.04250229150056839,\n",
       " -0.015787262469530106,\n",
       " -0.02743316814303398,\n",
       " 0.013406358659267426,\n",
       " -0.010360129177570343,\n",
       " -0.028264068067073822,\n",
       " -0.024170296266674995,\n",
       " 0.02492298185825348,\n",
       " -0.003178904065862298,\n",
       " 0.027651924639940262,\n",
       " 0.03348008543252945,\n",
       " 0.0059428769163787365,\n",
       " -0.04873722791671753,\n",
       " -0.02561560645699501,\n",
       " -0.020197581499814987,\n",
       " 0.033162157982587814,\n",
       " 0.015945518389344215,\n",
       " -0.022082561627030373,\n",
       " -0.005127405747771263,\n",
       " 0.037925876677036285,\n",
       " -0.007222112268209457,\n",
       " -0.036820411682128906,\n",
       " -0.015037375502288342,\n",
       " -0.02537127211689949,\n",
       " 0.050114765763282776,\n",
       " -0.009204842150211334,\n",
       " -0.00022707373136654496,\n",
       " -0.006722388789057732,\n",
       " -0.0036742782685905695,\n",
       " 0.02453632839024067,\n",
       " -0.007848947308957577,\n",
       " -0.003946391865611076,\n",
       " -0.0642908439040184,\n",
       " 0.008245771750807762,\n",
       " -0.02395797334611416,\n",
       " 0.03700050339102745,\n",
       " 0.05421994626522064,\n",
       " 0.008610772900283337,\n",
       " -0.002975529758259654,\n",
       " 0.01724071614444256,\n",
       " 0.028468672186136246,\n",
       " 0.04846725985407829,\n",
       " -0.0061178747564554214,\n",
       " -0.03491273149847984,\n",
       " 0.04361604154109955,\n",
       " -0.027153680101037025,\n",
       " -0.03434748202562332,\n",
       " 0.004719191696494818,\n",
       " -0.01134198997169733,\n",
       " -0.011766408570110798,\n",
       " -0.0023053253535181284,\n",
       " -0.0019783040042966604,\n",
       " 0.00783612858504057,\n",
       " 0.002790826139971614,\n",
       " 0.017044255509972572,\n",
       " -0.011112666688859463,\n",
       " -0.0010570310987532139,\n",
       " 0.007053925655782223,\n",
       " -0.0256141759455204,\n",
       " -0.059919025748968124,\n",
       " -0.010589949786663055,\n",
       " -0.029333574697375298,\n",
       " 0.00513229938223958,\n",
       " 0.05861468240618706,\n",
       " -0.038903895765542984,\n",
       " -0.01679154671728611,\n",
       " 0.01212620735168457,\n",
       " 0.04414703696966171,\n",
       " -0.02205563895404339,\n",
       " 0.014336016960442066,\n",
       " -0.024857977405190468,\n",
       " 0.005500044208019972,\n",
       " -0.025699276477098465,\n",
       " 0.013863494619727135,\n",
       " -0.037095457315444946,\n",
       " 0.03771134838461876,\n",
       " -0.05520058795809746,\n",
       " -0.028178220614790916,\n",
       " -0.011334056966006756,\n",
       " 0.0106479711830616,\n",
       " -0.004544062539935112,\n",
       " 0.028556281700730324,\n",
       " -0.02619313821196556,\n",
       " 0.028198469430208206,\n",
       " -0.0003834091476164758,\n",
       " -0.016688598319888115,\n",
       " 0.023840203881263733,\n",
       " 0.03923992067575455,\n",
       " -0.013143843971192837,\n",
       " 0.0414883978664875,\n",
       " 0.05749376118183136,\n",
       " 0.004091393202543259,\n",
       " 0.014675666578114033,\n",
       " -0.0018054369138553739,\n",
       " -0.02182064577937126,\n",
       " -0.030016262084245682,\n",
       " 0.030795928090810776,\n",
       " 0.03373120352625847,\n",
       " -0.011746812611818314,\n",
       " 0.014067815616726875,\n",
       " -0.022776497527956963,\n",
       " 0.004314986057579517,\n",
       " -0.034239787608385086,\n",
       " -0.016720648854970932,\n",
       " -0.008830930106341839,\n",
       " 0.03424105420708656,\n",
       " -0.025745686143636703,\n",
       " -0.026202570647001266,\n",
       " -0.012241285294294357,\n",
       " 0.05258559808135033,\n",
       " 0.04846356809139252,\n",
       " 0.0041061402298510075,\n",
       " -0.0024142528418451548,\n",
       " -0.03288392350077629,\n",
       " -0.002283022040501237,\n",
       " -0.020378677174448967,\n",
       " -0.006512663792818785,\n",
       " 0.03180767968297005,\n",
       " 0.0017620858270674944,\n",
       " -0.010916934348642826,\n",
       " -0.037505824118852615,\n",
       " 0.015222929418087006,\n",
       " 0.025067826732993126,\n",
       " -0.011153597384691238,\n",
       " -0.007979734800755978,\n",
       " -0.05447828769683838,\n",
       " -0.06740102916955948,\n",
       " -0.019947394728660583,\n",
       " 0.003076550317928195,\n",
       " 0.008889637887477875,\n",
       " 0.04197806864976883,\n",
       " -0.04058999568223953,\n",
       " -0.013729555532336235,\n",
       " 0.0075941020622849464,\n",
       " -0.05407720059156418,\n",
       " 0.016130603849887848,\n",
       " -0.03689674660563469,\n",
       " -0.05960332229733467,\n",
       " -0.01718701235949993,\n",
       " 0.011685995385050774,\n",
       " -0.00906528439372778,\n",
       " 0.01796610839664936,\n",
       " -0.05016111582517624,\n",
       " 0.009511583484709263,\n",
       " -0.023935697972774506,\n",
       " -0.03634049743413925,\n",
       " -0.02001330815255642,\n",
       " 0.04298793524503708,\n",
       " 0.026905696839094162,\n",
       " 0.030607281252741814,\n",
       " 0.008280624635517597,\n",
       " -0.004973525181412697,\n",
       " -0.02186509408056736,\n",
       " 0.023368753492832184,\n",
       " -0.0011711323168128729,\n",
       " -0.036325447261333466,\n",
       " 0.020773790776729584,\n",
       " -0.0034365984611213207,\n",
       " 0.013721802271902561,\n",
       " -0.0007138825603760779,\n",
       " -0.03234650194644928,\n",
       " 0.004982922226190567,\n",
       " -0.02835010178387165,\n",
       " -0.011414242908358574,\n",
       " 0.010671522468328476,\n",
       " 0.019240353256464005,\n",
       " -0.03361924737691879,\n",
       " 0.04102902114391327,\n",
       " -0.06717552244663239,\n",
       " -0.02826613001525402,\n",
       " -0.0009448445634916425,\n",
       " -0.0056538707576692104,\n",
       " 0.029895491898059845,\n",
       " -0.025476660579442978,\n",
       " -0.04226497560739517,\n",
       " 0.024219153448939323,\n",
       " 0.0586920790374279,\n",
       " 0.00022416601132135838,\n",
       " -0.016111919656395912,\n",
       " -0.008699950762093067,\n",
       " 0.015198986977338791,\n",
       " 0.019121386110782623,\n",
       " -0.02714015543460846,\n",
       " 0.0007447691168636084,\n",
       " 0.02448752336204052,\n",
       " 0.023719601333141327,\n",
       " 0.008825060911476612,\n",
       " -0.036258015781641006,\n",
       " 0.016396695747971535,\n",
       " 0.017203982919454575,\n",
       " 0.0005040470859967172,\n",
       " -0.008091293275356293,\n",
       " 0.027550319209694862,\n",
       " 0.008146841078996658,\n",
       " 0.04597102850675583,\n",
       " 0.0079569136723876,\n",
       " 0.00018892013758886606,\n",
       " -0.017058828845620155,\n",
       " -0.00015857293328735977,\n",
       " -0.018946196883916855,\n",
       " 0.006267242599278688,\n",
       " -0.021901847794651985,\n",
       " 0.014354781247675419,\n",
       " -0.03167842701077461,\n",
       " -0.004925035871565342,\n",
       " 0.021780328825116158,\n",
       " -0.02470533736050129,\n",
       " 0.013946405611932278,\n",
       " -0.02258486859500408,\n",
       " -0.034956853836774826,\n",
       " -0.006399368867278099,\n",
       " 0.01943071000277996,\n",
       " 0.04788275808095932,\n",
       " 0.003613683395087719,\n",
       " 0.0005579503485932946,\n",
       " 0.018027035519480705,\n",
       " 0.013595814816653728,\n",
       " 0.05476895719766617,\n",
       " -0.0012807947350665927,\n",
       " 0.03141530603170395,\n",
       " -0.025033246725797653,\n",
       " -0.001438478589989245,\n",
       " -0.024048062041401863,\n",
       " -0.020603932440280914,\n",
       " 0.01777338609099388,\n",
       " -0.03518076241016388,\n",
       " -0.06060651317238808,\n",
       " 0.04375101253390312,\n",
       " 0.013740379363298416,\n",
       " 0.007388962898403406,\n",
       " 0.048818111419677734,\n",
       " -0.011543499305844307,\n",
       " -0.02089373953640461,\n",
       " -0.015165378339588642,\n",
       " 0.012234829366207123,\n",
       " 0.020128216594457626,\n",
       " 0.0505240261554718,\n",
       " 0.037457652390003204,\n",
       " -0.024797722697257996,\n",
       " -0.009449531324207783,\n",
       " 0.02366793341934681,\n",
       " -0.030198246240615845,\n",
       " 0.03355163335800171,\n",
       " 0.012273760512471199,\n",
       " 0.036440469324588776,\n",
       " 0.026232605800032616,\n",
       " 0.025343742221593857,\n",
       " 0.04458336532115936,\n",
       " 0.008436556905508041,\n",
       " 0.028991661965847015,\n",
       " 0.014141534455120564,\n",
       " -0.004108536522835493,\n",
       " -0.047859326004981995,\n",
       " 0.05806005373597145,\n",
       " -0.03157620131969452,\n",
       " -0.0037020931486040354,\n",
       " -0.02011479064822197,\n",
       " -0.012194535695016384,\n",
       " -0.03968149051070213,\n",
       " 0.04804276302456856,\n",
       " -0.0027435452211648226,\n",
       " -0.041532378643751144,\n",
       " -0.025054272264242172,\n",
       " 0.01781628094613552,\n",
       " -0.04805039241909981,\n",
       " 0.03776736930012703,\n",
       " 0.030888015404343605,\n",
       " 0.046807143837213516,\n",
       " 0.014117425307631493,\n",
       " -0.002566857961937785,\n",
       " -0.00879672821611166,\n",
       " 0.007175813894718885,\n",
       " 0.007094031199812889,\n",
       " -0.04725813493132591,\n",
       " -0.03810422122478485,\n",
       " -0.04549948126077652,\n",
       " -0.010970043949782848,\n",
       " -0.018917186185717583,\n",
       " -0.018857616931200027,\n",
       " -0.03127409517765045,\n",
       " 0.07060682028532028,\n",
       " -0.00556538300588727,\n",
       " -0.05904081463813782,\n",
       " -0.007889200933277607,\n",
       " 0.035063229501247406,\n",
       " 0.007665117271244526,\n",
       " 0.006530501414090395,\n",
       " 0.011859573423862457,\n",
       " 0.06702262163162231,\n",
       " 0.009929065592586994,\n",
       " 0.014036651700735092,\n",
       " -0.05146210268139839,\n",
       " 0.03108176775276661,\n",
       " 0.007721082307398319,\n",
       " 0.0012557335430756211,\n",
       " 0.01810063049197197,\n",
       " -0.048538487404584885,\n",
       " 0.033132005482912064,\n",
       " 0.03835528716444969,\n",
       " 0.0021053983364254236,\n",
       " 0.031274113804101944,\n",
       " -0.011824645102024078,\n",
       " -0.028488392010331154,\n",
       " -0.042901210486888885,\n",
       " 0.0060158176347613335,\n",
       " 0.02146153524518013,\n",
       " 0.007190723903477192,\n",
       " -0.04060141742229462,\n",
       " -0.0694197565317154,\n",
       " 0.0076758298091590405,\n",
       " -0.041472289711236954,\n",
       " -0.006991574540734291,\n",
       " -0.021313555538654327,\n",
       " 0.023559696972370148,\n",
       " -0.021263085305690765,\n",
       " 0.023837339133024216,\n",
       " 0.00618019187822938,\n",
       " -0.07080791145563126,\n",
       " 0.21785444021224976,\n",
       " 0.07706182450056076,\n",
       " 0.09800142049789429,\n",
       " 0.028202828019857407,\n",
       " -0.021022146567702293,\n",
       " 0.05890114977955818,\n",
       " -0.021397748962044716,\n",
       " -0.029856601729989052,\n",
       " -0.00710512138903141,\n",
       " -0.03613113611936569,\n",
       " 0.04194856062531471,\n",
       " 0.009116076864302158,\n",
       " -0.018259698525071144,\n",
       " 0.026420049369335175,\n",
       " -0.004536970518529415,\n",
       " 0.04355575144290924,\n",
       " -0.046945299953222275,\n",
       " 0.010494954884052277,\n",
       " 0.02359957993030548,\n",
       " -0.02783118188381195,\n",
       " -0.019389083608984947,\n",
       " 0.022907914593815804,\n",
       " -0.020874569192528725,\n",
       " 0.017028577625751495,\n",
       " -0.027132686227560043,\n",
       " -0.00293298764154315,\n",
       " 0.0793004259467125,\n",
       " -0.0690155029296875,\n",
       " 0.0029018160421401262,\n",
       " 0.006358926650136709,\n",
       " -0.011671883054077625,\n",
       " 0.00038326342473737895,\n",
       " 0.01053500548005104,\n",
       " 0.011805254966020584,\n",
       " 0.04817289859056473,\n",
       " -0.006467471364885569,\n",
       " -0.0034904752392321825,\n",
       " -0.07139609754085541,\n",
       " -0.03289879113435745,\n",
       " 0.020803265273571014,\n",
       " -0.017552515491843224,\n",
       " -0.040497276932001114,\n",
       " 0.0006836374523118138,\n",
       " -0.05047891288995743,\n",
       " 0.0011096245143562555,\n",
       " 0.0009684914257377386,\n",
       " -0.022059690207242966,\n",
       " 0.013184972107410431,\n",
       " -0.02020520158112049,\n",
       " -0.025780418887734413,\n",
       " 0.008650937117636204,\n",
       " -0.004297452978789806,\n",
       " -0.007853151299059391,\n",
       " -0.06472571939229965,\n",
       " -0.018669210374355316,\n",
       " 0.03165678307414055,\n",
       " 0.015270446427166462,\n",
       " -0.008737590163946152,\n",
       " -0.02408609725534916,\n",
       " 0.03383767232298851,\n",
       " -0.053209688514471054,\n",
       " 0.00455510476604104,\n",
       " -0.0213470496237278,\n",
       " -0.03469051048159599,\n",
       " -0.0017528439639136195,\n",
       " 0.03925533592700958,\n",
       " -0.0005934964283369482,\n",
       " 0.010441238060593605,\n",
       " -0.028416501358151436,\n",
       " 0.010357626713812351,\n",
       " -0.00033337765489704907,\n",
       " ...]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6a982c35-2442-4b0e-8a68-7f7b94f0a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d787140c-fb9d-48db-aaa6-8adf4666b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers\n",
    "# !pip install langchain_huggingface\n",
    "# !pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "470bae95-12a6-474b-b186-05d544c710ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f576a-37b9-4965-850d-44465296e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace sentence-transformers:\n",
    "#It’s a library for generating vector embeddings from text — basically turning sentences, paragraphs, or even documents into numbers \n",
    "# that a computer can understand for semantic similarity, search, clustering, or RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "14f0dc72-c48b-4066-861d-1036bf9a4e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64f3238921842928ee34f311b126031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc39023c02be4b2fa605d7dd0b48c9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7525a113d9f4ee7bd3e5a553515f6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1695b499f44d4c95a1ad9854312bdedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c8e3e353eb4de9ae1e07d7f118ac3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e16ac0926841609f93a063e8858378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9446f24843dc441391f439cc9a464b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52bb043f08945c58f177515960772f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452ec6dbc07e4d1392ca0678161ee7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dae6a64067640629d46ddde183bbc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13622a5c6bf9424d8da08b34fa498713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_embeddings = HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "dec40e5c-d957-49fd-919c-3cf231d44f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"This is a example test sentence to be converted into a vector embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7174e765-2c98-484a-bdcb-42ce7d4733e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result = hf_embeddings.embed_query(query)\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a86eb-04b8-4e58-99db-207630a1e159",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "74d02f74-22bd-4ae2-803a-47ddae19f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6d23a-622a-4001-8e6c-3e284897db4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Loading Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b6c0758f-5cbe-4649-8e6d-1029a1e5ec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "297b7a99-cb71-48bf-adac-50073eae2081",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_loader = TextLoader(\"./Data/Text/speech.txt\")\n",
    "loaded_text = text_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1af28b51-f778-4e88-97ff-fa4c5c5c4290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './Data/Text/speech.txt'}, page_content='Ladies and gentlemen, friends, and fellow builders,\\n\\nWe live in an era where change no longer knocks on the door. It breaks it down. Every year, technology moves faster, systems grow more complex, and decisions that once took decades now take months, sometimes weeks. And in the middle of all this acceleration, there is one uncomfortable truth we must face: progress without direction is not innovation—it is chaos.\\n\\nFor generations, we measured success by what we could build. Bigger machines. Faster networks. Smarter systems. But today, the real challenge is not whether we can build something. It is whether we should, and if we do, whether we understand the consequences of what we are putting into the world.\\n\\nTechnology is not neutral. Every line of code carries assumptions. Every model reflects priorities. Every system amplifies certain voices while muting others. When we say we are “just engineers” or “just researchers,” we ignore the fact that our work shapes how people live, learn, communicate, and even think.\\n\\nLook around us. Algorithms decide what news we see. Automated systems influence who gets a loan, who gets a job interview, and who is flagged as a risk. These systems are efficient, scalable, and impressive. But efficiency without accountability can quietly erode trust. And trust, once lost, is far harder to rebuild than any system we deploy.\\n\\nThe future we are building will not be defined by our tools alone. It will be defined by the values we embed into them. If we optimize only for speed, we will get speed. If we optimize only for profit, we will get profit. But if we fail to optimize for fairness, transparency, and human dignity, we should not be surprised when the systems we create feel cold, distant, and unfair.')]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5186f448-c5d8-45b4-a48d-4b8e2063fb62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Splitting Text into Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "eaa2034a-fbb4-4c2e-b820-718e292b8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1e3a0ad9-b756-40bd-9911-b1812fa9df76",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_txt_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "03786d23-d93a-4bdf-b75f-27a4dd487b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_documents = rec_txt_splitter.split_documents(loaded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "51573455-ce67-4c6d-a89c-ba18dc91f711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(text_documents), type(text_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb866d3a-8382-4fca-a98e-71d0edd13326",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Creating a FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "9caefdaf-02a6-4b39-8810-37d9d6030883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a7445bb7-2c5a-402b-8681-1588069efa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the OLLAMA embeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "10e956da-5207-422c-9432-2df582f918cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"mxbai-embed-large\"\n",
    "    # default llama2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7e159cef-70a9-409f-ab58-f72a823ece42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x169cd2460>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a FAISS vector store,\n",
    "# ollama has to be running\n",
    "db = FAISS.from_documents(text_documents, embedding=embeddings)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "baa1d2c8-64c4-4fa8-b3dd-aa1edd4bf298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since we had 5 documents, faiss db actually is a 2D matrix of 5 rows and number of embeddings per document columns (here, 1024)\n",
    "db.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b17ccf97-feff-4d52-b1fe-3afd65dfa3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index.d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26215659-b030-4019-9a2d-0212bddeeacc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Interacting with the Vector Store DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "37ccb7ad-3f8a-41f8-b42f-814a1d3318dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"What the speaker has to tell about bigger machines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "f4f50d52-fc6f-4f62-8897-0ad2dbfc8017",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "23cc5633-eb59-4488-86a6-bdd1c1c0365e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For generations, we measured success by what we could build. Bigger machines. Faster networks. Smarter systems. But today, the real challenge is not whether we can build something. It is whether we should, and if we do, whether we understand the consequences of what we are putting into the world.\n",
      "\n",
      "Ladies and gentlemen, friends, and fellow builders,\n",
      "\n",
      "We live in an era where change no longer knocks on the door. It breaks it down. Every year, technology moves faster, systems grow more complex, and decisions that once took decades now take months, sometimes weeks. And in the middle of all this acceleration, there is one uncomfortable truth we must face: progress without direction is not innovation—it is chaos.\n",
      "\n",
      "Technology is not neutral. Every line of code carries assumptions. Every model reflects priorities. Every system amplifies certain voices while muting others. When we say we are “just engineers” or “just researchers,” we ignore the fact that our work shapes how people live, learn, communicate, and even think.\n",
      "\n",
      "Look around us. Algorithms decide what news we see. Automated systems influence who gets a loan, who gets a job interview, and who is flagged as a risk. These systems are efficient, scalable, and impressive. But efficiency without accountability can quietly erode trust. And trust, once lost, is far harder to rebuild than any system we deploy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in similar_docs:\n",
    "    print(item.page_content, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9194240d-b597-4f7c-9685-650d28f602ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Converting the Vector Store to a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "0bbada70-cac8-4e78-a0f5-b839cb9db33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "041e6356-bb40-443c-bf91-18936d40c275",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_docs_using_retriever = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "65da0c78-9ce6-42ae-be9d-f8a38c40450f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For generations, we measured success by what we could build. Bigger machines. Faster networks. Smarter systems. But today, the real challenge is not whether we can build something. It is whether we should, and if we do, whether we understand the consequences of what we are putting into the world.\n",
      "\n",
      "Ladies and gentlemen, friends, and fellow builders,\n",
      "\n",
      "We live in an era where change no longer knocks on the door. It breaks it down. Every year, technology moves faster, systems grow more complex, and decisions that once took decades now take months, sometimes weeks. And in the middle of all this acceleration, there is one uncomfortable truth we must face: progress without direction is not innovation—it is chaos.\n",
      "\n",
      "Technology is not neutral. Every line of code carries assumptions. Every model reflects priorities. Every system amplifies certain voices while muting others. When we say we are “just engineers” or “just researchers,” we ignore the fact that our work shapes how people live, learn, communicate, and even think.\n",
      "\n",
      "Look around us. Algorithms decide what news we see. Automated systems influence who gets a loan, who gets a job interview, and who is flagged as a risk. These systems are efficient, scalable, and impressive. But efficiency without accountability can quietly erode trust. And trust, once lost, is far harder to rebuild than any system we deploy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in similar_docs_using_retriever:\n",
    "    print(item.page_content, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "15cb66b2-cb7a-4c68-b4bc-aa03fa8b066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# both the vector store db and the retriever give the same results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5ab398-c99e-4bf2-933c-b8edbce45059",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Similarity Search with Score using Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "30a97598-01f2-4577-93ac-413b00d8296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_docs_with_score = db.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e80396ce-37eb-4f9e-937c-74770a96b20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Document(id='e2a0acc7-277a-455e-9e5b-f8be36741a97', metadata={'source': './Data/Text/speech.txt'}, page_content='For generations, we measured success by what we could build. Bigger machines. Faster networks. Smarter systems. But today, the real challenge is not whether we can build something. It is whether we should, and if we do, whether we understand the consequences of what we are putting into the world.'), np.float32(0.75798464))\n",
      "\n",
      "(Document(id='fab83da3-2c97-49d6-9fe7-b7d42603944d', metadata={'source': './Data/Text/speech.txt'}, page_content='Ladies and gentlemen, friends, and fellow builders,\\n\\nWe live in an era where change no longer knocks on the door. It breaks it down. Every year, technology moves faster, systems grow more complex, and decisions that once took decades now take months, sometimes weeks. And in the middle of all this acceleration, there is one uncomfortable truth we must face: progress without direction is not innovation—it is chaos.'), np.float32(0.78366566))\n",
      "\n",
      "(Document(id='31dcd8e4-0988-4782-99fb-fe5884313114', metadata={'source': './Data/Text/speech.txt'}, page_content='Technology is not neutral. Every line of code carries assumptions. Every model reflects priorities. Every system amplifies certain voices while muting others. When we say we are “just engineers” or “just researchers,” we ignore the fact that our work shapes how people live, learn, communicate, and even think.'), np.float32(0.8077606))\n",
      "\n",
      "(Document(id='23b9127b-3834-4027-bb63-bd24b4aed160', metadata={'source': './Data/Text/speech.txt'}, page_content='Look around us. Algorithms decide what news we see. Automated systems influence who gets a loan, who gets a job interview, and who is flagged as a risk. These systems are efficient, scalable, and impressive. But efficiency without accountability can quietly erode trust. And trust, once lost, is far harder to rebuild than any system we deploy.'), np.float32(0.82879007))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in similar_docs_with_score:\n",
    "    print(item, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1796f-ec34-4582-8f0c-0a1c1b0c7b7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Interacting using vectors instead of text query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "3da0e3dd-3e39-4126-a647-357d6ab10dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding = embeddings.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "d426ba15-d4f2-4a44-acc3-3942ddd00538",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_docs_using_vectors = db.similarity_search_by_vector(text_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ece54ab9-2c3d-4dc1-92a1-dcdf3f3ef3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='For generations, we measured success by what we could build. Bigger machines. Faster networks. Smarter systems. But today, the real challenge is not whether we can build something. It is whether we should, and if we do, whether we understand the consequences of what we are putting into the world.' metadata={'source': './Data/Text/speech.txt'}\n",
      "\n",
      "page_content='Ladies and gentlemen, friends, and fellow builders,\n",
      "\n",
      "We live in an era where change no longer knocks on the door. It breaks it down. Every year, technology moves faster, systems grow more complex, and decisions that once took decades now take months, sometimes weeks. And in the middle of all this acceleration, there is one uncomfortable truth we must face: progress without direction is not innovation—it is chaos.' metadata={'source': './Data/Text/speech.txt'}\n",
      "\n",
      "page_content='Technology is not neutral. Every line of code carries assumptions. Every model reflects priorities. Every system amplifies certain voices while muting others. When we say we are “just engineers” or “just researchers,” we ignore the fact that our work shapes how people live, learn, communicate, and even think.' metadata={'source': './Data/Text/speech.txt'}\n",
      "\n",
      "page_content='Look around us. Algorithms decide what news we see. Automated systems influence who gets a loan, who gets a job interview, and who is flagged as a risk. These systems are efficient, scalable, and impressive. But efficiency without accountability can quietly erode trust. And trust, once lost, is far harder to rebuild than any system we deploy.' metadata={'source': './Data/Text/speech.txt'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in similar_docs_using_vectors:\n",
    "    print(item, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf671c8-93e6-4b7c-a5bd-aa817a21c2ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Saving the vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "9b631fcc-ac27-434b-8e49-9594f5d4289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7bea2-3b09-4888-a235-8ce85530a124",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Loading the vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "06b134eb-ffeb-4567-9d9a-6fdcb9e7d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbb = FAISS.load_local(\"faiss_index\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "# embeddings = OLLAMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f22c368-be90-4a61-8924-67fb03280d74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "9c3a4809-b35e-4f43-a083-4e4c98b12564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb\n",
    "# !pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "b252047c-f958-4319-921f-eb9776d03593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "3c4d7cb9-2ca0-440b-8941-eb6e5b107598",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromadb = Chroma.from_documents(text_documents, embedding=embeddings, persist_directory=\"./chromadb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "f2bac6fd-9372-4d2b-b595-c328fb1bcf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"What the speaker has to tell about bigger machines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "d6e855fc-bda1-44c9-a9b1-3c7edd0edf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ladies and gentlemen, friends, and fellow builders,\n",
      "\n",
      "We live in an era where change no longer knocks on the door. It breaks it down. Every year, technology moves faster, systems grow more complex, and decisions that once took decades now take months, sometimes weeks. And in the middle of all this acceleration, there is one uncomfortable truth we must face: progress without direction is not innovation—it is chaos.\n",
      "\n",
      "For generations, we measured success by what we could build. Bigger machines. Faster networks. Smarter systems. But today, the real challenge is not whether we can build something. It is whether we should, and if we do, whether we understand the consequences of what we are putting into the world.\n",
      "\n",
      "Technology is not neutral. Every line of code carries assumptions. Every model reflects priorities. Every system amplifies certain voices while muting others. When we say we are “just engineers” or “just researchers,” we ignore the fact that our work shapes how people live, learn, communicate, and even think.\n",
      "\n",
      "Look around us. Algorithms decide what news we see. Automated systems influence who gets a loan, who gets a job interview, and who is flagged as a risk. These systems are efficient, scalable, and impressive. But efficiency without accountability can quietly erode trust. And trust, once lost, is far harder to rebuild than any system we deploy.\n",
      "\n",
      "The future we are building will not be defined by our tools alone. It will be defined by the values we embed into them. If we optimize only for speed, we will get speed. If we optimize only for profit, we will get profit. But if we fail to optimize for fairness, transparency, and human dignity, we should not be surprised when the systems we create feel cold, distant, and unfair.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_docs = chromadb.similarity_search(query)\n",
    "for item in similar_docs:\n",
    "    print(item.page_content, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "d7592f0d-6516-4c57-bd4b-6ce7399c5926",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromadbb = Chroma(persist_directory=\"./chromadb\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "08e4cb3a-8888-400d-a4ee-b44ce1dd5ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ladies and gentlemen, friends, and fellow builders,\n",
      "\n",
      "We live in an era where change no longer knocks on the door. It breaks it down. Every year, technology moves faster, systems grow more complex, and decisions that once took decades now take months, sometimes weeks. And in the middle of all this acceleration, there is one uncomfortable truth we must face: progress without direction is not innovation—it is chaos.\n",
      "\n",
      "For generations, we measured success by what we could build. Bigger machines. Faster networks. Smarter systems. But today, the real challenge is not whether we can build something. It is whether we should, and if we do, whether we understand the consequences of what we are putting into the world.\n",
      "\n",
      "Technology is not neutral. Every line of code carries assumptions. Every model reflects priorities. Every system amplifies certain voices while muting others. When we say we are “just engineers” or “just researchers,” we ignore the fact that our work shapes how people live, learn, communicate, and even think.\n",
      "\n",
      "Look around us. Algorithms decide what news we see. Automated systems influence who gets a loan, who gets a job interview, and who is flagged as a risk. These systems are efficient, scalable, and impressive. But efficiency without accountability can quietly erode trust. And trust, once lost, is far harder to rebuild than any system we deploy.\n",
      "\n",
      "The future we are building will not be defined by our tools alone. It will be defined by the values we embed into them. If we optimize only for speed, we will get speed. If we optimize only for profit, we will get profit. But if we fail to optimize for fairness, transparency, and human dignity, we should not be surprised when the systems we create feel cold, distant, and unfair.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similar_docs = chromadbb.similarity_search(query)\n",
    "for item in similar_docs:\n",
    "    print(item.page_content, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "fd51472b-1de4-4045-a2dc-fa8b349f3428",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_chroma = chromadb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "3907e437-69f2-465b-a6ea-a545f0e89eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents_using_retriever_chroma = retriever_chroma.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "1c053411-17e2-478a-a868-1777eea83e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ladies and gentlemen, friends, and fellow builders,\n",
      "\n",
      "We live in an era where change no longer knocks on the door. It breaks it down. Every year, technology moves faster, systems grow more complex, and decisions that once took decades now take months, sometimes weeks. And in the middle of all this acceleration, there is one uncomfortable truth we must face: progress without direction is not innovation—it is chaos.\n",
      "\n",
      "For generations, we measured success by what we could build. Bigger machines. Faster networks. Smarter systems. But today, the real challenge is not whether we can build something. It is whether we should, and if we do, whether we understand the consequences of what we are putting into the world.\n",
      "\n",
      "Technology is not neutral. Every line of code carries assumptions. Every model reflects priorities. Every system amplifies certain voices while muting others. When we say we are “just engineers” or “just researchers,” we ignore the fact that our work shapes how people live, learn, communicate, and even think.\n",
      "\n",
      "Look around us. Algorithms decide what news we see. Automated systems influence who gets a loan, who gets a job interview, and who is flagged as a risk. These systems are efficient, scalable, and impressive. But efficiency without accountability can quietly erode trust. And trust, once lost, is far harder to rebuild than any system we deploy.\n",
      "\n",
      "The future we are building will not be defined by our tools alone. It will be defined by the values we embed into them. If we optimize only for speed, we will get speed. If we optimize only for profit, we will get profit. But if we fail to optimize for fairness, transparency, and human dignity, we should not be surprised when the systems we create feel cold, distant, and unfair.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in retrieved_documents_using_retriever_chroma:\n",
    "    print(item.page_content, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfcf35c-65d4-4762-a520-93b486b9a6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
